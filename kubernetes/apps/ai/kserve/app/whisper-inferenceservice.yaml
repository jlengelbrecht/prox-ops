---
# KServe InferenceService: Whisper Medium (Speech-to-Text)
# STORY-030-4: Deploy Whisper for voice calling STT
# Model: Systran/faster-whisper-medium (~1.5GB disk, ~1.4GB VRAM)
# Framework: speaches (formerly faster-whisper-server) with OpenAI-compatible API
# GPU: RTX A2000 12GB (k8s-work-4) - uses time-slicing, can coexist with Plex
#
# Performance Characteristics:
# - Cold start: ~30-60s (model loading from pre-downloaded cache)
# - Request timeout: 600s (allows for cold start + transcription)
# - Warm inference: ~1-3s for 10s audio clip
# - Scale-to-zero: 5 minutes idle timeout
# - VRAM usage: ~1.4GB baseline on A2000 (model + runtime)
# - First request: May load model on demand (PRELOAD_MODELS not working in 0.6.0-rc)
#
# API Endpoints (OpenAI-compatible):
# - POST /v1/audio/transcriptions - Transcribe audio to text
# - POST /v1/audio/translations - Translate audio to English text
#
# Usage:
#   curl -X POST http://whisper-medium-predictor.ai.svc.cluster.local/v1/audio/transcriptions \
#     -H "Content-Type: multipart/form-data" \
#     -F file=@audio.mp3 \
#     -F model=medium
#
# GPU Time-Slicing Note (STORY-030-1):
# - A2000 has 3 virtual GPU slots (time-slicing enabled)
# - Whisper uses 1 slot, Plex uses 1 slot, 1 slot spare
# - Time-slicing shares GPU compute; VRAM is NOT isolated
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: whisper-medium
  namespace: ai
  labels:
    app.kubernetes.io/name: whisper-medium
    app.kubernetes.io/component: inference
    app.kubernetes.io/part-of: kserve
    model.framework: speaches
    model.type: audio-transcription
    model.size: medium
  annotations:
    serving.kserve.io/enable-prometheus-scraping: "true"
    security.homelab/gpu-required: "NVIDIA RTX A2000 for STT inference"
    security.homelab/network: "Internal only - ClusterIP service"
    # Knative timeout configuration for model loading + transcription
    # Default is 300s which times out during cold start model loading
    # 600s allows: model download (60s) + conversion (30s) + transcription (variable)
    serving.knative.dev/progress-deadline: "600s"
spec:
  predictor:
    # Scale-to-zero configuration (Knative Serving)
    minReplicas: 0  # Enable scale-to-zero (GPU freed when idle)
    maxReplicas: 1  # Single instance - voice calling is sequential

    # Request timeout: model loading (~90s) + transcription time
    # Prevents 504 gateway timeout during cold starts
    timeout: 600

    # Target A2000 GPU node (leaves A5000 for TTS and LLM)
    nodeSelector:
      kubernetes.io/hostname: k8s-work-4
      gpu.nvidia.com/model: rtx-a2000

    containers:
      - name: speaches
        # speaches (formerly faster-whisper-server): OpenAI-compatible Whisper API
        # CTranslate2 backend provides 4x faster inference than original Whisper
        # v0.6.0+ adds OpenAI-compatible /v1/audio/transcriptions endpoint
        # renovate: datasource=docker depName=fedirz/faster-whisper-server
        image: fedirz/faster-whisper-server:0.6.0-rc.3-cuda

        # Environment configuration (uses WHISPER__ prefix for nested config)
        # Reference: https://speaches.ai/configuration/
        env:
          # Server binding (uses uvicorn config in 0.6.0+)
          - name: UVICORN_HOST
            value: "0.0.0.0"

          - name: UVICORN_PORT
            value: "8000"

          # Model configuration - preload medium model
          - name: WHISPER__MODEL
            value: "medium"

          # Device configuration
          - name: WHISPER__INFERENCE_DEVICE
            value: "cuda"

          # Compute type for GPU (float16 for efficiency on A2000)
          - name: WHISPER__COMPUTE_TYPE
            value: "float16"

          # Disable Gradio UI (not needed for API-only use)
          - name: ENABLE_UI
            value: "false"

          # Keep model loaded longer to reduce cold starts
          # -1 = never unload, but we use 600s to allow scale-to-zero
          - name: STT_MODEL_TTL
            value: "600"

          # Reduce log verbosity
          - name: LOG_LEVEL
            value: "info"

          # Preload model at startup to avoid first-request delay
          # Model will be downloaded/converted during container startup
          # This increases cold start time but ensures first request is fast
          - name: PRELOAD_MODELS
            value: '["Systran/faster-whisper-medium"]'

          # Cache directory for model files
          # Must match download job cache structure: /models/.cache/huggingface/hub/
          - name: HF_HOME
            value: "/models/.cache/huggingface"

          # Disable telemetry
          - name: HF_HUB_DISABLE_TELEMETRY
            value: "1"

          # Force offline mode to use pre-downloaded model from cache
          # Prevents API calls to HuggingFace which are blocked by NetworkPolicy
          - name: HF_HUB_OFFLINE
            value: "1"

          # Fix for non-root user
          - name: HOME
            value: "/tmp"

        # Resource requirements (small model, efficient runtime)
        resources:
          requests:
            cpu: 1
            memory: 2Gi
            nvidia.com/gpu: 1
          limits:
            cpu: 4
            memory: 4Gi
            nvidia.com/gpu: 1

        # Health checks
        ports:
          - containerPort: 8000
            name: http1
            protocol: TCP

        # Probes optimized for speaches with model preloading
        # Model download + CTranslate2 conversion at startup ~60-90s
        # Increased delays to allow PRELOAD_MODELS to complete
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 90   # Increased for model preload
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 12  # 90 + 12*10 = 210s max

        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60   # Start checking after model likely loaded
          periodSeconds: 5
          timeoutSeconds: 5
          failureThreshold: 30  # 60 + 30*5 = 210s max

        # Volume mounts
        volumeMounts:
          # Model cache (consistent with other KServe services)
          - name: model-cache
            mountPath: /models
          # Shared memory for faster inference
          - name: dshm
            mountPath: /dev/shm

        # Security context
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities:
            drop:
              - ALL

    # NVIDIA container runtime for GPU access
    runtimeClassName: nvidia

    # Security context (pod-level)
    securityContext:
      fsGroup: 1000
      seccompProfile:
        type: RuntimeDefault

    serviceAccountName: default

    # Volumes
    volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 1Gi

---
# KServe InferenceService: Dolphin3.0-Llama3.1-8B (vLLM Text-Only)
# STORY-142: Uncensored chat model deployment
# Model: cognitivecomputations/Dolphin3.0-Llama3.1-8B (~16GB VRAM, uncensored)
# Framework: vLLM (regular, NOT vLLM-Omni) for text-only autoregressive models
# GPU: RTX A5000 24GB (k8s-work-10 ONLY)
#
# Performance Characteristics (based on qwen-coder learnings):
# - Cold start: ~23-30s (enforce-eager mode, API ready)
# - Warm inference: ~50-80ms (eager mode)
# - Scale-to-zero: 5 minutes idle timeout
# - VRAM usage: ~16GB / 24GB available
# - Single GPU: maxReplicas=1 enforced (only 1 RTX A5000 available)
#
# Usage:
#   kubectl run -it --rm test-dolphin --image=curlimages/curl --restart=Never -- \
#     curl -X POST http://dolphin-chat.ai.svc.cluster.local/v1/chat/completions \
#     -H "Content-Type: application/json" \
#     -d '{"model":"/models/cognitivecomputations/Dolphin3.0-Llama3.1-8B","messages":[{"role":"user","content":"Hello"}]}'
#
# Expected behavior:
# - First request triggers cold start (pod 0->1, ~23-30s delay)
# - Model loads from /models/cognitivecomputations/Dolphin3.0-Llama3.1-8B (safetensors format)
# - Subsequent requests: ~50-80ms (eager mode inference)
# - After 5min idle, pod scales to 0 (GPU freed for other workloads)
#
# GPU Contention Note:
# - Only ONE InferenceService can run at a time (single RTX A5000)
# - Scale down qwen-coder before testing dolphin-chat, or wait for auto-scale-to-zero
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: dolphin-chat
  namespace: ai
  labels:
    app.kubernetes.io/name: dolphin-chat
    app.kubernetes.io/component: inference
    app.kubernetes.io/part-of: kserve
    model.framework: vllm
    model.type: text-only
    model.size: 8b
  annotations:
    serving.kserve.io/enable-prometheus-scraping: "true"
    security.homelab/gpu-required: "NVIDIA RTX A5000 for LLM inference"
    security.homelab/network: "Internal only - ClusterIP service"
spec:
  predictor:
    # Scale-to-zero configuration (Knative Serving)
    minReplicas: 0  # Enable scale-to-zero (GPU freed when idle)
    maxReplicas: 1  # Single GPU workload - only 1 pod can run at a time

    # CRITICAL: GPU and Node Targeting
    # Target ONLY k8s-work-10 (RTX A5000 24GB)
    # Do NOT schedule on Plex node (rtx-a2000 reserved for transcoding)
    nodeSelector:
      kubernetes.io/hostname: k8s-work-10
      gpu.nvidia.com/model: rtx-a5000

    # Custom container approach (no ClusterServingRuntime needed)
    # Using official vLLM OpenAI-compatible image
    containers:
      - name: vllm
        # renovate: datasource=docker depName=vllm/vllm-openai
        image: vllm/vllm-openai:v0.11.0

        # vLLM server arguments
        args:
          # Model path (absolute path to cached model)
          - --model=/models/cognitivecomputations/Dolphin3.0-Llama3.1-8B

          # Load format optimization (safetensors, not mmap)
          - --load-format=safetensors

          # Inference optimization
          - --dtype=auto  # Use FP16/BF16 automatically
          - --max-model-len=8192  # Context window (8k tokens)
          - --gpu-memory-utilization=0.9  # Use 90% of VRAM efficiently

          # Fast cold start optimization
          # --enforce-eager skips torch.compile and CUDA graph capture
          - --enforce-eager

          # Inference optimizations
          - --enable-chunked-prefill  # 20-40% faster TTFT
          - --enable-prefix-caching  # 85% faster on repeated prompts

          # Server configuration
          - --host=0.0.0.0
          - --port=8080

          # Disable telemetry
          - --disable-log-stats
          - --disable-log-requests

        # Environment variables
        env:
          # HuggingFace token (optional, model is public)
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: huggingface-token
                key: HF_TOKEN
                optional: true

          # Disable HuggingFace telemetry
          - name: HF_HUB_DISABLE_TELEMETRY
            value: "1"

          # Enable offline mode (use cached model files only)
          - name: HF_HUB_OFFLINE
            value: "1"

          # NVIDIA GPU visibility
          - name: NVIDIA_VISIBLE_DEVICES
            value: "all"

          # Fix for vLLM v0.11.0 non-root user compatibility
          - name: HOME
            value: "/tmp"

        # Resource requirements
        resources:
          requests:
            cpu: 4
            memory: 16Gi
            nvidia.com/gpu: 1
          limits:
            cpu: 8
            memory: 20Gi
            nvidia.com/gpu: 1

        # Health checks
        ports:
          - containerPort: 8080
            name: http1  # Knative requires port name to be 'http1' or 'h2c'
            protocol: TCP

        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 6

        # Volume mounts
        volumeMounts:
          - name: model-cache
            mountPath: /models
            readOnly: false

        # Security context
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false  # vLLM needs writable /tmp
          capabilities:
            drop:
              - ALL

    # NVIDIA container runtime for GPU access
    runtimeClassName: nvidia

    # Security context (pod-level)
    securityContext:
      fsGroup: 1000
      seccompProfile:
        type: RuntimeDefault

    # Disable service account token mounting
    serviceAccountName: default

    # Volumes
    volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache

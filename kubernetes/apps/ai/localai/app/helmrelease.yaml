---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: localai
  namespace: ai
spec:
  chart:
    spec:
      chart: local-ai
      version: 3.4.2
      sourceRef:
        kind: HelmRepository
        name: go-skynet
        namespace: ai
  interval: 1h
  values:
    replicaCount: 1

    deployment:
      # NVIDIA runtime for GPU access (must be under deployment: per chart template)
      runtimeClassName: nvidia
      image:
        # GPU-enabled AIO image with CUDA 12 for RTX A5000 compatibility
        repository: localai/localai
        # renovate: datasource=docker depName=localai/localai
        tag: v3.8.0-aio-gpu-nvidia-cuda-12
      env:
        # Performance tuning for A5000 (24GB VRAM, 16 vCPU on host)
        threads: 8
        context_size: 4096
        # Model storage path
        modelsPath: "/models"
        # Disable debug in production
        debug: "false"
        # Enable parallel requests
        parallel_requests: "true"
        # CORS disabled - internal ClusterIP service only
        cors: "false"
        # VRAM Management: Idle watchdog auto-unloads models after inactivity
        # Models reload in ~7 seconds from Ceph RBD SSD storage
        LOCALAI_WATCHDOG_IDLE: "true"
        LOCALAI_WATCHDOG_IDLE_TIMEOUT: "15m"
        # NOTE: BUSY watchdog intentionally disabled - it cannot distinguish
        # between stuck processes and legitimately slow complex inference,
        # and will kill active reasoning tasks (see GitHub issue #5221)
      # API Key authentication (injected from 1Password via ExternalSecret)
      secretEnv:
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: localai-secret
              key: api-key

    resources:
      requests:
        cpu: 2
        memory: 8Gi
        nvidia.com/gpu: 1
      limits:
        cpu: 14
        memory: 48Gi
        # GPU resources must have equal request/limit per K8s extended resource spec
        nvidia.com/gpu: 1

    # Target k8s-work-10 which has the RTX A5000
    nodeSelector:
      kubernetes.io/hostname: k8s-work-10
      nvidia.com/gpu.present: "true"

    # Tolerate GPU nodes (if taints are applied)
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

    # Model storage - Ceph RBD PVCs (chart creates its own)
    persistence:
      models:
        enabled: true
        storageClass: ceph-block
        accessModes:
          - ReadWriteOnce
        size: 200Gi
      output:
        enabled: true
        storageClass: ceph-block
        accessModes:
          - ReadWriteOnce
        size: 10Gi

    # ClusterIP service for internal access only
    service:
      type: ClusterIP
      port: 8080

    # Health probes
    livenessProbe:
      httpGet:
        path: /readyz
        port: 8080
      initialDelaySeconds: 60
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 3

    readinessProbe:
      httpGet:
        path: /readyz
        port: 8080
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3

    # Disable ingress - internal access only via ClusterIP
    ingress:
      enabled: false

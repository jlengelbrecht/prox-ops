---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: litellm
  namespace: ai
spec:
  chart:
    spec:
      chart: litellm-helm
      # renovate: datasource=helm registryUrl=oci://ghcr.io/berriai
      version: 0.1.837
      sourceRef:
        kind: HelmRepository
        name: litellm
        namespace: ai
  interval: 1h
  # Depend on PostgreSQL to be ready before LiteLLM
  dependsOn:
    - name: litellm-postgres
      namespace: ai
  values:
    # Image configuration - uses litellm-database (debian-based) for Prisma compatibility
    image:
      repository: ghcr.io/berriai/litellm-database
      pullPolicy: Always
      # renovate: datasource=docker depName=ghcr.io/berriai/litellm-database
      tag: main-v1.81.0-stable

    # Replica count
    replicaCount: 1

    # Master key from ExternalSecret
    masterkeySecretName: litellm-masterkey
    masterkeySecretKey: masterkey

    # Database configuration - use existing PostgreSQL
    db:
      useExisting: true
      deployStandalone: false
      endpoint: litellm-postgres.ai.svc.cluster.local
      database: litellm
      secret:
        name: litellm-db-credentials
        usernameKey: username
        passwordKey: password

    # Use existing ConfigMap for proxy configuration
    proxyConfigMap:
      create: false
      name: litellm-config
      key: config.yaml

    # Pod annotations
    podAnnotations:
      security.homelab/network: "Internal + external API via envoy-internal"
      security.homelab/purpose: "OpenAI-compatible API gateway for Ollama"
      # Restart pods when any ExternalSecret-backed secret is updated
      secret.reloader.stakater.com/reload: "litellm-masterkey,litellm-oidc,litellm-openai,litellm-graphiti-mcp,litellm-n8n-mcp,firecrawl-secrets,litellm-valkey,litellm-lakera,litellm-anthropic"

    # Pod security context
    # Note: litellm-database image specifies USER as text "root" in Dockerfile,
    # which prevents Kubernetes from verifying runAsNonRoot. The image uses
    # supervisord to manage processes and runs as an internal non-root user.
    # See: https://github.com/BerriAI/litellm/blob/main/docker/litellm-database/Dockerfile
    podSecurityContext:
      seccompProfile:
        type: RuntimeDefault

    # Container security context
    # Note: readOnlyRootFilesystem=false required for Prisma + supervisord
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: false
      capabilities:
        drop:
          - ALL

    # Resource limits
    resources:
      requests:
        cpu: 200m
        memory: 256Mi
      limits:
        cpu: 1
        memory: 1Gi

    # Service configuration
    service:
      type: ClusterIP
      port: 4000

    # Separate health app for more reliable probes
    separateHealthApp: true
    separateHealthPort: 8081

    # Migration job settings
    migrationJob:
      enabled: true
      retries: 3
      backoffLimit: 4
      disableSchemaUpdate: false
      ttlSecondsAfterFinished: 120
      hooks:
        # Enable Helm hooks for migration job orchestration during install/upgrade
        argocd:
          enabled: false
        helm:
          enabled: true

    # Disable internal PostgreSQL (we use our own)
    postgresql:
      enabled: false

    # Disable Redis (not needed for single-node)
    redis:
      enabled: false

    # Environment variables
    envVars:
      LITELLM_TELEMETRY: "false"
      LITELLM_LOG: "INFO"
      # Enable database storage for UI settings (logo, themes, etc.)
      STORE_MODEL_IN_DB: "True"

    # Extra environment variables from secrets
    # LiteLLM UI requires LITELLM_MASTER_KEY (chart only sets PROXY_MASTER_KEY)
    extraEnvVars:
      - name: LITELLM_MASTER_KEY
        valueFrom:
          secretKeyRef:
            name: litellm-masterkey
            key: masterkey
      # SSO/OIDC Configuration - Authentik Generic Provider
      # Use email as user_id (Authentik returns 'preferred_username' by default)
      - name: GENERIC_USER_ID_ATTRIBUTE
        value: "email"
      - name: GENERIC_CLIENT_ID
        valueFrom:
          secretKeyRef:
            name: litellm-oidc
            key: GENERIC_CLIENT_ID
      - name: GENERIC_CLIENT_SECRET
        valueFrom:
          secretKeyRef:
            name: litellm-oidc
            key: GENERIC_CLIENT_SECRET
      - name: GENERIC_AUTHORIZATION_ENDPOINT
        value: "https://authentik.homelab0.org/application/o/authorize/"
      - name: GENERIC_TOKEN_ENDPOINT
        value: "https://authentik.homelab0.org/application/o/token/"
      - name: GENERIC_USERINFO_ENDPOINT
        value: "https://authentik.homelab0.org/application/o/userinfo/"
      - name: PROXY_BASE_URL
        value: "https://ai-api.homelab0.org"
      # Admin user - gets proxy_admin role on SSO login (stored in 1Password)
      - name: PROXY_ADMIN_ID
        valueFrom:
          secretKeyRef:
            name: litellm-oidc
            key: PROXY_ADMIN_ID
      # Disable basic auth login (SSO only)
      - name: UI_USERNAME
        value: ""
      - name: UI_PASSWORD
        value: ""
      # OpenAI API key for external LLM fallback
      # Used when local Ollama is overloaded or for faster inference
      - name: OPENAI_API_KEY
        valueFrom:
          secretKeyRef:
            name: litellm-openai
            key: api-key
      # Graphiti MCP auth token for MCP gateway
      # Used by mcp_servers.graphiti in config.yaml
      - name: GRAPHITI_MCP_TOKEN
        valueFrom:
          secretKeyRef:
            name: litellm-graphiti-mcp
            key: token
      # n8n MCP auth token for MCP gateway
      # Used by mcp_servers.n8n in config.yaml
      - name: N8N_MCP_TOKEN
        valueFrom:
          secretKeyRef:
            name: litellm-n8n-mcp
            key: token
      # STORY-059: Firecrawl API key for search tools
      # Used by search_tools.firecrawl-search in config.yaml
      - name: FIRECRAWL_API_KEY
        valueFrom:
          secretKeyRef:
            name: firecrawl-secrets
            key: api-key
      # STORY-060: Valkey/Redis configuration for response caching
      # Used by litellm_settings.cache_params in config.yaml
      - name: REDIS_HOST
        value: "valkey.cache.svc.cluster.local"
      - name: REDIS_PORT
        value: "6379"
      - name: REDIS_PASSWORD
        valueFrom:
          secretKeyRef:
            name: litellm-valkey
            key: password
      # EPIC-022: Lakera AI API key for UI-managed guardrails
      # Used for prompt injection detection via database-managed guardrails
      - name: LAKERA_API_KEY
        valueFrom:
          secretKeyRef:
            name: litellm-lakera
            key: api-key
      # STORY-063: Anthropic OAuth token for Claude Max subscription
      # Used by custom callback to inject Authorization: Bearer header
      # NOT set as ANTHROPIC_API_KEY to avoid x-api-key header (OAuth uses Bearer)
      - name: ANTHROPIC_OAUTH_TOKEN
        valueFrom:
          secretKeyRef:
            name: litellm-anthropic
            key: api-key
      # STORY-063: Add custom callback directory to Python path
      - name: PYTHONPATH
        value: "/app/custom_callbacks:${PYTHONPATH}"

    # STORY-063: Mount custom OAuth callback for Anthropic
    extraVolumes:
      - name: oauth-callback
        configMap:
          name: litellm-oauth-callback

    extraVolumeMounts:
      - name: oauth-callback
        mountPath: /app/custom_callbacks
        readOnly: true

---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: gha-runner-scale-set
  namespace: github-actions
spec:
  chartRef:
    kind: OCIRepository
    name: gha-runner-scale-set
  interval: 1h
  dependsOn:
    - name: gha-runner-scale-set-controller
      namespace: github-actions
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    # Runner scale set name (used in runs-on: [self-hosted, cattle-upgrade])
    runnerScaleSetName: "cattle-upgrade"

    # GitHub configuration
    githubConfigUrl: "https://github.com/jlengelbrecht/prox-ops"

    # GitHub App authentication (secret created by ExternalSecret)
    githubConfigSecret: github-runner-registration

    # CRITICAL FIX: Dynamic scaling instead of pre-provisioned runners
    # Previous config (minRunners: 3) created unusable pre-provisioned runners
    # that couldn't acquire jobs due to ARC's ephemeral architecture
    minRunners: 0   # Don't pre-provision - create on-demand
    maxRunners: 10  # Allow scaling for parallel jobs

    # Runner template
    template:
      spec:
        # Service account with kubectl/cluster access
        serviceAccountName: github-actions-runner

        # Pod-level security context
        securityContext:
          runAsNonRoot: true
          runAsUser: 1001
          fsGroup: 1001
          seccompProfile:
            type: RuntimeDefault

        # Container spec
        containers:
          - name: runner
            image: ghcr.io/actions/actions-runner:2.329.0
            imagePullPolicy: IfNotPresent

            # Resource allocation
            resources:
              requests:
                cpu: 500m
                memory: 1Gi
              limits:
                cpu: 2000m
                memory: 4Gi

            # Security context (container-level)
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                  - ALL

            # Add pre-stop hook for graceful shutdown during node drain
            lifecycle:
              preStop:
                exec:
                  command:
                    - /bin/sh
                    - -c
                    - |
                      # Give running job time to checkpoint/save state
                      echo "PreStop hook: Graceful shutdown initiated"
                      sleep 25

            # Volume mounts for tools
            volumeMounts:
              - name: work
                mountPath: /home/runner/_work

        # Volumes
        volumes:
          - name: work
            emptyDir: {}

        # Graceful termination
        terminationGracePeriodSeconds: 30

        # Affinity rules for spreading runners across nodes
        affinity:
          # Node affinity: prefer worker nodes, avoid control plane
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                preference:
                  matchExpressions:
                    - key: node-role.kubernetes.io/control-plane
                      operator: DoesNotExist

          # Pod anti-affinity: PREFERRED (not required) to spread runners
          # Changed from required to preferred for better scheduling flexibility
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: actions.github.com/scale-set-name
                        operator: In
                        values:
                          - cattle-upgrade
                  topologyKey: kubernetes.io/hostname

        # Tolerations (avoid GPU nodes unless necessary)
        tolerations: []

    # Controller service account
    controllerServiceAccount:
      name: gha-runner-scale-set-controller
      namespace: github-actions

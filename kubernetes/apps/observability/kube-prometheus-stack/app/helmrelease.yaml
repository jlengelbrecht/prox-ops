---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: observability
spec:
  chart:
    spec:
      chart: kube-prometheus-stack
      # renovate: datasource=helm registryUrl=https://prometheus-community.github.io/helm-charts
      version: 82.1.1
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: observability
  interval: 1h
  timeout: 15m
  install:
    remediation:
      retries: 3
    crds: CreateReplace
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
    crds: CreateReplace
  values:
    # Prometheus configuration
    prometheus:
      enabled: true
      prometheusSpec:
        # Data retention
        retention: 30d
        retentionSize: "90GB"

        # Resource limits
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 4Gi

        # Startup probe - allow extended time for WAL replay after downtime
        # WAL replay can take up to 40 minutes with 548 segments
        # Chart default periodSeconds is 15s, so 240 failures = 60 minutes max startup
        startupProbe:
          failureThreshold: 240  # 240 * 15s (chart default) = 60 min max startup

        # Storage configuration
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: ceph-block
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 100Gi

        # Service monitors
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false

        # Additional alert rules from ConfigMaps
        additionalPrometheusRulesSelector:
          matchLabels:
            prometheus: kube-prometheus-stack
            role: alert-rules

        # Enable remote write receiver (for future use)
        enableRemoteWriteReceiver: false

        # Enable feature flags
        enableFeatures: []

        # Scrape configs
        additionalScrapeConfigs: []

        # Pod configuration
        podMetadata:
          labels:
            app: prometheus

        # Replicas (for homelab, single replica is sufficient)
        replicas: 1

        # WAL compression
        walCompression: true

    # Grafana configuration
    grafana:
      enabled: true

      # Disable default datasource provisioning from chart
      forceDeployDatasources: false

      # Admin credentials from secret
      admin:
        existingSecret: grafana-admin
        userKey: admin-user
        passwordKey: admin-password

      # Persistence
      persistence:
        enabled: true
        type: pvc
        storageClassName: ceph-block
        accessModes:
          - ReadWriteOnce
        size: 10Gi

      # Service configuration for LoadBalancer
      service:
        type: LoadBalancer
        port: 80
        targetPort: 3000
        annotations:
          io.cilium/lb-ipam-ips: "10.20.67.24"

      # Ingress disabled (using LoadBalancer)
      ingress:
        enabled: false

      # Resource limits
      resources:
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 500m
          memory: 512Mi

      # Deployment strategy - use Recreate for RWO volumes
      deploymentStrategy:
        type: Recreate

      # Grafana configuration
      grafana.ini:
        server:
          domain: grafana.homelab0.org
          root_url: "http://grafana.homelab0.org"
        analytics:
          check_for_updates: false
          reporting_enabled: false
        log:
          mode: console
        paths:
          data: /var/lib/grafana/
          logs: /var/log/grafana
          plugins: /var/lib/grafana/plugins
          provisioning: /etc/grafana/provisioning

      # Default dashboards
      defaultDashboardsEnabled: true
      defaultDashboardsTimezone: UTC

      # Datasources - manually provision since we disabled chart defaults
      datasources:
        datasources.yaml:
          apiVersion: 1
          datasources:
            - name: Prometheus
              type: prometheus
              url: http://kube-prometheus-stack-prometheus.observability.svc.cluster.local:9090
              access: proxy
              isDefault: true
              jsonData:
                timeInterval: 30s
            - name: Loki
              type: loki
              url: http://loki:3100
              access: proxy
              isDefault: false
              # Disable alert rule management - Loki is for logs, not Prometheus-style metrics
              editable: false
              jsonData:
                maxLines: 1000
                # Disable alerting UI - Loki uses LogQL, not PromQL for alerts
                manageAlerts: false
                derivedFields:
                  - datasourceUid: Prometheus
                    matcherRegex: "traceID=(\\w+)"
                    name: TraceID
                    url: "$${__value.raw}"

      # Dashboard providers for external dashboards (gnetId)
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: 'unpoller'
              orgId: 1
              folder: 'UniFi'
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/unpoller

      # UnPoller dashboards from grafana.com
      dashboards:
        unpoller:
          unpoller-client-dpi:
            gnetId: 11310
            revision: 9
            datasource: Prometheus
          unpoller-sites:
            gnetId: 11311
            revision: 5
            datasource: Prometheus
          unpoller-usw:
            gnetId: 11312
            revision: 9
            datasource: Prometheus
          unpoller-usg:
            gnetId: 11313
            revision: 9
            datasource: Prometheus
          unpoller-uap:
            gnetId: 11314
            revision: 10
            datasource: Prometheus
          unpoller-clients:
            gnetId: 11315
            revision: 9
            datasource: Prometheus
          unpoller-network-sites:
            gnetId: 22143
            revision: 2
            datasource: Prometheus
          # PDU dashboard uses ConfigMap (dashboards/unpoller-pdu-dashboard.yaml)
          # due to datasource variable format incompatibility with gnetId provisioning

      # Sidecar for dashboard discovery
      sidecar:
        dashboards:
          enabled: true
          defaultFolderName: "General"
          label: grafana_dashboard
          labelValue: "1"
          folderAnnotation: grafana_folder
          searchNamespace: ALL
          provider:
            foldersFromFilesStructure: true
        datasources:
          enabled: false

    # AlertManager configuration
    alertmanager:
      enabled: true
      alertmanagerSpec:
        # Resource limits
        resources:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi

        # Storage configuration
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: ceph-block
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 5Gi

        # Replicas
        replicas: 1

        # Retention
        retention: 168h  # 7 days

        # Pod configuration
        podMetadata:
          labels:
            app: alertmanager

      # AlertManager routing configuration
      # Minimal config - configure receivers via AlertManager UI
      config:
        global:
          resolve_timeout: 5m

        route:
          receiver: 'null'
          group_by: ['alertname']
          group_wait: 10s
          group_interval: 10s
          repeat_interval: 1h

        receivers:
          - name: 'null'

    # Prometheus Operator configuration
    prometheusOperator:
      enabled: true

      # Resource limits
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 256Mi

      # Prometheus config reloader
      prometheusConfigReloader:
        resources:
          requests:
            cpu: 10m
            memory: 32Mi
          limits:
            cpu: 200m
            memory: 64Mi

    # Kube State Metrics
    kubeStateMetrics:
      enabled: true

    # Node Exporter
    nodeExporter:
      enabled: true

      # Resource limits
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 128Mi

    # Default rules
    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: false  # Disabled - Talos runs etcd on host, metrics not exposed
        configReloaders: true
        general: true
        k8s: true
        kubeApiserverAvailability: true
        kubeApiserverSlos: true
        kubeControllerManager: true
        kubelet: true
        kubeProxy: false  # Disabled - Talos uses Cilium which replaces kube-proxy
        kubePrometheusGeneral: true
        kubePrometheusNodeRecording: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeSchedulerAlerting: true
        kubeSchedulerRecording: true
        kubeStateMetrics: true
        network: true
        node: true
        nodeExporterAlerting: true
        nodeExporterRecording: true
        prometheus: true
        prometheusOperator: true

    # Global configuration
    global:
      rbac:
        create: true
        createAggregateClusterRoles: true

    # Disable etcd monitoring - Talos runs etcd on host, metrics not exposed to Kubernetes
    kubeEtcd:
      enabled: false

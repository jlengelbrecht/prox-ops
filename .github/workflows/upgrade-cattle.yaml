---
name: Cattle Upgrade - Complete Workflow

on:
  workflow_call:
    inputs:
      old_version:
        description: Previous Talos version
        type: string
        required: true
      new_version:
        description: New Talos version to upgrade to
        type: string
        required: true
      test_mode:
        description: Test mode (only 2 workers, skip control plane)
        type: boolean
        default: true
  workflow_dispatch:
    inputs:
      old_version:
        description: Previous Talos version
        type: string
        required: true
      new_version:
        description: New Talos version to upgrade to
        type: string
        required: true
      test_mode:
        description: Test mode (only 2 workers, skip control plane)
        type: boolean
        default: true

# Prevent concurrent cattle upgrades (Terraform state lock protection)
# Cancel old queued runs when new run starts (prevents zombie workflow blocking)
concurrency:
  group: cattle-upgrade-v2
  cancel-in-progress: true

env:
  AWS_REGION: us-east-1
  TERRAFORM_STATE_BUCKET: prox-ops-terraform-state
  TERRAFORM_VERSION: "latest"

jobs:
  # ==========================================================================
  # PHASE 1: TEMPLATE REBUILD (CRITICAL - MISSING IN PREVIOUS IMPLEMENTATION)
  # ==========================================================================
  rebuild-templates:
    name: Rebuild Templates v${{ inputs.new_version }}
    runs-on: cattle-runner
    timeout-minutes: 60
    # Skip template rebuild if versions are identical (prevents VM destruction due to implicit clone dependencies)
    if: inputs.old_version != inputs.new_version
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Validate version inputs
        run: |
          if ! [[ "${{ inputs.old_version }}" =~ ^[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
            echo "::error::Invalid old_version format: '${{ inputs.old_version }}'"
            exit 1
          fi
          if ! [[ "${{ inputs.new_version }}" =~ ^[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
            echo "::error::Invalid new_version format: '${{ inputs.new_version }}'"
            exit 1
          fi
          echo "✅ Version inputs validated"

      - name: Setup Node.js (required for Terraform action)
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Proxmox SSH key
        env:
          PROXMOX_SSH_KEY: ${{ secrets.PROXMOX_SSH_KEY }}
        run: |
          mkdir -p ~/.ssh
          printf '%s\n' "$PROXMOX_SSH_KEY" > ~/.ssh/proxmox_terraform
          chmod 600 ~/.ssh/proxmox_terraform

      - name: Initialize Terraform
        working-directory: ./terraform
        run: terraform init

      - name: Clear stale Terraform locks
        working-directory: ./terraform
        continue-on-error: true
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "Checking for stale Terraform state locks..."

          # Try a terraform plan with short lock timeout to detect locks
          # Capture both stdout and stderr to parse lock ID
          if ! LOCK_OUTPUT=$(terraform plan -input=false -lock-timeout=1s 2>&1); then
            echo "Lock detected. Output:"
            echo "$LOCK_OUTPUT"

            # Extract lock ID from error message (format: "ID: <uuid>")
            LOCK_ID=$(echo "$LOCK_OUTPUT" | grep -oP '(?<=ID:\s{8})[a-f0-9-]{36}' | head -1)

            if [ -n "$LOCK_ID" ]; then
              echo ""
              echo "Found stale lock ID: $LOCK_ID"
              echo "Attempting to force-unlock..."

              if terraform force-unlock -force "$LOCK_ID"; then
                echo "✓ Successfully cleared stale lock"
              else
                echo "✗ Failed to unlock (may require manual intervention)"
                echo "  Manual command: terraform force-unlock -force $LOCK_ID"
              fi
            else
              echo "Could not extract lock ID from error message"
              echo "Lock may need to be cleared manually"
            fi
          else
            echo "✓ No lock detected - state is accessible"
          fi

      - name: Plan template changes (SAFETY MODE - NO EXECUTION)
        working-directory: ./terraform
        timeout-minutes: 15
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Terraform Plan for Template Rebuild (SAFETY MODE)"
          echo "================================================================"
          echo "SAFETY MODE ENABLED: This workflow will ONLY show what Terraform"
          echo "WOULD do when targeting templates. It will NOT execute any changes."
          echo ""
          echo "This is to identify the bug causing Terraform to destroy VMs"
          echo "instead of just rebuilding templates."
          echo "================================================================"
          echo ""
          echo "Running terraform plan for 8 template modules..."
          echo ""

          # SAFETY: Run plan only, no apply/destroy
          terraform plan \
            -input=false \
            -out=template-rebuild.tfplan \
            -target=module.template_baldar_controller \
            -target=module.template_baldar_worker \
            -target=module.template_heimdall_controller \
            -target=module.template_heimdall_worker \
            -target=module.template_odin_controller \
            -target=module.template_odin_worker \
            -target=module.template_thor_controller \
            -target=module.template_thor_worker

          echo ""
          echo "================================================================"
          echo "Plan completed. Review the output above to identify:"
          echo "  1. What resources Terraform wants to destroy"
          echo "  2. What resources Terraform wants to create"
          echo "  3. Whether VM modules are incorrectly included"
          echo ""
          echo "Expected: Only template_* modules should be affected"
          echo "BUG: If worker_nodes or control_plane_nodes appear, the bug exists"
          echo "================================================================"
          echo "::endgroup::"

          # Show plan summary
          echo ""
          echo "::group::Plan Summary"
          terraform show -no-color template-rebuild.tfplan | head -100
          echo "::endgroup::"

      - name: Analyze plan output for bugs
        working-directory: ./terraform
        run: |
          echo "::group::Plan Analysis - Identifying Terraform Bug"
          echo "Analyzing terraform plan output to identify unintended targets..."
          echo ""

          # Check if plan file exists
          if [[ ! -f template-rebuild.tfplan ]]; then
            echo "::error::Plan file not found - terraform plan may have failed"
            exit 1
          fi

          # Show full plan in readable format
          echo "Full plan output:"
          terraform show -no-color template-rebuild.tfplan > plan-full.txt
          cat plan-full.txt

          echo ""
          echo "================================================================"
          echo "BUG DETECTION ANALYSIS"
          echo "================================================================"

          # Check for unintended VM module targets
          echo ""
          echo "Checking for unintended VM modules in plan..."

          if grep -q "module.control_plane_nodes" plan-full.txt; then
            echo "::error::BUG DETECTED: control_plane_nodes module found in plan"
            echo "::error::This would destroy control plane VMs!"
            echo ""
            grep "module.control_plane_nodes" plan-full.txt
            echo ""
          else
            echo "✅ No control_plane_nodes in plan"
          fi

          if grep -q "module.worker_nodes" plan-full.txt; then
            echo "::error::BUG DETECTED: worker_nodes module found in plan"
            echo "::error::This would destroy worker VMs!"
            echo ""
            grep "module.worker_nodes" plan-full.txt
            echo ""
          else
            echo "✅ No worker_nodes in plan"
          fi

          # Show resources to be destroyed
          echo ""
          echo "Resources Terraform plans to DESTROY:"
          grep -A 5 "# .* will be destroyed" plan-full.txt | head -50 || echo "None"

          # Show resources to be created
          echo ""
          echo "Resources Terraform plans to CREATE:"
          grep -A 5 "# .* will be created" plan-full.txt | head -50 || echo "None"

          echo ""
          echo "================================================================"
          echo "SAFETY STATUS: No changes executed (plan-only mode)"
          echo "================================================================"
          echo "::endgroup::"

      - name: Template rebuild summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY <<EOF
          ## Phase 1: Template Rebuild ✅

          **Status**: All templates successfully rebuilt

          ### Templates Created
          - Baldar: controller + worker
          - Heimdall: controller + worker
          - Odin: controller + worker
          - Thor: controller + worker

          **Total**: 8 templates
          **Version**: v${{ inputs.new_version }}

          VMs created from these templates will run Talos **v${{ inputs.new_version }}**.
          EOF

  # ==========================================================================
  # PHASE 2: CONTROL PLANE UPGRADE (SEQUENTIAL)
  # ==========================================================================
  upgrade-control-plane:
    name: Upgrade Control ${{ matrix.node.name }}
    needs: rebuild-templates
    runs-on: cattle-runner
    timeout-minutes: 30
    if: inputs.test_mode == false
    strategy:
      max-parallel: 1
      fail-fast: true
      matrix:
        node:
          - name: "k8s-ctrl-1"
            ip: "10.20.67.1"
            secret_suffix: "CTRL_1"
          - name: "k8s-ctrl-2"
            ip: "10.20.67.2"
            secret_suffix: "CTRL_2"
          - name: "k8s-ctrl-3"
            ip: "10.20.67.3"
            secret_suffix: "CTRL_3"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Proxmox SSH key
        env:
          PROXMOX_SSH_KEY: ${{ secrets.PROXMOX_SSH_KEY }}
        run: |
          mkdir -p ~/.ssh
          printf '%s\n' "$PROXMOX_SSH_KEY" > ~/.ssh/proxmox_terraform
          chmod 600 ~/.ssh/proxmox_terraform

      - name: Initialize Terraform
        working-directory: ./terraform
        run: terraform init

      - name: Validate etcd quorum health
        run: |
          echo "::group::Checking etcd quorum health"
          echo "Ensuring cluster has healthy quorum before upgrading ${{ matrix.node.name }}..."

          # Wait up to 2 minutes for etcd to be healthy
          for i in {1..12}; do
            # Find any healthy etcd pod
            ETCD_POD=$(kubectl get pod -n kube-system -l component=etcd \
              --field-selector=status.phase=Running -o name 2>/dev/null | head -n1)

            if [[ -z "$ETCD_POD" ]]; then
              echo "::error::No healthy etcd pods found"
              exit 1
            fi

            HEALTHY_MEMBERS=$(kubectl exec -n kube-system "$ETCD_POD" -- \
              etcdctl member list 2>/dev/null | grep started | wc -l || echo "0")

            echo "Attempt $i/12: $HEALTHY_MEMBERS healthy etcd members"

            # SECURITY FIX: Require at least 2 healthy members BEFORE destroying ANY node
            if [[ "$HEALTHY_MEMBERS" -ge 2 ]]; then
              echo "✅ etcd quorum healthy ($HEALTHY_MEMBERS members)"
              echo "Safe to upgrade ${{ matrix.node.name }} - quorum will survive with 2 members"
              echo "::endgroup::"
              exit 0
            fi

            sleep 10
          done

          echo "::error::Cannot proceed: etcd quorum at risk"
          echo "::error::Only $HEALTHY_MEMBERS healthy members (need at least 2 before destroying any node)"
          exit 1

      - name: Cordon node
        run: |
          echo "Cordoning ${{ matrix.node.name }}..."
          kubectl cordon "${{ matrix.node.name }}"

      - name: Drain node
        timeout-minutes: 5
        run: |
          echo "::group::Draining ${{ matrix.node.name }}"
          echo "Attempting graceful drain (respects PodDisruptionBudget)..."

          # Layer 3: Graceful drain with timeout + force drain fallback
          # Try graceful drain first (respects PDB)
          if timeout 240 kubectl drain "${{ matrix.node.name }}" \
            --ignore-daemonsets \
            --delete-emptydir-data \
            --grace-period=30; then

            echo "✅ Graceful drain succeeded"
          else
            echo "⚠️  Graceful drain timed out (likely PDB blocking runner eviction)"
            echo "Falling back to force drain..."

            # Force drain bypasses PDB (used when runner pod is on this node)
            kubectl drain "${{ matrix.node.name }}" \
              --ignore-daemonsets \
              --delete-emptydir-data \
              --disable-eviction \
              --force \
              --grace-period=0

            echo "✅ Force drain completed"
            echo "ℹ️  If runner pod was evicted, GitHub will re-queue job to surviving runner"

            # Give GitHub Actions time to detect runner termination and re-queue
            sleep 30
          fi

          echo "✅ Node drained successfully"
          echo "::endgroup::"

      - name: Backup Terraform state
        working-directory: ./terraform
        run: |
          echo "Creating state backup before destroy..."

          # SECURITY: Check for state lock before proceeding
          if aws s3 ls s3://${{ env.TERRAFORM_STATE_BUCKET }}/.terraform.tfstate.lock.info 2>/dev/null; then
            echo "::error::Terraform state is locked - another operation in progress"
            echo "::error::Wait for other operation to complete or manually remove lock if stale"
            exit 1
          fi

          aws s3 cp s3://${{ env.TERRAFORM_STATE_BUCKET }}/terraform.tfstate \
            s3://${{ env.TERRAFORM_STATE_BUCKET }}/backups/pre-upgrade-${{ matrix.node.name }}-$(date +%s).tfstate

      - name: Destroy VM
        working-directory: ./terraform
        timeout-minutes: 5
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Destroying ${{ matrix.node.name }} VM"
          terraform destroy \
            -input=false \
            -auto-approve \
            -target=module.control_plane_nodes[\"${{ matrix.node.name }}\"]

          echo "✅ VM destroyed"
          echo "::endgroup::"

      - name: Recreate VM from new template
        working-directory: ./terraform
        timeout-minutes: 10
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Recreating ${{ matrix.node.name }} from v${{ inputs.new_version }} template"
          terraform apply \
            -input=false \
            -auto-approve \
            -target=module.control_plane_nodes[\"${{ matrix.node.name }}\"]

          echo "✅ VM recreated from new template"
          echo "::endgroup::"

      # ==========================================================================
      # CRITICAL SECURITY OPERATION: Apply Machine Configuration
      # This step implements the 5-layer security architecture to inject
      # Talos machine configs from GitHub Secrets without leaking secrets
      # ==========================================================================
      - name: Apply machine configuration (CRITICAL SECURITY OPERATION)
        id: apply_config
        env:
          NODE_IP: ${{ matrix.node.ip }}
          NODE_NAME: ${{ matrix.node.name }}
          # SECURITY: Secret passed directly to script, not stored in env var
          NODE_CONFIG_SECRET: ${{ secrets[format('TALOS_MACHINE_CONFIG_{0}', matrix.node.secret_suffix)] }}
        shell: bash
        timeout-minutes: 5
        run: |
          # CRITICAL: Absolutely no command echo
          set +x
          set -euo pipefail

          echo "Starting secure configuration deployment for $NODE_NAME..."

          # ============================================================
          # SECURITY LAYER 1: Disable Command Echo (set +x above)
          # ============================================================

          # ============================================================
          # SECURITY LAYER 2: Ephemeral Secure Storage
          # ============================================================

          # Create dedicated config directory with maximum restrictions
          CONFIG_DIR="$(mktemp -d -t talos-config-XXXXXX)"
          chmod 700 "$CONFIG_DIR"

          # Set aggressive cleanup trap
          cleanup_config() {
            if [[ -d "$CONFIG_DIR" ]]; then
              find "$CONFIG_DIR" -type f -exec shred -fz -n 3 {} \; 2>/dev/null || true
              rm -rf "$CONFIG_DIR"
            fi
          }
          trap cleanup_config EXIT

          # ============================================================
          # SECURITY LAYER 3: Decode and Protect
          # ============================================================

          # Decode configuration with maximum protection and error handling
          CONFIG_FILE="$CONFIG_DIR/machine.yaml"

          # Validate secret exists and is not empty
          if [[ -z "$NODE_CONFIG_SECRET" ]]; then
            echo "ERROR: Machine config secret TALOS_MACHINE_CONFIG_${{ matrix.node.secret_suffix }} is not set"
            exit 1
          fi

          if ! echo "$NODE_CONFIG_SECRET" | base64 -d > "$CONFIG_FILE" 2>/dev/null; then
            echo "ERROR: Failed to decode configuration"
            exit 1
          fi
          chmod 600 "$CONFIG_FILE"

          # Validate configuration has expected structure with error suppression
          if ! grep -q "machine:" "$CONFIG_FILE" 2>/dev/null; then
            echo "ERROR: Invalid machine configuration structure"
            exit 1
          fi

          echo "Configuration decoded and validated"

          # ============================================================
          # SECURITY LAYER 4: Apply with Maximum Output Suppression
          # ============================================================

          echo "Applying configuration to node $NODE_IP..."

          # Wait for VM to be network-ready (VM just created)
          echo "Waiting for VM to be network-ready..."
          MAX_WAIT=60
          for i in {1..12}; do
            if talosctl -n "$NODE_IP" version --insecure --timeout=5s >/dev/null 2>&1; then
              echo "✓ VM is network-ready after $((i * 5)) seconds"
              break
            fi
            echo "  Waiting... ($((i * 5))/${MAX_WAIT}s)"
            sleep 5
          done

          # Application with retry logic and complete output suppression
          MAX_ATTEMPTS=3
          ATTEMPT=1
          SUCCESS=false

          while [[ $ATTEMPT -le $MAX_ATTEMPTS ]]; do
            echo "  Attempt $ATTEMPT of $MAX_ATTEMPTS..."

            # Create a log file for this attempt with restrictive permissions
            ATTEMPT_LOG="$CONFIG_DIR/attempt_${ATTEMPT}.log"
            touch "$ATTEMPT_LOG"
            chmod 600 "$ATTEMPT_LOG"

            # Apply configuration with all output captured
            if talosctl -n "$NODE_IP" apply-config \
                --insecure \
                --file "$CONFIG_FILE" \
                --timeout=120s \
                >"$ATTEMPT_LOG" 2>&1; then

              # CRITICAL: Sanitize log IMMEDIATELY even on success
              sed -i 's/token:.*/token: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/secret:.*/secret: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/certificate:.*/certificate: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN .* PRIVATE KEY-----.*/[PRIVATE KEY REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN CERTIFICATE-----.*/[CERTIFICATE REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true

              echo "  ✓ Configuration applied successfully"
              SUCCESS=true

              # Shred sanitized log (non-verbose to avoid path leaks)
              shred -fz -n 3 "$ATTEMPT_LOG" 2>/dev/null || true
              break

            else
              # CRITICAL: Sanitize log BEFORE checking error types
              sed -i 's/token:.*/token: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/secret:.*/secret: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/certificate:.*/certificate: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN .* PRIVATE KEY-----.*/[PRIVATE KEY REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN CERTIFICATE-----.*/[CERTIFICATE REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true

              echo "  ⚠ Attempt $ATTEMPT failed"

              # Now safe to check error types from sanitized log
              if grep -q "connection refused" "$ATTEMPT_LOG" 2>/dev/null; then
                echo "    Node not yet ready, waiting..."
                sleep $((ATTEMPT * 10))
              elif grep -q "timeout" "$ATTEMPT_LOG" 2>/dev/null; then
                echo "    Operation timed out, retrying..."
                sleep 5
              else
                echo "    Unknown error occurred"
              fi

              # Shred sanitized log (non-verbose to avoid path leaks)
              shred -fz -n 3 "$ATTEMPT_LOG" 2>/dev/null || true
            fi

            ((ATTEMPT++))
          done

          if [[ "$SUCCESS" != "true" ]]; then
            echo "ERROR: Failed to apply configuration after $MAX_ATTEMPTS attempts"
            exit 1
          fi

          # ============================================================
          # SECURITY LAYER 5: Aggressive Cleanup
          # ============================================================

          # Cleanup happens automatically via EXIT trap
          echo "Configuration deployment completed securely"

      - name: Wait for node to be Ready
        timeout-minutes: 10
        run: |
          echo "Waiting for ${{ matrix.node.name }} to become Ready..."

          for i in {1..60}; do
            if kubectl wait --for=condition=Ready node/"${{ matrix.node.name }}" --timeout=10s 2>/dev/null; then
              echo "✅ Node is Ready"
              break
            fi

            # Show node status every minute (every 6 attempts)
            if [[ $((i % 6)) -eq 0 ]]; then
              echo "Node status after $i attempts:"
              kubectl get node "${{ matrix.node.name }}" || true
              kubectl describe node "${{ matrix.node.name }}" | tail -20 || true
            fi

            echo "Attempt $i/60: Node not ready yet..."
            sleep 10
          done

          # Final check
          kubectl wait --for=condition=Ready node/"${{ matrix.node.name }}" --timeout=30s

      - name: Verify version
        run: |
          NODE_VERSION=$(kubectl get node "${{ matrix.node.name }}" -o jsonpath='{.status.nodeInfo.kubeletVersion}')
          echo "Node version: $NODE_VERSION"

          if [[ "$NODE_VERSION" != *"${{ inputs.new_version }}"* ]]; then
            echo "::error::Version mismatch: expected v${{ inputs.new_version }}, got $NODE_VERSION"
            exit 1
          fi

          echo "✅ Node running correct version: $NODE_VERSION"

      - name: Uncordon node
        run: |
          echo "Uncordoning ${{ matrix.node.name }}..."
          kubectl uncordon "${{ matrix.node.name }}"

      - name: Upgrade summary
        if: always()
        run: |
          # Store GHA expressions in bash variables first
          NODE_NAME="${{ matrix.node.name }}"
          NEW_VERSION="${{ inputs.new_version }}"
          JOB_STATUS="${{ job.status }}"

          # Determine status emoji and message based on job status
          if [[ "$JOB_STATUS" == "success" ]]; then
            STATUS_EMOJI="✅"
            STATUS_MSG="Upgrade successful"
          else
            STATUS_EMOJI="❌"
            STATUS_MSG="Upgrade failed"
          fi

          cat >> $GITHUB_STEP_SUMMARY <<EOF
          ### Control Plane Node: $NODE_NAME $STATUS_EMOJI

          **Status**: $STATUS_MSG
          **New Version**: v$NEW_VERSION
          **Job Status**: $JOB_STATUS
          EOF

  # ==========================================================================
  # PHASE 3: WORKER UPGRADE (SEQUENTIAL WITH GPU PATCH SUPPORT)
  # ==========================================================================
  upgrade-workers:
    name: Upgrade Worker ${{ matrix.node }}
    needs: [rebuild-templates, upgrade-control-plane]
    if: |
      always() &&
      (needs.rebuild-templates.result == 'success' || needs.rebuild-templates.result == 'skipped') &&
      (inputs.test_mode || needs.upgrade-control-plane.result == 'success')
    runs-on: cattle-runner
    timeout-minutes: 30
    strategy:
      max-parallel: 1
      fail-fast: false
      matrix:
        node: ${{ inputs.test_mode && fromJSON('["k8s-work-1","k8s-work-2"]') || fromJSON('["k8s-work-1","k8s-work-2","k8s-work-3","k8s-work-4","k8s-work-5","k8s-work-6","k8s-work-11","k8s-work-12","k8s-work-13","k8s-work-14","k8s-work-15","k8s-work-16"]') }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Proxmox SSH key
        env:
          PROXMOX_SSH_KEY: ${{ secrets.PROXMOX_SSH_KEY }}
        run: |
          mkdir -p ~/.ssh
          printf '%s\n' "$PROXMOX_SSH_KEY" > ~/.ssh/proxmox_terraform
          chmod 600 ~/.ssh/proxmox_terraform

      - name: Initialize Terraform
        working-directory: ./terraform
        run: terraform init

      - name: Pre-drain health check
        run: |
          echo "::group::Pre-drain health check"
          echo "Checking critical workload distribution before draining ${{ matrix.node }}..."

          # Ensure CoreDNS has replicas on other nodes
          OTHER_DNS=$(kubectl get pods -n kube-system -l k8s-app=kube-dns \
            -o wide 2>/dev/null | grep -vF '${{ matrix.node }}' | grep Running | wc -l || echo "0")

          echo "CoreDNS replicas on other nodes: $OTHER_DNS"

          if [[ $OTHER_DNS -lt 1 ]]; then
            echo "::warning::No CoreDNS replicas on other nodes, scaling up..."
            kubectl scale deployment coredns -n kube-system --replicas=3
            sleep 30
          fi

          # Check for Rook-Ceph OSDs on this node
          OSD_COUNT=$(kubectl get pods -n rook-ceph -l app=rook-ceph-osd \
            -o wide 2>/dev/null | grep -F '${{ matrix.node }}' | grep Running | wc -l || echo "0")

          if [[ $OSD_COUNT -gt 0 ]]; then
            echo "::notice::Node has $OSD_COUNT Ceph OSD(s)"
            echo "Setting Ceph noout flag to prevent rebalancing during upgrade..."
            kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd set noout || true
            echo "osd_count=$OSD_COUNT" >> $GITHUB_ENV
          else
            echo "osd_count=0" >> $GITHUB_ENV
          fi

          echo "::endgroup::"

      - name: Cordon node
        run: |
          echo "Cordoning ${{ matrix.node }}..."
          kubectl cordon "${{ matrix.node }}"

      - name: Drain node
        timeout-minutes: 5
        run: |
          echo "::group::Draining ${{ matrix.node }}"
          echo "Attempting graceful drain (respects PodDisruptionBudget)..."

          # Layer 3: Graceful drain with timeout + force drain fallback
          # Try graceful drain first (respects PDB)
          if timeout 240 kubectl drain "${{ matrix.node }}" \
            --ignore-daemonsets \
            --delete-emptydir-data \
            --grace-period=30; then

            echo "✅ Graceful drain succeeded"
          else
            echo "⚠️  Graceful drain timed out (likely PDB blocking runner eviction)"
            echo "Falling back to force drain..."

            # Force drain bypasses PDB (used when runner pod is on this node)
            kubectl drain "${{ matrix.node }}" \
              --ignore-daemonsets \
              --delete-emptydir-data \
              --disable-eviction \
              --force \
              --grace-period=0

            echo "✅ Force drain completed"
            echo "ℹ️  If runner pod was evicted, GitHub will re-queue job to surviving runner"

            # Give GitHub Actions time to detect runner termination and re-queue
            sleep 30
          fi

          # Show pods remaining (should be only daemonsets)
          echo ""
          echo "Pods remaining on node (daemonsets only):"
          kubectl get pods --all-namespaces -o wide 2>/dev/null | grep -F '${{ matrix.node }}' || echo "None"

          echo "✅ Node drained successfully"
          echo "::endgroup::"

      - name: Backup Terraform state
        working-directory: ./terraform
        run: |
          echo "Creating state backup before destroy..."

          # SECURITY: Check for state lock before proceeding
          if aws s3 ls s3://${{ env.TERRAFORM_STATE_BUCKET }}/.terraform.tfstate.lock.info 2>/dev/null; then
            echo "::error::Terraform state is locked - another operation in progress"
            echo "::error::Wait for other operation to complete or manually remove lock if stale"
            exit 1
          fi

          aws s3 cp s3://${{ env.TERRAFORM_STATE_BUCKET }}/terraform.tfstate \
            s3://${{ env.TERRAFORM_STATE_BUCKET }}/backups/pre-upgrade-${{ matrix.node }}-$(date +%s).tfstate

      - name: Destroy VM
        working-directory: ./terraform
        timeout-minutes: 5
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Destroying ${{ matrix.node }} VM"
          terraform destroy \
            -input=false \
            -auto-approve \
            -target=module.worker_nodes[\"${{ matrix.node }}\"]

          echo "✅ VM destroyed"
          echo "::endgroup::"

      - name: Recreate VM from new template
        working-directory: ./terraform
        timeout-minutes: 10
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Recreating ${{ matrix.node }} from v${{ inputs.new_version }} template"
          terraform apply \
            -input=false \
            -auto-approve \
            -target=module.worker_nodes[\"${{ matrix.node }}\"]

          echo "✅ VM recreated from new template"
          echo "::endgroup::"

      # ==========================================================================
      # CRITICAL SECURITY OPERATION: Apply Machine Configuration
      # This step implements the 5-layer security architecture to inject
      # Talos machine configs from GitHub Secrets without leaking secrets
      # ==========================================================================
      - name: Apply machine configuration (CRITICAL SECURITY OPERATION)
        id: apply_config
        env:
          NODE_NAME: ${{ matrix.node }}
          # SECURITY: Secret passed directly to script, not stored in env var
          # Worker node configs: TALOS_MACHINE_CONFIG_WORK_1, WORK_2, WORK_3, etc.
          NODE_CONFIG_SECRET: ${{ secrets[format('TALOS_MACHINE_CONFIG_{0}', matrix.node == 'k8s-work-1' && 'WORK_1' || matrix.node == 'k8s-work-2' && 'WORK_2' || matrix.node == 'k8s-work-3' && 'WORK_3' || matrix.node == 'k8s-work-4' && 'WORK_4' || matrix.node == 'k8s-work-5' && 'WORK_5' || matrix.node == 'k8s-work-6' && 'WORK_6' || matrix.node == 'k8s-work-11' && 'WORK_11' || matrix.node == 'k8s-work-12' && 'WORK_12' || matrix.node == 'k8s-work-13' && 'WORK_13' || matrix.node == 'k8s-work-14' && 'WORK_14' || matrix.node == 'k8s-work-15' && 'WORK_15' || matrix.node == 'k8s-work-16' && 'WORK_16')] }}
        shell: bash
        timeout-minutes: 5
        run: |
          # CRITICAL: Absolutely no command echo
          set +x
          set -euo pipefail

          echo "Starting secure configuration deployment for $NODE_NAME..."

          # ============================================================
          # SECURITY LAYER 1: Disable Command Echo (set +x above)
          # ============================================================

          # ============================================================
          # SECURITY LAYER 2: Ephemeral Secure Storage
          # ============================================================

          # Create dedicated config directory with maximum restrictions
          CONFIG_DIR="$(mktemp -d -t talos-config-XXXXXX)"
          chmod 700 "$CONFIG_DIR"

          # Set aggressive cleanup trap
          cleanup_config() {
            if [[ -d "$CONFIG_DIR" ]]; then
              find "$CONFIG_DIR" -type f -exec shred -fz -n 3 {} \; 2>/dev/null || true
              rm -rf "$CONFIG_DIR"
            fi
          }
          trap cleanup_config EXIT

          # ============================================================
          # SECURITY LAYER 3: Decode and Protect
          # ============================================================

          # Decode configuration with maximum protection and error handling
          CONFIG_FILE="$CONFIG_DIR/machine.yaml"

          # Validate secret exists and is not empty
          if [[ -z "$NODE_CONFIG_SECRET" ]]; then
            echo "ERROR: Machine config secret for $NODE_NAME is not set"
            echo "ERROR: Expected secret: TALOS_MACHINE_CONFIG_<WORK_X>"
            exit 1
          fi

          if ! echo "$NODE_CONFIG_SECRET" | base64 -d > "$CONFIG_FILE" 2>/dev/null; then
            echo "ERROR: Failed to decode configuration"
            exit 1
          fi
          chmod 600 "$CONFIG_FILE"

          # Validate configuration has expected structure with error suppression
          if ! grep -q "machine:" "$CONFIG_FILE" 2>/dev/null; then
            echo "ERROR: Invalid machine configuration structure"
            exit 1
          fi

          echo "Configuration decoded and validated"

          # ============================================================
          # SECURITY LAYER 4: Apply with Maximum Output Suppression
          # ============================================================

          # Map node name to IP (hardcoded since node doesn't exist in kubectl yet)
          case "$NODE_NAME" in
            k8s-work-1)  NODE_IP="10.20.67.4" ;;
            k8s-work-2)  NODE_IP="10.20.67.5" ;;
            k8s-work-3)  NODE_IP="10.20.67.6" ;;
            k8s-work-4)  NODE_IP="10.20.67.7" ;;
            k8s-work-5)  NODE_IP="10.20.67.8" ;;
            k8s-work-6)  NODE_IP="10.20.67.9" ;;
            k8s-work-11) NODE_IP="10.20.67.10" ;;
            k8s-work-12) NODE_IP="10.20.67.11" ;;
            k8s-work-13) NODE_IP="10.20.67.12" ;;
            k8s-work-14) NODE_IP="10.20.67.13" ;;
            k8s-work-15) NODE_IP="10.20.67.14" ;;
            k8s-work-16) NODE_IP="10.20.67.15" ;;
            *)
              echo "ERROR: Unknown worker node: $NODE_NAME"
              exit 1
              ;;
          esac

          echo "Applying configuration to node $NODE_IP..."

          # Wait for VM to be network-ready (VM just created)
          echo "Waiting for VM to be network-ready..."
          MAX_WAIT=60
          for i in {1..12}; do
            if talosctl -n "$NODE_IP" version --insecure --timeout=5s >/dev/null 2>&1; then
              echo "✓ VM is network-ready after $((i * 5)) seconds"
              break
            fi
            echo "  Waiting... ($((i * 5))/${MAX_WAIT}s)"
            sleep 5
          done

          # Application with retry logic and complete output suppression
          MAX_ATTEMPTS=3
          ATTEMPT=1
          SUCCESS=false

          while [[ $ATTEMPT -le $MAX_ATTEMPTS ]]; do
            echo "  Attempt $ATTEMPT of $MAX_ATTEMPTS..."

            # Create a log file for this attempt with restrictive permissions
            ATTEMPT_LOG="$CONFIG_DIR/attempt_${ATTEMPT}.log"
            touch "$ATTEMPT_LOG"
            chmod 600 "$ATTEMPT_LOG"

            # Apply configuration with all output captured
            if talosctl -n "$NODE_IP" apply-config \
                --insecure \
                --file "$CONFIG_FILE" \
                --timeout=120s \
                >"$ATTEMPT_LOG" 2>&1; then

              # CRITICAL: Sanitize log IMMEDIATELY even on success
              sed -i 's/token:.*/token: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/secret:.*/secret: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/certificate:.*/certificate: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN .* PRIVATE KEY-----.*/[PRIVATE KEY REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN CERTIFICATE-----.*/[CERTIFICATE REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true

              echo "  ✓ Configuration applied successfully"
              SUCCESS=true

              # Shred sanitized log (non-verbose to avoid path leaks)
              shred -fz -n 3 "$ATTEMPT_LOG" 2>/dev/null || true
              break

            else
              # CRITICAL: Sanitize log BEFORE checking error types
              sed -i 's/token:.*/token: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/secret:.*/secret: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/certificate:.*/certificate: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN .* PRIVATE KEY-----.*/[PRIVATE KEY REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN CERTIFICATE-----.*/[CERTIFICATE REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true

              echo "  ⚠ Attempt $ATTEMPT failed"

              # Now safe to check error types from sanitized log
              if grep -q "connection refused" "$ATTEMPT_LOG" 2>/dev/null; then
                echo "    Node not yet ready, waiting..."
                sleep $((ATTEMPT * 10))
              elif grep -q "timeout" "$ATTEMPT_LOG" 2>/dev/null; then
                echo "    Operation timed out, retrying..."
                sleep 5
              else
                echo "    Unknown error occurred"
              fi

              # Shred sanitized log (non-verbose to avoid path leaks)
              shred -fz -n 3 "$ATTEMPT_LOG" 2>/dev/null || true
            fi

            ((ATTEMPT++))
          done

          if [[ "$SUCCESS" != "true" ]]; then
            echo "ERROR: Failed to apply configuration after $MAX_ATTEMPTS attempts"
            exit 1
          fi

          # ============================================================
          # SECURITY LAYER 5: Aggressive Cleanup
          # ============================================================

          # Cleanup happens automatically via EXIT trap
          echo "Configuration deployment completed securely"

      - name: Wait for node to be Ready
        timeout-minutes: 10
        run: |
          echo "Waiting for ${{ matrix.node }} to become Ready..."

          for i in {1..60}; do
            if kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=10s 2>/dev/null; then
              echo "✅ Node is Ready"
              break
            fi

            # Show node status every minute (every 6 attempts)
            if [[ $((i % 6)) -eq 0 ]]; then
              echo "Node status after $i attempts:"
              kubectl get node "${{ matrix.node }}" || true
              kubectl describe node "${{ matrix.node }}" | tail -20 || true
            fi

            echo "Attempt $i/60: Node not ready yet..."
            sleep 10
          done

          # Final check
          kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=30s

      - name: Verify version
        run: |
          NODE_VERSION=$(kubectl get node "${{ matrix.node }}" -o jsonpath='{.status.nodeInfo.kubeletVersion}')
          echo "Node version: $NODE_VERSION"

          if [[ "$NODE_VERSION" != *"${{ inputs.new_version }}"* ]]; then
            echo "::error::Version mismatch: expected v${{ inputs.new_version }}, got $NODE_VERSION"
            exit 1
          fi

          echo "✅ Node running correct version: $NODE_VERSION"

      - name: Apply global patches
        timeout-minutes: 5
        run: |
          echo "::group::Applying global patches to ${{ matrix.node }}"
          echo "These patches ensure consistent sysctls, kubelet config, network, and time settings"

          # Download talosctl for the new version
          echo "Installing talosctl v${{ inputs.new_version }}..."

          # Download binary
          curl -sL "https://github.com/siderolabs/talos/releases/download/v${{ inputs.new_version }}/talosctl-linux-amd64" \
            -o talosctl

          # Download checksums
          curl -sL "https://github.com/siderolabs/talos/releases/download/v${{ inputs.new_version }}/sha512sum.txt" \
            -o sha512sum.txt

          # SECURITY: Verify checksum
          if ! sha512sum -c --ignore-missing sha512sum.txt 2>&1 | grep -q "talosctl-linux-amd64: OK"; then
            echo "::error::Checksum verification failed for talosctl v${{ inputs.new_version }}"
            exit 1
          fi

          chmod +x talosctl
          ./talosctl version --client

          # Determine node IP
          NODE_IP=$(kubectl get node "${{ matrix.node }}" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}')
          echo "Node IP: $NODE_IP"

          # Apply global patches in order
          echo "Applying global patches..."

          GLOBAL_PATCHES=("machine-kubelet" "machine-network" "machine-sysctls" "machine-time")

          for patch in "${GLOBAL_PATCHES[@]}"; do
            echo "  → Applying talos/patches/global/${patch}.yaml..."
            ./talosctl patch machineconfig \
              --nodes $NODE_IP \
              --patch-file talos/patches/global/${patch}.yaml
          done

          echo "✅ All global patches applied"

          # Wait for patches to be processed
          echo "Waiting for patches to be applied (30s)..."
          sleep 30

          # Wait for node to be ready again
          echo "Waiting for node to become Ready..."
          kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=300s

          echo "✅ Global patches successfully applied to ${{ matrix.node }}"
          echo "::endgroup::"

      - name: Apply GPU patches
        if: matrix.node == 'k8s-work-4' || matrix.node == 'k8s-work-14'
        timeout-minutes: 5
        run: |
          echo "::group::Applying GPU patches for ${{ matrix.node }}"
          echo "Detected GPU node: ${{ matrix.node }}"

          # Download talosctl for the new version
          echo "Installing talosctl v${{ inputs.new_version }}..."

          # Download binary
          curl -sL "https://github.com/siderolabs/talos/releases/download/v${{ inputs.new_version }}/talosctl-linux-amd64" \
            -o talosctl

          # Download checksums
          curl -sL "https://github.com/siderolabs/talos/releases/download/v${{ inputs.new_version }}/sha512sum.txt" \
            -o sha512sum.txt

          # SECURITY: Verify checksum
          if ! sha512sum -c --ignore-missing sha512sum.txt 2>&1 | grep -q "talosctl-linux-amd64: OK"; then
            echo "::error::Checksum verification failed for talosctl v${{ inputs.new_version }}"
            exit 1
          fi

          chmod +x talosctl
          ./talosctl version --client

          # Determine node IP
          NODE_IP=$(kubectl get node "${{ matrix.node }}" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}')
          echo "Node IP: $NODE_IP"

          # Apply GPU-specific patches
          echo "Applying GPU patch from talos/patches/${{ matrix.node }}/nvidia-gpu.yaml..."
          ./talosctl patch machineconfig \
            --nodes $NODE_IP \
            --patch-file talos/patches/${{ matrix.node }}/nvidia-gpu.yaml

          # Wait for patch to be processed
          echo "Waiting for GPU patch to be applied (30s)..."
          sleep 30

          # Wait for node to be ready again
          kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=300s

          # Validate GPU detected
          echo "Validating GPU detection..."
          GPU_PRESENT=$(kubectl get node "${{ matrix.node }}" \
            -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' || echo "")

          if [[ "$GPU_PRESENT" != "true" ]]; then
            echo "::error::GPU not detected on ${{ matrix.node }}"
            echo "::error::Expected label nvidia.com/gpu.present=true, got: '$GPU_PRESENT'"
            exit 1
          fi

          echo "✅ GPU patch successfully applied to ${{ matrix.node }}"
          echo "✅ GPU detected: nvidia.com/gpu.present label found"
          echo "::endgroup::"

      - name: Uncordon node
        run: |
          echo "Uncordoning ${{ matrix.node }}..."
          kubectl uncordon "${{ matrix.node }}"

      - name: Post-upgrade validation
        run: |
          echo "::group::Post-upgrade validation"

          # Wait for pods to reschedule
          echo "Waiting for workloads to redistribute (30s)..."
          sleep 30

          # Check for unhealthy deployments
          echo "Checking deployment health..."
          UNHEALTHY=$(kubectl get deployments --all-namespaces -o json 2>/dev/null | \
            jq -r '.items[] |
              select(.status.replicas != .status.readyReplicas) |
              "\(.metadata.namespace)/\(.metadata.name): \(.status.readyReplicas)/\(.status.replicas)"' || echo "")

          if [[ -n "$UNHEALTHY" ]]; then
            echo "::warning::Some deployments not fully ready:"
            echo "$UNHEALTHY"
          else
            echo "✅ All deployments healthy"
          fi

          # Remove Ceph noout flag if it was set
          if [[ "${{ env.osd_count }}" != "0" ]]; then
            echo "Removing Ceph noout flag..."
            kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd unset noout || true
          fi

          echo "::endgroup::"

      - name: Upgrade summary
        if: always()
        run: |
          # Store GHA expressions in bash variables first
          NODE_NAME="${{ matrix.node }}"
          NEW_VERSION="${{ inputs.new_version }}"
          JOB_STATUS="${{ job.status }}"

          GPU_STATUS=""
          if [[ "$NODE_NAME" == "k8s-work-4" ]] || [[ "$NODE_NAME" == "k8s-work-14" ]]; then
            GPU_STATUS=" (GPU node)"
          fi

          # Determine status emoji and message based on job status
          if [[ "$JOB_STATUS" == "success" ]]; then
            STATUS_EMOJI="✅"
            STATUS_MSG="Upgrade successful"
          else
            STATUS_EMOJI="❌"
            STATUS_MSG="Upgrade failed"
          fi

          cat >> $GITHUB_STEP_SUMMARY <<EOF
          ### Worker Node: $NODE_NAME$GPU_STATUS $STATUS_EMOJI

          **Status**: $STATUS_MSG
          **New Version**: v$NEW_VERSION
          **Job Status**: $JOB_STATUS
          EOF

  # ==========================================================================
  # PHASE 4: CLUSTER VALIDATION
  # ==========================================================================
  validate-cluster:
    name: Validate Cluster Health
    needs: [rebuild-templates, upgrade-control-plane, upgrade-workers]
    if: always()
    runs-on: cattle-runner
    timeout-minutes: 10
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      - name: Check all nodes upgraded
        run: |
          echo "::group::Validating node versions"
          EXPECTED_VERSION="${{ inputs.new_version }}"
          FAILED_NODES=""

          echo "Expected version: v$EXPECTED_VERSION"
          echo ""
          echo "Node version check:"

          for node in $(kubectl get nodes -o name | cut -d/ -f2); do
            VERSION=$(kubectl get node "$node" -o jsonpath='{.status.nodeInfo.kubeletVersion}')
            echo "  $node: $VERSION"

            if [[ "$VERSION" != *"$EXPECTED_VERSION"* ]]; then
              FAILED_NODES="$FAILED_NODES $node($VERSION)"
            fi
          done

          if [[ -n "$FAILED_NODES" ]]; then
            echo ""
            echo "::error::Nodes not upgraded:$FAILED_NODES"
            exit 1
          fi

          echo ""
          echo "✅ All nodes running v$EXPECTED_VERSION"
          echo "::endgroup::"

      - name: Validate GPU nodes
        if: inputs.test_mode == false
        run: |
          echo "::group::Validating GPU functionality"

          # Check k8s-work-4 (RTX A2000)
          echo "Checking k8s-work-4 (RTX A2000)..."
          GPU_4=$(kubectl get node k8s-work-4 \
            -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' || echo "")

          if [[ "$GPU_4" != "true" ]]; then
            echo "::error::GPU not detected on k8s-work-4"
            echo "::error::Expected label nvidia.com/gpu.present=true, got: '$GPU_4'"
            exit 1
          fi
          echo "✅ k8s-work-4: GPU detected"

          # Check k8s-work-14 (RTX A5000)
          echo "Checking k8s-work-14 (RTX A5000)..."
          GPU_14=$(kubectl get node k8s-work-14 \
            -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' || echo "")

          if [[ "$GPU_14" != "true" ]]; then
            echo "::error::GPU not detected on k8s-work-14"
            echo "::error::Expected label nvidia.com/gpu.present=true, got: '$GPU_14'"
            exit 1
          fi
          echo "✅ k8s-work-14: GPU detected"

          echo ""
          echo "✅ All GPU nodes validated"
          echo "::endgroup::"

      - name: Validate workloads
        run: |
          echo "::group::Validating workload health"

          # Check HelmReleases
          echo "Checking HelmRelease status..."
          FAILED_HR=$(kubectl get helmreleases --all-namespaces -o json 2>/dev/null | \
            jq -r '.items[] |
              select(
                (.status.conditions // []) |
                map(select(.type == "Ready" and .status != "True")) |
                length > 0
              ) |
              "\(.metadata.namespace)/\(.metadata.name): \(.status.conditions[] | select(.type == "Ready") | .message // "Unknown")"' || echo "")

          if [[ -n "$FAILED_HR" ]]; then
            echo "::warning::Some HelmReleases unhealthy:"
            echo "$FAILED_HR"
          else
            echo "✅ All HelmReleases healthy"
          fi

          # Check for non-running pods (excluding completed jobs)
          echo ""
          echo "Checking pod status..."
          UNHEALTHY_PODS=$(kubectl get pods --all-namespaces 2>/dev/null | \
            grep -v Running | grep -v Completed || echo "")

          if [[ -n "$UNHEALTHY_PODS" ]]; then
            echo "::warning::Some pods not running:"
            echo "$UNHEALTHY_PODS"
          else
            echo "✅ All pods running or completed"
          fi

          echo "::endgroup::"

      - name: Final summary
        if: always()
        run: |
          # Determine emojis based on actual phase results
          case "${{ needs.rebuild-templates.result }}" in
            success) TEMPLATE_EMOJI="✅" ;;
            skipped) TEMPLATE_EMOJI="⏭️" ;;
            failure) TEMPLATE_EMOJI="❌" ;;
            *) TEMPLATE_EMOJI="❓" ;;
          esac

          case "${{ needs.upgrade-control-plane.result }}" in
            success) CTRL_EMOJI="✅" ;;
            skipped) CTRL_EMOJI="⏭️" ;;
            failure) CTRL_EMOJI="❌" ;;
            *) CTRL_EMOJI="❓" ;;
          esac

          case "${{ needs.upgrade-workers.result }}" in
            success) WORKER_EMOJI="✅" ;;
            skipped) WORKER_EMOJI="⏭️" ;;
            failure) WORKER_EMOJI="❌" ;;
            *) WORKER_EMOJI="❓" ;;
          esac

          # Determine overall workflow status
          if [[ "${{ job.status }}" == "success" ]]; then
            OVERALL_EMOJI="🎉"
            OVERALL_TITLE="Cattle Upgrade Complete"
          else
            OVERALL_EMOJI="⚠️"
            OVERALL_TITLE="Cattle Upgrade Failed"
          fi

          cat >> $GITHUB_STEP_SUMMARY <<EOF
          ## $OVERALL_TITLE $OVERALL_EMOJI

          **Old Version**: v${{ inputs.old_version }}
          **New Version**: v${{ inputs.new_version }}
          **Test Mode**: ${{ inputs.test_mode }}

          ### Phase Results
          - $TEMPLATE_EMOJI Template Rebuild: ${{ needs.rebuild-templates.result }}
          - $CTRL_EMOJI Control Plane Upgrade: ${{ inputs.test_mode && 'skipped (test mode)' || needs.upgrade-control-plane.result }}
          - $WORKER_EMOJI Worker Upgrade: ${{ needs.upgrade-workers.result }}
          - ${{ job.status == 'success' && '✅' || '❌' }} Validation: ${{ job.status }}

          ### Cluster State
          \`\`\`
          $(kubectl get nodes -o wide)
          \`\`\`

          $(if [[ "${{ inputs.test_mode }}" == "true" ]]; then
            echo "### Next Steps"
            echo ""
            echo "This was a **test mode** run (2 workers only)."
            echo ""
            echo "To run full production upgrade:"
            echo "\`\`\`"
            echo "gh workflow run upgrade-cattle.yaml \\"
            echo "  --field old_version=${{ inputs.old_version }} \\"
            echo "  --field new_version=${{ inputs.new_version }} \\"
            echo "  --field test_mode=false"
            echo "\`\`\`"
          fi)
          EOF

          # Exit with error if any critical phase failed
          if [[ "${{ needs.rebuild-templates.result }}" != "success" ]] && [[ "${{ needs.rebuild-templates.result }}" != "skipped" ]]; then
            echo "::error::Template rebuild failed - upgrade incomplete"
            exit 1
          fi

          if [[ "${{ inputs.test_mode }}" == "false" ]] && [[ "${{ needs.upgrade-control-plane.result }}" != "success" ]]; then
            echo "::error::Control plane upgrade failed"
            exit 1
          fi

          echo "✅ Cattle upgrade workflow completed successfully"

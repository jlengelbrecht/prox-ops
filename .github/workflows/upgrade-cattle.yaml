---
name: Cattle Upgrade - Complete Workflow
#
# =============================================================================
# CATTLE UPGRADE WORKFLOW - CRITICAL DOCUMENTATION
# =============================================================================
#
# WHAT THIS WORKFLOW DOES:
# This workflow performs a "cattle" upgrade of the Talos Linux cluster by:
# 1. Creating new VM templates with the new Talos version
# 2. Destroying and recreating each node VM from the new template
# 3. Applying machine configuration to bootstrap Talos
# 4. Waiting for nodes to rejoin the Kubernetes cluster
#
# KNOWN LIMITATIONS:
#
# 1. CANCEL BUTTON BEHAVIOR
#    The GitHub Actions "Cancel" button may not immediately stop a running job.
#    - cancel-in-progress: true only affects QUEUED runs, not the currently executing job
#    - To emergency stop: You must either wait for the job to complete/fail, or
#      stop the self-hosted runner (cattle-runner VM) via Proxmox
#    - GitHub limitation: https://github.com/orgs/community/discussions/26531
#
# 2. PARALLEL WORKER PROCESSING
#    The max-parallel setting enables true parallel execution with multiple runners:
#    - safe mode (max-parallel=1): Sequential, one worker at a time
#    - standard mode (max-parallel=2): Pair parallelism, requires 2+ cattle-runners
#    - aggressive mode (max-parallel=4): Multi-pair parallelism, requires 4+ cattle-runners
#
#    TERRAFORM STATE LOCK HANDLING:
#    - All terraform operations share a single S3 state file with DynamoDB locking
#    - When parallel jobs contend for the lock, they retry with linear backoff
#    - Lock contention adds ~15-120s wait time per retry (up to 12 retries)
#    - Non-terraform steps (talosctl, kubectl) run truly in parallel
#
#    RUNNER REQUIREMENTS:
#    - Each concurrent job needs its own cattle-runner (external to k8s cluster)
#    - Runners must have cattle-runner label
#    - VM 9008 should be scaled for multiple runners (4 CPU, 16GB RAM for 3 runners)
#
# 3. APPLY MACHINE CONFIGURATION FAILURES
#    The "Apply machine configuration" step requires the VM to be in MAINTENANCE MODE.
#    Common failure causes:
#    a) Terraform silently failed - VM was never destroyed/recreated
#    b) Old VM still running full Talos (not maintenance mode)
#    c) Network connectivity issues to the new VM
#    d) Proxmox failed to properly clone the template
#
#    Symptoms: "certificate required" or "x509:" errors mean the node is running
#    FULL Talos (not maintenance mode) - the --insecure flag only works in maintenance.
#
# RECOVERY PROCEDURES:
#
# If a worker node fails mid-upgrade:
# 1. Check if VM exists: Proxmox UI or `terraform state list | grep <node>`
# 2. If VM exists but full Talos: terraform destroy then apply for that node
# 3. If VM doesn't exist: terraform apply -target=module.worker_nodes["<node>"]
# 4. Apply config manually: talosctl apply-config --insecure -n <IP> -f <config>
#
# Manual cattle recovery for a single node:
#   cd terraform
#   terraform destroy -target=module.worker_nodes["k8s-work-X"] -auto-approve
#   terraform apply -target=module.worker_nodes["k8s-work-X"] -auto-approve
#   talosctl apply-config --insecure -n 10.20.67.X -f ../talos/clusterconfig/kubernetes-k8s-work-X.yaml
#
# =============================================================================

on:
  workflow_call:
    inputs:
      old_version:
        description: Previous Talos version
        type: string
        required: true
      new_version:
        description: New Talos version to upgrade to
        type: string
        required: true
      test_templates:
        description: Test only template rebuild (skip controllers & workers)
        type: boolean
        default: false
      test_controllers:
        description: Test only controllers (1 controller, skip templates & workers)
        type: boolean
        default: false
      test_workers:
        description: Test only workers (2 workers, skip templates & controllers)
        type: boolean
        default: false
      worker_rollout_mode:
        description: 'Worker rollout strategy (safe: 1 pair at a time, standard: both in pair parallel, aggressive: multiple pairs parallel)'
        type: string
        default: standard
      taint_templates:
        description: Force template rebuild by tainting all template resources before plan
        type: boolean
        default: false
  workflow_dispatch:
    inputs:
      old_version:
        description: Previous Talos version
        type: string
        required: true
      new_version:
        description: New Talos version to upgrade to
        type: string
        required: true
      test_templates:
        description: Test only template rebuild (skip controllers & workers)
        type: boolean
        default: false
      test_controllers:
        description: Test only controllers (1 controller, skip templates & workers)
        type: boolean
        default: false
      test_workers:
        description: Test only workers (2 workers, skip templates & controllers)
        type: boolean
        default: false
      worker_rollout_mode:
        description: 'Worker rollout strategy (safe: 1 pair at a time, standard: both in pair parallel, aggressive: multiple pairs parallel)'
        type: choice
        options:
          - safe
          - standard
          - aggressive
        default: standard
      taint_templates:
        description: Force template rebuild by tainting all template resources before plan
        type: boolean
        default: false

# Prevent concurrent cattle upgrades (Terraform state lock protection)
# Cancel old queued runs when new run starts (prevents zombie workflow blocking)
concurrency:
  group: cattle-upgrade-v2
  cancel-in-progress: true

# Explicit minimal permissions for security (prevents broad default token access)
permissions:
  contents: read      # Read repository contents

env:
  AWS_REGION: us-east-2
  TERRAFORM_STATE_BUCKET: prox-ops-terraform-state
  TERRAFORM_VERSION: "1.14.5"  # Managed by Renovate
  # Cache Terraform providers to prevent GitHub rate limiting and download timeouts
  # (work-9 failed in run 20842324836 due to provider download timeout)
  TF_PLUGIN_CACHE_DIR: /tmp/terraform-plugin-cache

jobs:
  # ==========================================================================
  # INPUT VALIDATION: Ensure test flags are mutually exclusive
  # ==========================================================================
  validate-inputs:
    name: Validate Input Flags
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Validate test flags are mutually exclusive
        run: |
          count=0
          [[ "${{ inputs.test_templates }}" == "true" ]] && count=$((count + 1))
          [[ "${{ inputs.test_controllers }}" == "true" ]] && count=$((count + 1))
          [[ "${{ inputs.test_workers }}" == "true" ]] && count=$((count + 1))

          if [[ $count -gt 1 ]]; then
            echo "::error::Multiple test flags set. Only one test flag can be true at a time."
            echo "::error::test_templates=${{ inputs.test_templates }}"
            echo "::error::test_controllers=${{ inputs.test_controllers }}"
            echo "::error::test_workers=${{ inputs.test_workers }}"
            exit 1
          fi

          # Validate worker_rollout_mode parameter (security requirement)
          MODE="${{ inputs.worker_rollout_mode }}"
          if [[ ! "$MODE" =~ ^(safe|standard|aggressive)$ ]]; then
            echo "::error::Invalid worker_rollout_mode: '$MODE'"
            echo "::error::Must be one of: safe, standard, aggressive"
            exit 1
          fi

          echo "‚úÖ Input validation passed"
          if [[ $count -eq 0 ]]; then
            echo "Mode: Production (all nodes will be upgraded)"
          elif [[ "${{ inputs.test_templates }}" == "true" ]]; then
            echo "Mode: Template testing only"
          elif [[ "${{ inputs.test_controllers }}" == "true" ]]; then
            echo "Mode: Controller testing (1 controller only)"
          elif [[ "${{ inputs.test_workers }}" == "true" ]]; then
            echo "Mode: Worker testing (2 workers only)"
          fi

          if [[ "${{ inputs.taint_templates }}" == "true" ]]; then
            echo "üîÑ Taint Templates: ENABLED (all 8 template modules will be tainted for rebuild)"
          fi

  # ==========================================================================
  # PHASE 1: TEMPLATE REBUILD (CRITICAL - MISSING IN PREVIOUS IMPLEMENTATION)
  # ==========================================================================
  rebuild-templates:
    name: Rebuild Templates v${{ inputs.new_version }}
    runs-on: cattle-runner
    timeout-minutes: 60
    needs: validate-inputs
    # Run when: test_templates=true OR production mode (all test flags false)
    if: inputs.test_templates == true || (inputs.test_templates == false && inputs.test_controllers == false && inputs.test_workers == false)
    steps:
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

      - name: Validate version inputs
        run: |
          if ! [[ "${{ inputs.old_version }}" =~ ^[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
            echo "::error::Invalid old_version format: '${{ inputs.old_version }}'"
            exit 1
          fi
          if ! [[ "${{ inputs.new_version }}" =~ ^[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
            echo "::error::Invalid new_version format: '${{ inputs.new_version }}'"
            exit 1
          fi
          echo "‚úÖ Version inputs validated"

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      # Terraform now installed via mise (in setup-cluster-tools action above)

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@61815dcd50bd041e203e49132bacad1fd04d2708 # v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Proxmox SSH key
        env:
          PROXMOX_SSH_KEY: ${{ secrets.PROXMOX_SSH_KEY }}
        run: |
          mkdir -p ~/.ssh
          printf '%s\n' "$PROXMOX_SSH_KEY" > ~/.ssh/proxmox_terraform
          chmod 600 ~/.ssh/proxmox_terraform

      - name: Initialize Terraform (with provider caching)
        working-directory: ./terraform
        run: |
          # Create provider cache directory (prevents GitHub rate limiting)
          mkdir -p "$TF_PLUGIN_CACHE_DIR"

          # Retry terraform init up to 3 times (GitHub occasionally rate-limits provider downloads)
          for attempt in 1 2 3; do
            echo "::group::Terraform init attempt $attempt"
            if terraform init; then
              echo "::endgroup::"
              echo "‚úÖ Terraform initialized successfully"
              exit 0
            fi
            echo "::endgroup::"
            if [[ $attempt -lt 3 ]]; then
              echo "‚ö†Ô∏è terraform init failed, retrying in 30 seconds..."
              sleep 30
            fi
          done
          echo "::error::terraform init failed after 3 attempts"
          exit 1

      # Taint all template resources to force rebuild when checkbox is checked
      # This eliminates the need to manually run terraform taint before the workflow
      - name: Taint template resources (if requested)
        if: inputs.taint_templates == true
        working-directory: ./terraform
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "============================================"
          echo "Tainting all 8 template modules for rebuild"
          echo "============================================"

          # List of all template modules (4 nodes √ó 2 templates each = 8 total)
          TEMPLATES=(
            "template_baldar_base"
            "template_baldar_gpu"
            "template_heimdall_base"
            "template_heimdall_gpu"
            "template_odin_base"
            "template_odin_gpu"
            "template_thor_base"
            "template_thor_gpu"
          )

          TAINTED=0
          SKIPPED=0
          ERRORS=0

          for template in "${TEMPLATES[@]}"; do
            echo ""
            echo "Processing module.$template..."

            # Taint BOTH download and create resources
            # download_talos_image: Ensures image is downloaded on fresh runners
            # create_template: Triggers full template rebuild
            for resource in "download_talos_image" "create_template"; do
              TAINT_OUTPUT=$(terraform taint "module.${template}.null_resource.${resource}" 2>&1) || true

              if echo "$TAINT_OUTPUT" | grep -q "has been marked as tainted"; then
                echo "  ‚úÖ Tainted module.${template}.null_resource.${resource}"
                TAINTED=$((TAINTED + 1))
              elif echo "$TAINT_OUTPUT" | grep -qE "(does not exist|Resource not found|No matching resource)"; then
                echo "  ‚ö†Ô∏è  Skipped module.${template}.${resource} (resource not in state yet)"
                SKIPPED=$((SKIPPED + 1))
              else
                echo "  ‚ùå Error tainting module.${template}.${resource}:"
                echo "     $TAINT_OUTPUT"
                ERRORS=$((ERRORS + 1))
              fi
            done
          done

          echo ""
          echo "============================================"
          echo "Taint Summary: $TAINTED tainted, $SKIPPED skipped, $ERRORS errors"
          echo "============================================"

          if [[ $ERRORS -gt 0 ]]; then
            echo "::error::$ERRORS template(s) failed to taint due to errors (see logs above)"
            exit 1
          fi

          if [[ $TAINTED -eq 0 ]]; then
            echo "::warning::No templates were tainted. They may not exist in state yet."
          fi

      # S3-native lock clearing - runs immediately before terraform operations
      - name: Clear stale Terraform locks (pre-plan)
        uses: ./.github/actions/clear-terraform-lock
        timeout-minutes: 1
        with:
          terraform_state_bucket: ${{ env.TERRAFORM_STATE_BUCKET }}
          terraform_state_key: terraform.tfstate

      - name: Plan template changes
        working-directory: ./terraform
        timeout-minutes: 15
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Terraform Plan for Template Rebuild"
          echo "================================================================"
          echo "Planning template rebuild for 8 template modules"
          echo "================================================================"
          echo ""
          echo "This step will plan changes for:"
          echo "  - Baldar: controller + worker templates"
          echo "  - Heimdall: controller + worker templates"
          echo "  - Odin: controller + worker templates"
          echo "  - Thor: controller + worker templates"
          echo ""
          echo "Total: 8 template modules (16 null_resources)"
          echo ""
          echo "NOTE: Only templates are targeted - VMs are NOT affected"
          echo "================================================================"
          echo ""

          # Generate plan for validation
          terraform plan \
            -input=false \
            -out=template-rebuild.tfplan \
            -target=module.template_baldar_base \
            -target=module.template_baldar_gpu \
            -target=module.template_heimdall_base \
            -target=module.template_heimdall_gpu \
            -target=module.template_odin_base \
            -target=module.template_odin_gpu \
            -target=module.template_thor_base \
            -target=module.template_thor_gpu

          echo ""
          echo "================================================================"
          echo "Plan completed. Review the output above to identify:"
          echo "  1. What resources Terraform wants to destroy"
          echo "  2. What resources Terraform wants to create"
          echo "  3. Whether VM modules are incorrectly included"
          echo ""
          echo "Expected: Only template_* modules should be affected"
          echo "BUG: If worker_nodes or control_plane_nodes appear, the bug exists"
          echo "================================================================"
          echo "::endgroup::"

          # Show plan summary
          echo ""
          echo "::group::Plan Summary"
          terraform show -no-color template-rebuild.tfplan | head -100
          echo "::endgroup::"

      - name: Analyze plan output for bugs
        working-directory: ./terraform
        run: |
          echo "::group::Plan Analysis - Identifying Terraform Bug"
          echo "Analyzing terraform plan output to identify unintended targets..."
          echo ""

          # Check if plan file exists
          if [[ ! -f template-rebuild.tfplan ]]; then
            echo "::error::Plan file not found - terraform plan may have failed"
            exit 1
          fi

          # Show full plan in readable format
          echo "Full plan output:"
          terraform show -no-color template-rebuild.tfplan > plan-full.txt
          cat plan-full.txt

          echo ""
          echo "================================================================"
          echo "BUG DETECTION ANALYSIS"
          echo "================================================================"

          # Check for unintended VM module targets
          echo ""
          echo "Checking for unintended VM modules in plan..."

          BUG_DETECTED=false

          if grep -q "module.control_plane_nodes" plan-full.txt; then
            echo "::error::BUG DETECTED: control_plane_nodes module found in plan"
            echo "::error::This would destroy control plane VMs!"
            echo ""
            grep "module.control_plane_nodes" plan-full.txt
            echo ""
            BUG_DETECTED=true
          else
            echo "‚úÖ No control_plane_nodes in plan"
          fi

          if grep -q "module.worker_nodes" plan-full.txt; then
            echo "::error::BUG DETECTED: worker_nodes module found in plan"
            echo "::error::This would destroy worker VMs!"
            echo ""
            grep "module.worker_nodes" plan-full.txt
            echo ""
            BUG_DETECTED=true
          else
            echo "‚úÖ No worker_nodes in plan"
          fi

          # FAIL WORKFLOW if VM modules detected
          if [ "$BUG_DETECTED" = true ]; then
            echo ""
            echo "================================================================"
            echo "‚ùå WORKFLOW FAILED: Terraform would destroy VM nodes"
            echo "================================================================"
            echo "::error::Terraform plan includes VM modules (control_plane_nodes or worker_nodes)"
            echo "::error::This is the bug causing cluster outages!"
            echo "::error::Workflow terminated to prevent accidental node destruction"
            echo "::error::Review plan output above to identify root cause"
            echo ""
            echo "Next steps:"
            echo "1. Document findings in .claude/.ai-docs/troubleshooting/CLUSTER_OUTAGE_2025-11-21.md"
            echo "2. Identify why Terraform dependency graph includes VM modules"
            echo "3. Implement permanent fix"
            echo "4. Re-test until plan shows ONLY template modules"
            echo ""
            exit 1
          fi

          # POSITIVE ASSERTION: Verify template modules ARE present (Opus recommendation)
          echo ""
          echo "Verifying template modules are present in plan..."
          TEMPLATE_COUNT=0
          for module in template_baldar_base template_baldar_gpu \
                        template_heimdall_base template_heimdall_gpu \
                        template_odin_base template_odin_gpu \
                        template_thor_base template_thor_gpu; do
            if grep -q "module.$module" plan-full.txt; then
              TEMPLATE_COUNT=$((TEMPLATE_COUNT + 1))
              echo "  ‚úì Found module.$module"
            else
              echo "  ‚ö† Missing module.$module"
            fi
          done

          if [[ $TEMPLATE_COUNT -eq 0 ]]; then
            echo ""
            echo "::error::VALIDATION FAILED: No template modules found in plan"
            echo "::error::Expected 8 template modules, found 0"
            echo "::error::This likely indicates a configuration error"
            exit 1
          elif [[ $TEMPLATE_COUNT -lt 8 ]]; then
            echo ""
            echo "::warning::Only $TEMPLATE_COUNT/8 template modules found in plan"
            echo "::warning::Expected all 8 template modules to be affected"
          else
            echo ""
            echo "‚úÖ All 8 template modules found in plan"
          fi

          # Show resources to be destroyed
          echo ""
          echo "Resources Terraform plans to DESTROY:"
          grep -A 5 "# .* will be destroyed" plan-full.txt | head -50 || echo "None"

          # Show resources to be created
          echo ""
          echo "Resources Terraform plans to CREATE:"
          grep -A 5 "# .* will be created" plan-full.txt | head -50 || echo "None"

          echo ""
          echo "================================================================"
          echo "Validation passed - plan contains ONLY templates"
          echo "================================================================"
          echo "::endgroup::"

      - name: Template rebuild plan summary
        working-directory: ./terraform
        run: |
          cat >> $GITHUB_STEP_SUMMARY <<'EOF'
          ## Phase 1: Template Rebuild Plan ‚úÖ

          **Status**: Plan generated and validated

          ### Templates Targeted
          - Baldar: controller + worker
          - Heimdall: controller + worker
          - Odin: controller + worker
          - Thor: controller + worker

          **Total**: 8 template modules (16 null_resources)
          EOF

          # Add terraform plan output to summary
          if [[ -f plan-full.txt ]]; then
            cat >> $GITHUB_STEP_SUMMARY <<'EOF'

          ### Terraform Plan Output

          <details>
          <summary>Click to expand full terraform plan</summary>

          ```
          EOF
            cat plan-full.txt >> $GITHUB_STEP_SUMMARY
            cat >> $GITHUB_STEP_SUMMARY <<'EOF'
          ```

          </details>

          **Validation**: Plan will be validated before execution
          EOF
          else
            cat >> $GITHUB_STEP_SUMMARY <<'EOF'

          **Note**: Terraform plan output not available
          EOF
          fi

      - name: Execute template rebuild
        working-directory: ./terraform
        timeout-minutes: 20
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Executing Template Rebuild"
          echo "================================================================"
          echo "VALIDATION PASSED - Executing template rebuild"
          echo "================================================================"
          echo ""

          echo "This step will:"
          echo "  1. Destroy old templates (8 templates)"
          echo "  2. Download new Talos images"
          echo "  3. Create new templates with updated version"
          echo ""
          echo "NOTE: VMs are NOT affected - only templates are rebuilt"
          echo ""
          echo "IMPORTANT: Not using plan file for batched execution"
          echo "Reason: Terraform ignores -target flags when applying from plan file"
          echo "Solution: Inline plan+apply for each batch ensures targets are honored"
          echo "================================================================"
          echo ""

          # Apply templates in batches to avoid Ceph lock contention
          # Strategy: 2 templates at a time across different Proxmox nodes
          # Reduces parallel Ceph writes from 8 ‚Üí 2 (75% reduction in contention)
          # Estimated time: 4 batches √ó 2 min + delays = ~9.5 minutes
          #
          # CRITICAL: Do NOT use saved plan file (template-rebuild.tfplan)
          # Terraform ignores -target flags when applying from a plan file!
          # Instead, we let terraform create inline plans for each batch.

          echo "Applying template changes in batches..."
          echo "Batching strategy: 2 templates per batch across different nodes"
          echo ""

          # Batch 1: Baldar base + Heimdall GPU (different nodes, different schematics)
          echo "================================================================"
          echo "Batch 1/4: Creating templates on Baldar (base) + Heimdall (GPU)"
          echo "================================================================"
          terraform apply -input=false -auto-approve \
            -target=module.template_baldar_base \
            -target=module.template_heimdall_gpu

          echo ""
          echo "‚úì Batch 1 complete. Waiting 30 seconds for Ceph to settle..."
          sleep 30

          # Batch 2: Baldar GPU + Heimdall base (different nodes, different schematics)
          echo ""
          echo "================================================================"
          echo "Batch 2/4: Creating templates on Baldar (GPU) + Heimdall (base)"
          echo "================================================================"
          terraform apply -input=false -auto-approve \
            -target=module.template_baldar_gpu \
            -target=module.template_heimdall_base

          echo ""
          echo "‚úì Batch 2 complete. Waiting 30 seconds for Ceph to settle..."
          sleep 30

          # Batch 3: Odin base + Thor GPU (different nodes, different schematics)
          echo ""
          echo "================================================================"
          echo "Batch 3/4: Creating templates on Odin (base) + Thor (GPU)"
          echo "================================================================"
          terraform apply -input=false -auto-approve \
            -target=module.template_odin_base \
            -target=module.template_thor_gpu

          echo ""
          echo "‚úì Batch 3 complete. Waiting 30 seconds for Ceph to settle..."
          sleep 30

          # Batch 4: Odin GPU + Thor base (different nodes, different schematics)
          echo ""
          echo "================================================================"
          echo "Batch 4/4: Creating templates on Odin (GPU) + Thor (base)"
          echo "================================================================"
          terraform apply -input=false -auto-approve \
            -target=module.template_odin_gpu \
            -target=module.template_thor_base

          echo ""
          echo "================================================================"
          echo "‚úÖ All 4 batches completed successfully"
          echo "‚úÖ Template rebuild completed: 8 templates created"
          echo "================================================================"
          echo "::endgroup::"

      - name: Verify template rebuild
        working-directory: ./terraform
        timeout-minutes: 10  # Copilot #6: Add timeout for consistency and safety
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Verifying Template Rebuild"
          echo "Running terraform refresh to verify new templates..."

          terraform refresh \
            -input=false \
            -target=module.template_baldar_base \
            -target=module.template_baldar_gpu \
            -target=module.template_heimdall_base \
            -target=module.template_heimdall_gpu \
            -target=module.template_odin_base \
            -target=module.template_odin_gpu \
            -target=module.template_thor_base \
            -target=module.template_thor_gpu

          echo ""
          echo "Terraform refresh complete"

          # Copilot #5: Verify templates exist in state (not just refresh)
          echo ""
          echo "Verifying template modules are present in Terraform state..."

          MISSING_TEMPLATES=()
          for module in template_baldar_base template_baldar_gpu \
                        template_heimdall_base template_heimdall_gpu \
                        template_odin_base template_odin_gpu \
                        template_thor_base template_thor_gpu; do
            if terraform state list | grep -q "module.$module"; then
              echo "  ‚úì module.$module present in state"
            else
              echo "  ‚úó module.$module MISSING from state"
              MISSING_TEMPLATES+=("$module")
            fi
          done

          if [[ ${#MISSING_TEMPLATES[@]} -gt 0 ]]; then
            echo ""
            echo "::error::Template verification failed: ${#MISSING_TEMPLATES[@]} modules missing from state"
            for module in "${MISSING_TEMPLATES[@]}"; do
              echo "::error::Missing: module.$module"
            done
            exit 1
          fi

          echo ""
          echo "‚úÖ All 8 template modules present in Terraform state"
          echo "‚úÖ Templates successfully rebuilt and verified"
          echo "::endgroup::"

          # Update summary
          cat >> $GITHUB_STEP_SUMMARY <<'EOF'

          ### Template Rebuild Execution ‚úÖ

          Templates have been successfully rebuilt and verified:
          - Old templates destroyed
          - New Talos images downloaded
          - New templates created with updated version
          - All 8 template modules present in Terraform state

          **Next**: Control plane and worker nodes will be upgraded sequentially
          EOF

      # CRITICAL: Cleanup any stale locks left by this job on failure
      - name: Cleanup Terraform locks on failure
        if: failure()
        uses: ./.github/actions/clear-terraform-lock
        timeout-minutes: 2
        with:
          terraform_state_bucket: ${{ env.TERRAFORM_STATE_BUCKET }}
          terraform_state_key: terraform.tfstate
          force_clear: "true"

  # ==========================================================================
  # PHASE 2: CONTROL PLANE UPGRADE (SEQUENTIAL)
  # ==========================================================================
  upgrade-control-plane:
    name: Upgrade Control ${{ matrix.node.name }}
    needs: [validate-inputs, rebuild-templates]
    runs-on: cattle-runner
    timeout-minutes: 30
    # Run when: test_controllers=true OR production mode (all test flags false)
    # Use always() to force evaluation even when rebuild-templates is skipped
    # Also require rebuild-templates to have succeeded OR been skipped (not failed)
    if: |
      always() &&
      (inputs.test_controllers == true || (inputs.test_templates == false && inputs.test_controllers == false && inputs.test_workers == false)) &&
      (needs.rebuild-templates.result == 'success' || needs.rebuild-templates.result == 'skipped')
    # CRITICAL: Enforce sequential controller upgrades (Terraform state lock protection)
    # max-parallel alone doesn't work with multiple self-hosted runners
    # This concurrency group ensures only ONE controller upgrades at a time
    concurrency:
      group: cattle-upgrade-controllers-${{ github.run_id }}
      cancel-in-progress: false
    strategy:
      max-parallel: 1
      fail-fast: true
      matrix:
        # When test_controllers=true: Only k8s-ctrl-1
        # When production (all test flags false): All 3 controllers
        node: >-
          ${{
            inputs.test_controllers == true && fromJSON('[
              {"name": "k8s-ctrl-1", "ip": "10.20.67.1", "secret_suffix": "CTRL_1"}
            ]') || fromJSON('[
              {"name": "k8s-ctrl-1", "ip": "10.20.67.1", "secret_suffix": "CTRL_1"},
              {"name": "k8s-ctrl-2", "ip": "10.20.67.2", "secret_suffix": "CTRL_2"},
              {"name": "k8s-ctrl-3", "ip": "10.20.67.3", "secret_suffix": "CTRL_3"}
            ]')
          }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      # Terraform now installed via mise (in setup-cluster-tools action above)

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@61815dcd50bd041e203e49132bacad1fd04d2708 # v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Proxmox SSH key
        env:
          PROXMOX_SSH_KEY: ${{ secrets.PROXMOX_SSH_KEY }}
        run: |
          mkdir -p ~/.ssh
          printf '%s\n' "$PROXMOX_SSH_KEY" > ~/.ssh/proxmox_terraform
          chmod 600 ~/.ssh/proxmox_terraform

      - name: Initialize Terraform (with provider caching)
        working-directory: ./terraform
        run: |
          # Create provider cache directory (prevents GitHub rate limiting)
          mkdir -p "$TF_PLUGIN_CACHE_DIR"

          # Retry terraform init up to 3 times (GitHub occasionally rate-limits provider downloads)
          for attempt in 1 2 3; do
            echo "::group::Terraform init attempt $attempt"
            if terraform init; then
              echo "::endgroup::"
              echo "‚úÖ Terraform initialized successfully"
              exit 0
            fi
            echo "::endgroup::"
            if [[ $attempt -lt 3 ]]; then
              echo "‚ö†Ô∏è terraform init failed, retrying in 30 seconds..."
              sleep 30
            fi
          done
          echo "::error::terraform init failed after 3 attempts"
          exit 1

      - name: Validate etcd quorum health
        run: |
          echo "::group::Checking etcd quorum health"
          echo "Ensuring cluster has healthy quorum before upgrading ${{ matrix.node.name }}..."

          # Get all control plane IPs
          CTRL_NODES="10.20.67.1,10.20.67.2,10.20.67.3"

          # Wait up to 2 minutes for etcd to be healthy
          for i in {1..12}; do
            # Count healthy etcd voting members using talosctl
            # In Talos, etcd runs as a system service, not a Kubernetes pod
            # Query all control plane nodes for connectivity, then deduplicate by member ID
            HEALTHY_MEMBERS=$(talosctl --nodes "$CTRL_NODES" etcd members 2>/dev/null | \
              awk '/false$/ {print $2}' | sort -u | wc -l || echo "0")  # Extract IDs (col 2), deduplicate, count unique

            echo "Attempt $i/12: $HEALTHY_MEMBERS healthy etcd voting members"

            # SECURITY FIX: Require at least 2 healthy members BEFORE destroying ANY node
            if [[ "$HEALTHY_MEMBERS" -ge 2 ]]; then
              echo "‚úÖ etcd quorum healthy ($HEALTHY_MEMBERS voting members)"
              echo "Safe to upgrade ${{ matrix.node.name }} - quorum will survive with 2 members"

              # Show member list for visibility
              echo "Current etcd members:"
              talosctl --nodes "$CTRL_NODES" etcd members | head -n 5

              echo "::endgroup::"
              exit 0
            fi

            sleep 10
          done

          echo "::error::Cannot proceed: etcd quorum at risk"
          echo "::error::Only $HEALTHY_MEMBERS healthy voting members (need at least 2 before destroying any node)"
          exit 1

      - name: Cordon node
        run: |
          echo "Cordoning ${{ matrix.node.name }}..."
          kubectl cordon "${{ matrix.node.name }}"

      - name: Drain node
        timeout-minutes: 5
        run: |
          echo "::group::Draining ${{ matrix.node.name }}"
          echo "Attempting graceful drain (respects PodDisruptionBudget)..."

          # Layer 3: Graceful drain with timeout + force drain fallback
          # Try graceful drain first (respects PDB)
          if timeout 240 kubectl drain "${{ matrix.node.name }}" \
            --ignore-daemonsets \
            --delete-emptydir-data \
            --grace-period=30; then

            echo "‚úÖ Graceful drain succeeded"
          else
            echo "‚ö†Ô∏è  Graceful drain timed out (likely PDB blocking runner eviction)"
            echo "Falling back to force drain..."

            # Force drain bypasses PDB (used when runner pod is on this node)
            kubectl drain "${{ matrix.node.name }}" \
              --ignore-daemonsets \
              --delete-emptydir-data \
              --disable-eviction \
              --force \
              --grace-period=0

            echo "‚úÖ Force drain completed"
            echo "‚ÑπÔ∏è  If runner pod was evicted, GitHub will re-queue job to surviving runner"

            # Give GitHub Actions time to detect runner termination and re-queue
            sleep 30
          fi

          echo "‚úÖ Node drained successfully"
          echo "::endgroup::"

      - name: Backup Terraform state
        working-directory: ./terraform
        run: |
          echo "Creating state backup before destroy..."

          # SECURITY: Check for state lock before proceeding
          if aws s3 ls s3://${{ env.TERRAFORM_STATE_BUCKET }}/terraform.tfstate.tflock 2>/dev/null; then
            echo "::error::Terraform state is locked - another operation in progress"
            echo "::error::Wait for other operation to complete or manually remove lock if stale"
            exit 1
          fi

          aws s3 cp s3://${{ env.TERRAFORM_STATE_BUCKET }}/terraform.tfstate \
            s3://${{ env.TERRAFORM_STATE_BUCKET }}/backups/pre-upgrade-${{ matrix.node.name }}-$(date +%s).tfstate

      # S3-native lock clearing - immediately before terraform operations
      - name: Clear stale Terraform locks (pre-destroy-plan)
        uses: ./.github/actions/clear-terraform-lock
        timeout-minutes: 1
        with:
          terraform_state_bucket: ${{ env.TERRAFORM_STATE_BUCKET }}
          terraform_state_key: terraform.tfstate

      - name: Plan controller destruction
        working-directory: ./terraform
        timeout-minutes: 15
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Planning destruction of ${{ matrix.node.name }}"

          terraform plan \
            -input=false \
            -destroy \
            -out=controller-destroy-${{ matrix.node.name }}.tfplan \
            -target=module.control_plane_nodes[\"${{ matrix.node.name }}\"]

          echo "Plan saved to controller-destroy-${{ matrix.node.name }}.tfplan"
          echo "::endgroup::"

      - name: Validate destruction plan
        working-directory: ./terraform
        run: |
          echo "::group::Validating destruction plan for ${{ matrix.node.name }}"

          # Check plan file exists
          if [[ ! -f controller-destroy-${{ matrix.node.name }}.tfplan ]]; then
            echo "::error::Plan file not found"
            exit 1
          fi

          # Show full plan
          terraform show -no-color controller-destroy-${{ matrix.node.name }}.tfplan > destroy-plan-full.txt
          cat destroy-plan-full.txt

          echo ""
          echo "================================================================"
          echo "SAFETY VALIDATION - Negative Assertions"
          echo "================================================================"

          BUG_DETECTED=false

          # NEGATIVE ASSERTION: No other controllers
          for other_ctrl in k8s-ctrl-1 k8s-ctrl-2 k8s-ctrl-3; do
            if [[ "$other_ctrl" == "${{ matrix.node.name }}" ]]; then
              continue  # Skip self
            fi

            if grep -q "module.control_plane_nodes\[\"$other_ctrl\"\]" destroy-plan-full.txt; then
              echo "::error::SAFETY VIOLATION: Plan includes $other_ctrl"
              echo "::error::This would destroy multiple controllers!"
              BUG_DETECTED=true
            fi
          done

          # NEGATIVE ASSERTION: No workers
          if grep -q "module.worker_nodes" destroy-plan-full.txt; then
            echo "::error::SAFETY VIOLATION: Plan includes worker_nodes"
            echo "::error::This would destroy worker VMs!"
            BUG_DETECTED=true
          fi

          # NEGATIVE ASSERTION: No templates
          if grep -q "module.template_" destroy-plan-full.txt; then
            echo "::error::SAFETY VIOLATION: Plan includes template modules"
            echo "::error::This would destroy templates!"
            BUG_DETECTED=true
          fi

          if [ "$BUG_DETECTED" = true ]; then
            echo ""
            echo "::error::Terraform plan includes unintended targets"
            echo "::error::Workflow terminated to prevent accidental destruction"
            exit 1
          fi

          echo "‚úÖ Negative assertions passed - no unintended targets"

          # POSITIVE ASSERTION: Verify specific controller IS in plan
          if ! grep -q "module.control_plane_nodes\[\"${{ matrix.node.name }}\"\]" destroy-plan-full.txt; then
            echo "::error::SAFETY VIOLATION: Target controller not found in plan"
            echo "::error::Expected: module.control_plane_nodes[\"${{ matrix.node.name }}\"]"
            exit 1
          fi

          echo "‚úÖ Positive assertion passed - target controller found"

          # Count VMs to be destroyed (should be exactly 1)
          VM_COUNT=$(grep -c "module.control_plane_nodes\[\"${{ matrix.node.name }}\"\]" destroy-plan-full.txt || echo "0")
          echo "VMs to be destroyed: $VM_COUNT"

          if [[ $VM_COUNT -ne 1 ]]; then
            echo "::error::Expected exactly 1 VM to be destroyed, found $VM_COUNT"
            exit 1
          fi

          echo ""
          echo "================================================================"
          echo "Validation passed - safe to destroy ${{ matrix.node.name }}"
          echo "================================================================"
          echo "::endgroup::"

      - name: Destroy VM with retry
        working-directory: ./terraform
        timeout-minutes: 10
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Destroying ${{ matrix.node.name }} VM with retry logic"

          MAX_ATTEMPTS=3
          ATTEMPT=1
          SUCCESS=false

          while [[ $ATTEMPT -le $MAX_ATTEMPTS ]]; do
            echo "Destroy attempt $ATTEMPT of $MAX_ATTEMPTS..."

            if terraform apply \
                -input=false \
                -auto-approve \
                controller-destroy-${{ matrix.node.name }}.tfplan; then

              echo "‚úÖ VM destroyed successfully on attempt $ATTEMPT"
              SUCCESS=true
              break
            else
              EXIT_CODE=$?
              echo "‚ö†Ô∏è  Destroy attempt $ATTEMPT failed with exit code $EXIT_CODE"

              if [[ $ATTEMPT -lt $MAX_ATTEMPTS ]]; then
                WAIT_TIME=$((ATTEMPT * 10))
                echo "Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              fi
            fi

            ((ATTEMPT++))
          done

          if [[ "$SUCCESS" != "true" ]]; then
            echo "::error::Failed to destroy VM after $MAX_ATTEMPTS attempts"
            exit 1
          fi

          echo "::endgroup::"

      # S3-native lock clearing - immediately before terraform operations
      - name: Clear stale Terraform locks (pre-create-plan)
        uses: ./.github/actions/clear-terraform-lock
        timeout-minutes: 1
        with:
          terraform_state_bucket: ${{ env.TERRAFORM_STATE_BUCKET }}
          terraform_state_key: terraform.tfstate

      - name: Plan controller recreation
        working-directory: ./terraform
        timeout-minutes: 15
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Planning recreation of ${{ matrix.node.name }}"

          terraform plan \
            -input=false \
            -out=controller-create-${{ matrix.node.name }}.tfplan \
            -target=module.control_plane_nodes[\"${{ matrix.node.name }}\"]

          echo "Plan saved to controller-create-${{ matrix.node.name }}.tfplan"
          echo "::endgroup::"

      - name: Validate recreation plan
        working-directory: ./terraform
        run: |
          echo "::group::Validating recreation plan for ${{ matrix.node.name }}"

          # Check plan file exists
          if [[ ! -f controller-create-${{ matrix.node.name }}.tfplan ]]; then
            echo "::error::Plan file not found"
            exit 1
          fi

          # Show full plan
          terraform show -no-color controller-create-${{ matrix.node.name }}.tfplan > create-plan-full.txt
          cat create-plan-full.txt

          echo ""
          echo "================================================================"
          echo "SAFETY VALIDATION - Negative Assertions"
          echo "================================================================"

          BUG_DETECTED=false

          # NEGATIVE ASSERTION: No other controllers
          for other_ctrl in k8s-ctrl-1 k8s-ctrl-2 k8s-ctrl-3; do
            if [[ "$other_ctrl" == "${{ matrix.node.name }}" ]]; then
              continue  # Skip self
            fi

            if grep -q "module.control_plane_nodes\[\"$other_ctrl\"\]" create-plan-full.txt; then
              echo "::error::SAFETY VIOLATION: Plan includes $other_ctrl"
              BUG_DETECTED=true
            fi
          done

          # NEGATIVE ASSERTION: No workers
          if grep -q "module.worker_nodes" create-plan-full.txt; then
            echo "::error::SAFETY VIOLATION: Plan includes worker_nodes"
            BUG_DETECTED=true
          fi

          # NEGATIVE ASSERTION: No templates
          if grep -q "module.template_" create-plan-full.txt; then
            echo "::error::SAFETY VIOLATION: Plan includes template modules"
            BUG_DETECTED=true
          fi

          if [ "$BUG_DETECTED" = true ]; then
            echo ""
            echo "::error::Terraform plan includes unintended targets"
            echo "::error::Workflow terminated to prevent accidental operations"
            exit 1
          fi

          echo "‚úÖ Negative assertions passed - no unintended targets"

          # POSITIVE ASSERTION: Verify specific controller IS in plan
          if ! grep -q "module.control_plane_nodes\[\"${{ matrix.node.name }}\"\]" create-plan-full.txt; then
            echo "::error::SAFETY VIOLATION: Target controller not found in plan"
            exit 1
          fi

          echo "‚úÖ Positive assertion passed - target controller found"

          echo ""
          echo "================================================================"
          echo "Validation passed - safe to recreate ${{ matrix.node.name }}"
          echo "================================================================"
          echo "::endgroup::"

      # S3-native lock clearing - immediately before terraform operations
      - name: Clear stale Terraform locks (pre-apply)
        uses: ./.github/actions/clear-terraform-lock
        timeout-minutes: 1
        with:
          terraform_state_bucket: ${{ env.TERRAFORM_STATE_BUCKET }}
          terraform_state_key: terraform.tfstate

      - name: Recreate VM with retry
        working-directory: ./terraform
        timeout-minutes: 15
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Recreating ${{ matrix.node.name }} from v${{ inputs.new_version }} template with retry logic"

          MAX_ATTEMPTS=3
          ATTEMPT=1
          SUCCESS=false

          while [[ $ATTEMPT -le $MAX_ATTEMPTS ]]; do
            echo "Recreate attempt $ATTEMPT of $MAX_ATTEMPTS..."

            if terraform apply \
                -input=false \
                -auto-approve \
                controller-create-${{ matrix.node.name }}.tfplan; then

              echo "‚úÖ VM recreated successfully on attempt $ATTEMPT"
              SUCCESS=true
              break
            else
              EXIT_CODE=$?
              echo "‚ö†Ô∏è  Recreate attempt $ATTEMPT failed with exit code $EXIT_CODE"

              if [[ $ATTEMPT -lt $MAX_ATTEMPTS ]]; then
                WAIT_TIME=$((ATTEMPT * 10))
                echo "Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              fi
            fi

            ((ATTEMPT++))
          done

          if [[ "$SUCCESS" != "true" ]]; then
            echo "::error::Failed to recreate VM after $MAX_ATTEMPTS attempts"
            exit 1
          fi

          echo "::endgroup::"

      # ==========================================================================
      # CRITICAL SECURITY OPERATION: Apply Machine Configuration
      # This step implements the 5-layer security architecture to inject
      # Talos machine configs from GitHub Secrets without leaking secrets
      # ==========================================================================
      - name: Apply machine configuration (CRITICAL SECURITY OPERATION)
        id: apply_config
        env:
          NODE_IP: ${{ matrix.node.ip }}
          NODE_NAME: ${{ matrix.node.name }}
          # SECURITY: Secret passed directly to script, not stored in env var
          NODE_CONFIG_SECRET: ${{ secrets[format('TALOS_MACHINE_CONFIG_{0}', matrix.node.secret_suffix)] }}
        shell: bash
        timeout-minutes: 5
        run: |
          # CRITICAL: Absolutely no command echo
          set +x
          set -euo pipefail

          echo "Starting secure configuration deployment for $NODE_NAME..."

          # ============================================================
          # SECURITY LAYER 1: Disable Command Echo (set +x above)
          # ============================================================

          # ============================================================
          # SECURITY LAYER 2: Ephemeral Secure Storage
          # ============================================================

          # Create dedicated config directory with maximum restrictions
          CONFIG_DIR="$(mktemp -d -t talos-config-XXXXXX)"
          chmod 700 "$CONFIG_DIR"

          # Set aggressive cleanup trap
          cleanup_config() {
            if [[ -d "$CONFIG_DIR" ]]; then
              find "$CONFIG_DIR" -type f -exec shred -fz -n 3 {} \; 2>/dev/null || true
              rm -rf "$CONFIG_DIR"
            fi
          }
          trap cleanup_config EXIT

          # ============================================================
          # SECURITY LAYER 3: Decode and Protect
          # ============================================================

          # Decode configuration with maximum protection and error handling
          CONFIG_FILE="$CONFIG_DIR/machine.yaml"

          # Validate secret exists and is not empty
          if [[ -z "$NODE_CONFIG_SECRET" ]]; then
            echo "ERROR: Machine config secret TALOS_MACHINE_CONFIG_${{ matrix.node.secret_suffix }} is not set"
            exit 1
          fi

          if ! echo "$NODE_CONFIG_SECRET" | base64 -d > "$CONFIG_FILE" 2>/dev/null; then
            echo "ERROR: Failed to decode configuration"
            exit 1
          fi
          chmod 600 "$CONFIG_FILE"

          # Validate configuration has expected structure with error suppression
          if ! grep -q "machine:" "$CONFIG_FILE" 2>/dev/null; then
            echo "ERROR: Invalid machine configuration structure"
            exit 1
          fi

          echo "Configuration decoded and validated"

          # ============================================================
          # SECURITY LAYER 4: Apply with Maximum Output Suppression
          # ============================================================

          echo "Applying configuration to node $NODE_IP..."

          # Wait for VM to be network-ready (VM just created)
          # Increased from 60s to 120s after workflow failures where VMs
          # didn't boot fast enough during disk exhaustion conditions
          echo "Waiting for VM to be network-ready..."
          MAX_WAIT=120
          for i in {1..24}; do
            # Use system timeout command - talosctl version has no --timeout flag
            if timeout 5 talosctl -n "$NODE_IP" version --insecure >/dev/null 2>&1; then
              echo "‚úì VM is network-ready after $((i * 5)) seconds"
              break
            fi
            echo "  Waiting... ($((i * 5))/${MAX_WAIT}s)"
            sleep 5
          done

          # ============================================================
          # CRITICAL: Network Stabilization Delay
          # ============================================================
          # After VM responds to talosctl version --insecure, the network
          # interface goes through a brief reconfiguration phase where it
          # becomes temporarily unreachable ("no route to host").
          # Controllers typically stabilize faster than workers.
          # ============================================================
          STABILIZATION_DELAY=30  # 30s for controllers
          echo "Waiting ${STABILIZATION_DELAY}s for VM network to stabilize..."
          sleep $STABILIZATION_DELAY
          echo "‚úì Stabilization delay complete"

          # Application with retry logic and complete output suppression
          # Increased retry count for better reliability
          MAX_ATTEMPTS=5
          ATTEMPT=1
          SUCCESS=false

          while [[ $ATTEMPT -le $MAX_ATTEMPTS ]]; do
            echo "  Attempt $ATTEMPT of $MAX_ATTEMPTS..."

            # Create a log file for this attempt with restrictive permissions
            ATTEMPT_LOG="$CONFIG_DIR/attempt_${ATTEMPT}.log"
            touch "$ATTEMPT_LOG"
            chmod 600 "$ATTEMPT_LOG"

            # Apply configuration with all output captured
            if talosctl -n "$NODE_IP" apply-config \
                --insecure \
                --file "$CONFIG_FILE" \
                --timeout=120s \
                >"$ATTEMPT_LOG" 2>&1; then

              # CRITICAL: Sanitize log IMMEDIATELY even on success
              sed -i 's/token:.*/token: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/secret:.*/secret: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/certificate:.*/certificate: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN .* PRIVATE KEY-----.*/[PRIVATE KEY REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN CERTIFICATE-----.*/[CERTIFICATE REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true

              echo "  ‚úì Configuration applied successfully"
              SUCCESS=true

              # Shred sanitized log (non-verbose to avoid path leaks)
              shred -fz -n 3 "$ATTEMPT_LOG" 2>/dev/null || true
              break

            else
              # CRITICAL: Sanitize log BEFORE checking error types
              sed -i 's/token:.*/token: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/secret:.*/secret: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/certificate:.*/certificate: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN .* PRIVATE KEY-----.*/[PRIVATE KEY REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN CERTIFICATE-----.*/[CERTIFICATE REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true

              echo "  ‚ö† Attempt $ATTEMPT failed"

              # Display sanitized error for debugging
              echo "  Sanitized error output:"
              head -10 "$ATTEMPT_LOG" 2>/dev/null | sed 's/^/    /' || true

              # Now safe to check error types from sanitized log
              BASE_WAIT=15  # 15s base wait for controllers
              if grep -q "connection refused" "$ATTEMPT_LOG" 2>/dev/null; then
                WAIT_TIME=$((ATTEMPT * BASE_WAIT))
                echo "    Error type: Connection refused - node not yet ready"
                echo "    Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              elif grep -qi "no route to host" "$ATTEMPT_LOG" 2>/dev/null; then
                # "no route to host" indicates network interface still initializing
                WAIT_TIME=$((ATTEMPT * BASE_WAIT * 2))
                [[ $WAIT_TIME -gt 60 ]] && WAIT_TIME=60  # Cap at 60 seconds
                echo "    Error type: No route to host - network interface initializing"
                echo "    Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              elif grep -q "timeout" "$ATTEMPT_LOG" 2>/dev/null; then
                WAIT_TIME=$((BASE_WAIT / 2))
                echo "    Error type: Operation timed out"
                echo "    Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              else
                WAIT_TIME=$((ATTEMPT * BASE_WAIT))
                echo "    Error type: Unknown"
                echo "    Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              fi

              # Shred sanitized log (non-verbose to avoid path leaks)
              shred -fz -n 3 "$ATTEMPT_LOG" 2>/dev/null || true
            fi

            ((ATTEMPT++))
          done

          if [[ "$SUCCESS" != "true" ]]; then
            echo "::error::Failed to apply configuration after $MAX_ATTEMPTS attempts"
            exit 1
          fi

          # ============================================================
          # SECURITY LAYER 5: Aggressive Cleanup
          # ============================================================

          # Cleanup happens automatically via EXIT trap
          echo "Configuration deployment completed securely"

      - name: Wait for node to be Ready
        timeout-minutes: 10
        run: |
          echo "Waiting for ${{ matrix.node.name }} to become Ready..."

          for i in {1..60}; do
            if kubectl wait --for=condition=Ready node/"${{ matrix.node.name }}" --timeout=10s 2>/dev/null; then
              echo "‚úÖ Node is Ready"
              break
            fi

            # Show node status every minute (every 6 attempts)
            if [[ $((i % 6)) -eq 0 ]]; then
              echo "Node status after $i attempts:"
              kubectl get node "${{ matrix.node.name }}" || true
              kubectl describe node "${{ matrix.node.name }}" | tail -20 || true
            fi

            echo "Attempt $i/60: Node not ready yet..."
            sleep 10
          done

          # Final check
          kubectl wait --for=condition=Ready node/"${{ matrix.node.name }}" --timeout=30s

      - name: Verify machine configuration applied
        timeout-minutes: 2
        run: |
          echo "::group::Verifying machine configuration"
          NODE_IP="${{ matrix.node.ip }}"
          NODE_NAME="${{ matrix.node.name }}"

          echo "Retrieving applied machine configuration..."

          # Get current machine config from node
          if ! talosctl -n "$NODE_IP" get machineconfig -o yaml > /tmp/applied-config.yaml 2>/dev/null; then
            echo "::error::Failed to retrieve machine config from $NODE_NAME"
            exit 1
          fi

          echo "‚úÖ Machine config retrieved"

          # Verify config is not empty
          if [[ ! -s /tmp/applied-config.yaml ]]; then
            echo "::error::Machine config is empty"
            exit 1
          fi

          # Check for expected sections (proves config was applied)
          echo "Verifying configuration structure..."

          EXPECTED_SECTIONS=("machine:" "cluster:" "network:")
          MISSING_SECTIONS=()

          for section in "${EXPECTED_SECTIONS[@]}"; do
            if grep -q "$section" /tmp/applied-config.yaml; then
              echo "  ‚úì Found $section"
            else
              echo "  ‚úó Missing $section"
              MISSING_SECTIONS+=("$section")
            fi
          done

          if [[ ${#MISSING_SECTIONS[@]} -gt 0 ]]; then
            echo "::error::Machine config missing required sections: ${MISSING_SECTIONS[*]}"
            exit 1
          fi

          echo "‚úÖ Machine configuration structure verified"

          # Verify node can communicate with cluster (proves config is functional)
          echo "Verifying node cluster connectivity..."

          if talosctl -n "$NODE_IP" health --timeout=30s >/dev/null 2>&1; then
            echo "‚úÖ Node health check passed (config is functional)"
          else
            echo "::warning::Health check failed, but node is Ready (may be transient)"
          fi

          rm -f /tmp/applied-config.yaml
          echo "::endgroup::"

      - name: Verify Talos version
        timeout-minutes: 2
        run: |
          echo "Verifying Talos version on ${{ matrix.node.name }}..."

          # Get Talos version from node (not Kubernetes version)
          NODE_IP="${{ matrix.node.ip }}"
          EXPECTED_VERSION="v${{ inputs.new_version }}"

          # Extract version from talosctl using JSON parsing for reliability
          VERSION=$(talosctl version --nodes "$NODE_IP" --json 2>/dev/null | jq -r '.version.tag // empty')

          echo "Node Talos version: $VERSION"
          echo "Expected version: $EXPECTED_VERSION"

          if [[ "$VERSION" != "$EXPECTED_VERSION" ]]; then
            echo "::error::Version mismatch on ${{ matrix.node.name }}"
            echo "::error::Expected: $EXPECTED_VERSION"
            echo "::error::Got: $VERSION"
            exit 1
          fi

          echo "‚úÖ Node running correct Talos version: $VERSION"

      - name: Verify etcd quorum restored
        timeout-minutes: 5
        run: |
          echo "::group::Verifying etcd quorum restored"
          echo "Ensuring ${{ matrix.node.name }} rejoined etcd cluster..."

          # Get all control plane IPs
          CTRL_NODES="10.20.67.1,10.20.67.2,10.20.67.3"

          # Wait up to 5 minutes for etcd to fully restore
          for i in {1..30}; do
            # Count healthy etcd voting members using talosctl
            # In Talos, etcd runs as a system service, not a Kubernetes pod
            # Query all control plane nodes for connectivity, then deduplicate by member ID
            HEALTHY_MEMBERS=$(talosctl --nodes "$CTRL_NODES" etcd members 2>/dev/null | \
              awk '/false$/ {print $2}' | sort -u | wc -l || echo "0")  # Extract IDs (col 2), deduplicate, count unique

            echo "Attempt $i/30: $HEALTHY_MEMBERS healthy etcd voting members"

            # Require 3 healthy members (quorum fully restored)
            if [[ "$HEALTHY_MEMBERS" -eq 3 ]]; then
              echo "‚úÖ etcd quorum fully restored (3/3 voting members)"

              # Verify this node is a member
              MEMBER_LIST=$(talosctl --nodes "$CTRL_NODES" etcd members 2>/dev/null)

              echo "etcd member list:"
              echo "$MEMBER_LIST"

              # Check if this node's IP is in member list
              if echo "$MEMBER_LIST" | grep -q "${{ matrix.node.ip }}"; then
                echo "‚úÖ ${{ matrix.node.name }} is an active etcd member"
                echo "::endgroup::"
                exit 0
              else
                echo "‚ö†Ô∏è  ${{ matrix.node.name }} not yet in member list, waiting..."
              fi
            fi

            sleep 10
          done

          echo "::error::etcd quorum not fully restored after 5 minutes"
          echo "::error::Only $HEALTHY_MEMBERS/3 voting members healthy"
          exit 1

      - name: Allow workload rebalancing
        timeout-minutes: 1
        run: |
          echo "Allowing Kubernetes time to rebalance workloads..."
          echo "Waiting 20 seconds for scheduler to redistribute pods..."

          sleep 20

          # Show pod distribution after rebalancing
          echo ""
          echo "Pod distribution on ${{ matrix.node.name }}:"
          kubectl get pods --all-namespaces --field-selector spec.nodeName=${{ matrix.node.name }} \
            --no-headers 2>/dev/null | wc -l | xargs echo "Pods running on node:"

          echo "‚úÖ Rebalancing period complete"

      - name: Uncordon node
        run: |
          echo "Uncordoning ${{ matrix.node.name }}..."
          kubectl uncordon "${{ matrix.node.name }}"

      - name: Upgrade summary
        if: always()
        run: |
          # Store GHA expressions in bash variables first
          NODE_NAME="${{ matrix.node.name }}"
          NEW_VERSION="${{ inputs.new_version }}"
          JOB_STATUS="${{ job.status }}"

          # Determine status emoji and message based on job status
          if [[ "$JOB_STATUS" == "success" ]]; then
            STATUS_EMOJI="‚úÖ"
            STATUS_MSG="Upgrade successful"
          else
            STATUS_EMOJI="‚ùå"
            STATUS_MSG="Upgrade failed"
          fi

          cat >> $GITHUB_STEP_SUMMARY <<EOF
          ### Control Plane Node: $NODE_NAME $STATUS_EMOJI

          **Status**: $STATUS_MSG
          **New Version**: v$NEW_VERSION
          **Job Status**: $JOB_STATUS
          EOF

      # CRITICAL: Cleanup any stale locks left by this job on failure
      - name: Cleanup Terraform locks on failure
        if: failure()
        uses: ./.github/actions/clear-terraform-lock
        timeout-minutes: 2
        with:
          terraform_state_bucket: ${{ env.TERRAFORM_STATE_BUCKET }}
          terraform_state_key: terraform.tfstate
          force_clear: "true"

  # ==========================================================================
  # PHASE 2.5: PRE-WORKER HEALTH VALIDATION
  # ==========================================================================
  pre-worker-health-check:
    name: Pre-Worker Health Validation
    needs: [validate-inputs, rebuild-templates, upgrade-control-plane]
    # Run when workers will be upgraded
    if: |
      always() &&
      (inputs.test_workers == true || (inputs.test_templates == false && inputs.test_controllers == false && inputs.test_workers == false)) &&
      (needs.rebuild-templates.result == 'success' || needs.rebuild-templates.result == 'skipped') &&
      (needs.upgrade-control-plane.result == 'success' || needs.upgrade-control-plane.result == 'skipped')
    runs-on: cattle-runner
    timeout-minutes: 5
    steps:
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      - name: Validate cluster health
        run: |
          echo "::group::Cluster Health Pre-Flight Check"

          # Check all nodes are Ready
          echo "Checking node health..."
          NOT_READY=$(kubectl get nodes --no-headers | grep -v " Ready " | wc -l)
          if [[ $NOT_READY -gt 0 ]]; then
            echo "::error::Found $NOT_READY nodes not in Ready state"
            kubectl get nodes
            exit 1
          fi
          echo "‚úÖ All nodes Ready"

          # Check etcd health
          echo ""
          echo "Checking etcd health..."
          ETCD_HEALTH=$(kubectl exec -n kube-system etcd-k8s-ctrl-1 -- \
            etcdctl endpoint health --cluster 2>&1 || echo "failed")

          if echo "$ETCD_HEALTH" | grep -q "unhealthy"; then
            echo "::error::etcd cluster unhealthy"
            echo "$ETCD_HEALTH"
            exit 1
          fi
          echo "‚úÖ etcd cluster healthy"

          # Check for pods in CrashLoopBackOff
          echo ""
          echo "Checking for crashing pods..."
          CRASH_PODS=$(kubectl get pods -A --field-selector=status.phase!=Running,status.phase!=Succeeded \
            --no-headers 2>/dev/null | grep "CrashLoopBackOff" | wc -l || echo "0")
          CRASH_PODS=${CRASH_PODS:-0}  # Ensure numeric value

          if [[ $CRASH_PODS -gt 0 ]]; then
            echo "::warning::Found $CRASH_PODS pods in CrashLoopBackOff"
            kubectl get pods -A --field-selector=status.phase!=Running,status.phase!=Succeeded
          else
            echo "‚úÖ No pods in CrashLoopBackOff"
          fi

          # Check Rook-Ceph health (if deployed)
          echo ""
          echo "Checking Rook-Ceph health..."
          if kubectl get namespace rook-ceph &>/dev/null; then
            # Check if this is an external cluster (no rook-ceph-tools deployment)
            IS_EXTERNAL=$(kubectl get cephcluster -n rook-ceph -o jsonpath='{.items[0].spec.external.enable}' 2>/dev/null || echo "false")

            if [[ "$IS_EXTERNAL" == "true" ]]; then
              echo "‚ÑπÔ∏è  External Ceph cluster detected"
              # For external clusters, verify CSI drivers are running
              CSI_RUNNING=$(kubectl get pods -n rook-ceph -l app=csi-rbdplugin-provisioner --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l)
              if [[ $CSI_RUNNING -gt 0 ]]; then
                echo "‚úÖ Ceph CSI drivers running (external cluster healthy)"
              else
                echo "::warning::Ceph CSI drivers not running"
              fi
            elif kubectl get deploy -n rook-ceph rook-ceph-tools &>/dev/null; then
              # Standard Rook-Ceph deployment with tools
              CEPH_HEALTH=$(kubectl exec -n rook-ceph deploy/rook-ceph-tools -- \
                ceph status -f json 2>/dev/null | jq -r '.health.status' || echo "UNKNOWN")

              if [[ "$CEPH_HEALTH" == "HEALTH_OK" ]]; then
                echo "‚úÖ Ceph cluster healthy"
              elif [[ "$CEPH_HEALTH" == "HEALTH_WARN" ]]; then
                echo "::warning::Ceph cluster has warnings"
                kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph status
              else
                echo "::error::Ceph cluster unhealthy or status unknown"
                kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph status || true
                exit 1
              fi
            else
              echo "‚ÑπÔ∏è  rook-ceph-tools not deployed - skipping health check"
            fi
          else
            echo "‚ÑπÔ∏è  Rook-Ceph not deployed, skipping"
          fi

          echo ""
          echo "‚úÖ Cluster health validation passed"
          echo "::endgroup::"

  # ==========================================================================
  # PHASE 3: WORKER UPGRADE (PARALLEL WITH PAIRED ROLLOUT STRATEGY)
  # ==========================================================================
  # Pairing Strategy (cross-diagonal for fault tolerance):
  #   Pair 1: work-1 (baldar) + work-7 (odin)
  #   Pair 2: work-2 (baldar) + work-8 (odin)
  #   Pair 3: work-3 (baldar) + work-9 (odin)
  #   Pair 4: work-5 (heimdall) + work-11 (thor)
  #   Pair 5: work-6 (heimdall) + work-12 (thor)
  #   Pair 6: work-4 (heimdall, GPU RTX A2000) - sequential
  #   Pair 7: work-10 (thor, GPU RTX A5000) - sequential
  #
  # Rollout modes:
  #   safe: max-parallel=1 (one worker at a time, pairs are sequential)
  #   standard: max-parallel=2 (both workers in pair upgrade together, but wait for pair to finish)
  #   aggressive: max-parallel=4 (multiple pairs can run simultaneously)
  upgrade-workers:
    name: Upgrade Worker ${{ matrix.node }} (Pair ${{ matrix.pair_id }})
    needs: [validate-inputs, rebuild-templates, upgrade-control-plane, pre-worker-health-check]
    # Run when: test_workers=true OR production mode (all test flags false)
    # Use always() to force evaluation even when dependencies are skipped
    # Also require dependencies to have succeeded or been skipped (not failed)
    if: |
      always() &&
      (inputs.test_workers == true || (inputs.test_templates == false && inputs.test_controllers == false && inputs.test_workers == false)) &&
      (needs.rebuild-templates.result == 'success' || needs.rebuild-templates.result == 'skipped') &&
      (needs.upgrade-control-plane.result == 'success' || needs.upgrade-control-plane.result == 'skipped') &&
      (needs.pre-worker-health-check.result == 'success' || needs.pre-worker-health-check.result == 'skipped')
    runs-on: cattle-runner
    # GPU nodes need longer timeouts due to:
    # - Extended network-ready wait (3 min vs 1 min)
    # - More config apply retries (5 vs 3)
    # - GPU stabilization wait (60s+)
    # - Extended GPU validation (6 min vs 3.5 min)
    timeout-minutes: 45
    strategy:
      # Dynamic max-parallel based on rollout mode:
      # safe=1 (one worker at a time), standard=2 (pair together), aggressive=4 (multiple pairs)
      #
      # PARALLEL EXECUTION REQUIREMENTS:
      # - Each concurrent job needs its own cattle-runner (external VM runner)
      # - Terraform operations retry with linear backoff on lock contention
      # - Non-terraform steps (talosctl, kubectl) run truly in parallel
      # - See workflow header comments for runner scaling requirements
      max-parallel: ${{ inputs.worker_rollout_mode == 'aggressive' && 4 || (inputs.worker_rollout_mode == 'standard' && 2 || 1) }}
      fail-fast: false
      matrix:
        # Enhanced matrix with pair_id for cross-diagonal pairing
        # Test mode: pair_id=1 (work-1 + work-2, both on baldar for testing)
        # Production: Cross-diagonal pairs avoid single-host failure, GPU nodes sequential
        include: >-
          ${{
            inputs.test_workers == true && fromJSON('[
              {"node": "k8s-work-1", "pair_id": 1, "is_gpu": false, "proxmox_node": "baldar", "template_type": "base", "secret_suffix": "WORK_1"},
              {"node": "k8s-work-2", "pair_id": 1, "is_gpu": false, "proxmox_node": "baldar", "template_type": "base", "secret_suffix": "WORK_2"}
            ]') || fromJSON('[
              {"node": "k8s-work-1", "pair_id": 1, "is_gpu": false, "proxmox_node": "baldar", "template_type": "base", "secret_suffix": "WORK_1"},
              {"node": "k8s-work-7", "pair_id": 1, "is_gpu": false, "proxmox_node": "odin", "template_type": "base", "secret_suffix": "WORK_7"},
              {"node": "k8s-work-2", "pair_id": 2, "is_gpu": false, "proxmox_node": "baldar", "template_type": "base", "secret_suffix": "WORK_2"},
              {"node": "k8s-work-8", "pair_id": 2, "is_gpu": false, "proxmox_node": "odin", "template_type": "base", "secret_suffix": "WORK_8"},
              {"node": "k8s-work-3", "pair_id": 3, "is_gpu": false, "proxmox_node": "baldar", "template_type": "base", "secret_suffix": "WORK_3"},
              {"node": "k8s-work-9", "pair_id": 3, "is_gpu": false, "proxmox_node": "odin", "template_type": "base", "secret_suffix": "WORK_9"},
              {"node": "k8s-work-5", "pair_id": 4, "is_gpu": false, "proxmox_node": "heimdall", "template_type": "base", "secret_suffix": "WORK_5"},
              {"node": "k8s-work-11", "pair_id": 4, "is_gpu": false, "proxmox_node": "thor", "template_type": "base", "secret_suffix": "WORK_11"},
              {"node": "k8s-work-6", "pair_id": 5, "is_gpu": false, "proxmox_node": "heimdall", "template_type": "base", "secret_suffix": "WORK_6"},
              {"node": "k8s-work-12", "pair_id": 5, "is_gpu": false, "proxmox_node": "thor", "template_type": "base", "secret_suffix": "WORK_12"},
              {"node": "k8s-work-4", "pair_id": 6, "is_gpu": true, "proxmox_node": "heimdall", "template_type": "gpu", "gpu_model": "rtx-a2000", "secret_suffix": "WORK_4"},
              {"node": "k8s-work-10", "pair_id": 7, "is_gpu": true, "proxmox_node": "thor", "template_type": "gpu", "gpu_model": "rtx-a5000", "secret_suffix": "WORK_10"}
            ]')
          }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      # Terraform now installed via mise (in setup-cluster-tools action above)

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@61815dcd50bd041e203e49132bacad1fd04d2708 # v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Proxmox SSH key
        env:
          PROXMOX_SSH_KEY: ${{ secrets.PROXMOX_SSH_KEY }}
        run: |
          mkdir -p ~/.ssh
          printf '%s\n' "$PROXMOX_SSH_KEY" > ~/.ssh/proxmox_terraform
          chmod 600 ~/.ssh/proxmox_terraform

      - name: Initialize Terraform (with provider caching)
        working-directory: ./terraform
        run: |
          # Create provider cache directory (prevents GitHub rate limiting)
          mkdir -p "$TF_PLUGIN_CACHE_DIR"

          # Retry terraform init up to 3 times (GitHub occasionally rate-limits provider downloads)
          for attempt in 1 2 3; do
            echo "::group::Terraform init attempt $attempt"
            if terraform init; then
              echo "::endgroup::"
              echo "‚úÖ Terraform initialized successfully"
              exit 0
            fi
            echo "::endgroup::"
            if [[ $attempt -lt 3 ]]; then
              echo "‚ö†Ô∏è terraform init failed, retrying in 30 seconds..."
              sleep 30
            fi
          done
          echo "::error::terraform init failed after 3 attempts"
          exit 1

      - name: Pre-drain health check
        run: |
          echo "::group::Pre-drain health check"
          echo "Checking critical workload distribution before draining ${{ matrix.node }}..."

          # Ensure CoreDNS has replicas on other nodes
          OTHER_DNS=$(kubectl get pods -n kube-system -l k8s-app=kube-dns \
            -o wide 2>/dev/null | grep -vF '${{ matrix.node }}' | grep Running | wc -l || echo "0")

          echo "CoreDNS replicas on other nodes: $OTHER_DNS"

          if [[ $OTHER_DNS -lt 1 ]]; then
            echo "::warning::No CoreDNS replicas on other nodes, scaling up..."
            kubectl scale deployment coredns -n kube-system --replicas=3
            sleep 30
          fi

          # Check for Rook-Ceph OSDs on this node
          OSD_COUNT=$(kubectl get pods -n rook-ceph -l app=rook-ceph-osd \
            -o wide 2>/dev/null | grep -F '${{ matrix.node }}' | grep Running | wc -l || echo "0")

          if [[ $OSD_COUNT -gt 0 ]]; then
            echo "::notice::Node has $OSD_COUNT Ceph OSD(s)"
            echo "Setting Ceph noout flag to prevent rebalancing during upgrade..."
            kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd set noout || true
            echo "osd_count=$OSD_COUNT" >> $GITHUB_ENV
          else
            echo "osd_count=0" >> $GITHUB_ENV
          fi

          echo "::endgroup::"

      - name: Cordon node
        run: |
          echo "Cordoning ${{ matrix.node }}..."
          kubectl cordon "${{ matrix.node }}"

      - name: Drain node
        timeout-minutes: 5
        run: |
          echo "::group::Draining ${{ matrix.node }}"
          echo "Attempting graceful drain (respects PodDisruptionBudget)..."

          # Layer 3: Graceful drain with timeout + force drain fallback
          # Try graceful drain first (respects PDB)
          if timeout 240 kubectl drain "${{ matrix.node }}" \
            --ignore-daemonsets \
            --delete-emptydir-data \
            --grace-period=30; then

            echo "‚úÖ Graceful drain succeeded"
          else
            echo "‚ö†Ô∏è  Graceful drain timed out (likely PDB blocking runner eviction)"
            echo "Falling back to force drain..."

            # Force drain bypasses PDB (used when runner pod is on this node)
            kubectl drain "${{ matrix.node }}" \
              --ignore-daemonsets \
              --delete-emptydir-data \
              --disable-eviction \
              --force \
              --grace-period=0

            echo "‚úÖ Force drain completed"
            echo "‚ÑπÔ∏è  If runner pod was evicted, GitHub will re-queue job to surviving runner"

            # Give GitHub Actions time to detect runner termination and re-queue
            sleep 30
          fi

          # Show pods remaining (should be only daemonsets)
          echo ""
          echo "Pods remaining on node (daemonsets only):"
          kubectl get pods --all-namespaces -o wide 2>/dev/null | grep -F '${{ matrix.node }}' || echo "None"

          echo "‚úÖ Node drained successfully"
          echo "::endgroup::"

      - name: Backup Terraform state
        working-directory: ./terraform
        run: |
          echo "Creating state backup before destroy..."

          # NOTE: Lock check removed for parallel execution support
          # The retry logic in terraform operations handles lock contention properly
          # Checking for locks here would cause race conditions with parallel jobs

          # Backup the state (this is a read-only S3 copy operation)
          aws s3 cp s3://${{ env.TERRAFORM_STATE_BUCKET }}/terraform.tfstate \
            s3://${{ env.TERRAFORM_STATE_BUCKET }}/backups/pre-upgrade-${{ matrix.node }}-$(date +%s).tfstate

          echo "State backup created successfully"

      # NOTE: Lock clearing action removed for parallel execution support
      # When running parallel jobs, clearing locks can delete locks held by other jobs
      # The terraform retry logic handles lock contention by waiting and retrying

      - name: Plan worker destruction
        working-directory: ./terraform
        timeout-minutes: 15
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Planning destruction of ${{ matrix.node }}"

          # PARALLEL EXECUTION: Retry with linear backoff on lock contention
          # When multiple workers run in parallel, they may contend for the state lock
          MAX_LOCK_ATTEMPTS=12
          BASE_WAIT=15
          ATTEMPT=1

          while [[ $ATTEMPT -le $MAX_LOCK_ATTEMPTS ]]; do
            echo "Plan attempt $ATTEMPT of $MAX_LOCK_ATTEMPTS..."

            # Capture output to detect lock contention
            set +e
            PLAN_OUTPUT=$(terraform plan \
              -input=false \
              -destroy \
              -out=worker-destroy-${{ matrix.node }}.tfplan \
              -target=module.worker_nodes[\"${{ matrix.node }}\"] 2>&1)
            PLAN_EXIT=$?
            set -e

            if [[ $PLAN_EXIT -eq 0 ]]; then
              echo "$PLAN_OUTPUT"
              echo "‚úÖ Plan created successfully on attempt $ATTEMPT"
              break
            fi

            # Check if failure was due to lock contention
            if echo "$PLAN_OUTPUT" | grep -q "Error acquiring the state lock"; then
              echo "::notice::State lock held by another parallel job (attempt $ATTEMPT)"

              if [[ $ATTEMPT -lt $MAX_LOCK_ATTEMPTS ]]; then
                # Linear backoff with jitter to prevent thundering herd
                WAIT_TIME=$(( BASE_WAIT * ATTEMPT + RANDOM % 10 ))
                [[ $WAIT_TIME -gt 120 ]] && WAIT_TIME=120
                echo "Waiting ${WAIT_TIME}s before retry (parallel job has lock)..."
                sleep $WAIT_TIME
              fi
            else
              # Real error, not lock contention
              echo "$PLAN_OUTPUT"
              echo "::error::Terraform plan failed with non-lock error"
              exit 1
            fi

            ATTEMPT=$((ATTEMPT + 1))
          done

          if [[ $ATTEMPT -gt $MAX_LOCK_ATTEMPTS ]]; then
            echo "::error::Failed to acquire state lock after $MAX_LOCK_ATTEMPTS attempts"
            echo "::error::Another operation may be stuck - check for stale locks"
            exit 1
          fi

          echo "Plan saved to worker-destroy-${{ matrix.node }}.tfplan"
          echo "::endgroup::"

      - name: Validate destruction plan
        working-directory: ./terraform
        run: |
          echo "::group::Validating destruction plan for ${{ matrix.node }}"

          # Check plan file exists
          if [[ ! -f worker-destroy-${{ matrix.node }}.tfplan ]]; then
            echo "::error::Plan file not found"
            exit 1
          fi

          # Show full plan
          terraform show -no-color worker-destroy-${{ matrix.node }}.tfplan > destroy-plan-full.txt
          cat destroy-plan-full.txt

          echo ""
          echo "================================================================"
          echo "SAFETY VALIDATION - Negative Assertions"
          echo "================================================================"

          BUG_DETECTED=false

          # NEGATIVE ASSERTION 1: No controllers
          for ctrl in k8s-ctrl-1 k8s-ctrl-2 k8s-ctrl-3; do
            if grep -q "module.control_plane_nodes\[\"$ctrl\"\]" destroy-plan-full.txt; then
              echo "::error::SAFETY VIOLATION: Plan includes controller $ctrl"
              echo "::error::This would destroy control plane nodes!"
              BUG_DETECTED=true
            fi
          done

          # NEGATIVE ASSERTION 2: No templates
          if grep -q "module.template_" destroy-plan-full.txt; then
            echo "::error::SAFETY VIOLATION: Plan includes template modules"
            echo "::error::This would destroy VM templates!"
            BUG_DETECTED=true
          fi

          # NEGATIVE ASSERTION 3: No other workers
          for other_worker in k8s-work-1 k8s-work-2 k8s-work-3 k8s-work-4 k8s-work-5 \
                             k8s-work-6 k8s-work-7 k8s-work-8 k8s-work-9 \
                             k8s-work-10 k8s-work-11 k8s-work-12; do
            if [[ "$other_worker" == "${{ matrix.node }}" ]]; then
              continue  # Skip self
            fi

            if grep -q "module.worker_nodes\[\"$other_worker\"\]" destroy-plan-full.txt; then
              echo "::error::SAFETY VIOLATION: Plan includes $other_worker"
              echo "::error::This would destroy multiple workers!"
              BUG_DETECTED=true
            fi
          done

          if [ "$BUG_DETECTED" = true ]; then
            echo ""
            echo "::error::Terraform plan includes unintended targets"
            echo "::error::Workflow terminated to prevent accidental destruction"
            exit 1
          fi

          echo "‚úÖ Negative assertions passed - no unintended targets"

          # POSITIVE ASSERTION: Verify target worker IS in plan
          if ! grep -q "module.worker_nodes\[\"${{ matrix.node }}\"\]" destroy-plan-full.txt; then
            echo "::error::SAFETY VIOLATION: Target worker not found in plan"
            echo "::error::Expected: module.worker_nodes[\"${{ matrix.node }}\"]"
            exit 1
          fi

          echo "‚úÖ Positive assertion passed - target worker found"

          # Count VMs to be destroyed (should be exactly 1)
          VM_COUNT=$(grep -c "module.worker_nodes\[\"${{ matrix.node }}\"\]" destroy-plan-full.txt || echo "0")
          echo "VMs to be destroyed: $VM_COUNT"

          if [[ $VM_COUNT -ne 1 ]]; then
            echo "::error::Expected exactly 1 VM to be destroyed, found $VM_COUNT"
            exit 1
          fi

          echo ""
          echo "================================================================"
          echo "Validation passed - safe to destroy ${{ matrix.node }}"
          echo "================================================================"
          echo "::endgroup::"

      - name: Destroy VM with retry
        working-directory: ./terraform
        timeout-minutes: 15
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Destroying ${{ matrix.node }} VM with retry logic"

          # PARALLEL EXECUTION: Extended retries for lock contention
          # Apply operations are typically fast, but may wait for lock
          MAX_ATTEMPTS=10
          BASE_WAIT=15
          ATTEMPT=1
          SUCCESS=false

          while [[ $ATTEMPT -le $MAX_ATTEMPTS ]]; do
            echo "Destroy attempt $ATTEMPT of $MAX_ATTEMPTS..."

            # Capture output to detect lock contention
            set +e
            APPLY_OUTPUT=$(terraform apply \
                -input=false \
                -auto-approve \
                worker-destroy-${{ matrix.node }}.tfplan 2>&1)
            APPLY_EXIT=$?
            set -e

            if [[ $APPLY_EXIT -eq 0 ]]; then
              echo "$APPLY_OUTPUT"
              echo "‚úÖ VM destroyed successfully on attempt $ATTEMPT"
              SUCCESS=true
              break
            fi

            # PARALLEL EXECUTION: Handle lock release errors as non-fatal
            # If apply succeeded but lock release failed (e.g., lock file deleted by
            # another job), the operation was still successful
            if echo "$APPLY_OUTPUT" | grep -q "Error releasing the state lock"; then
              echo "$APPLY_OUTPUT"
              if echo "$APPLY_OUTPUT" | grep -q "Destroy complete"; then
                echo "::warning::Lock release failed but destroy operation completed successfully"
                echo "::warning::This can happen when parallel jobs interact with state lock"
                echo "‚úÖ VM destroyed successfully (lock release error ignored)"
                SUCCESS=true
                break
              else
                echo "::error::Lock release error but destroy did not complete"
              fi
            fi

            # Check if failure was due to lock contention (acquiring)
            if echo "$APPLY_OUTPUT" | grep -q "Error acquiring the state lock"; then
              echo "::notice::State lock held by another parallel job (attempt $ATTEMPT)"

              if [[ $ATTEMPT -lt $MAX_ATTEMPTS ]]; then
                # Linear backoff with jitter
                WAIT_TIME=$(( BASE_WAIT * ATTEMPT + RANDOM % 10 ))
                [[ $WAIT_TIME -gt 120 ]] && WAIT_TIME=120
                echo "Waiting ${WAIT_TIME}s before retry (parallel job has lock)..."
                sleep $WAIT_TIME

                # Re-plan since state may have changed
                echo "Re-generating plan after lock wait..."
                if ! terraform plan \
                  -input=false \
                  -destroy \
                  -out=worker-destroy-${{ matrix.node }}.tfplan \
                  -target=module.worker_nodes[\"${{ matrix.node }}\"] 2>&1; then
                  echo "::warning::Re-plan failed, will retry with next attempt"
                fi
              fi
            elif echo "$APPLY_OUTPUT" | grep -q "saved plan is stale"; then
              echo "::notice::Plan became stale (state changed by parallel job)"

              if [[ $ATTEMPT -lt $MAX_ATTEMPTS ]]; then
                WAIT_TIME=$(( BASE_WAIT + RANDOM % 10 ))
                echo "Waiting ${WAIT_TIME}s then re-planning..."
                sleep $WAIT_TIME

                # Re-plan with fresh state
                if ! terraform plan \
                  -input=false \
                  -destroy \
                  -out=worker-destroy-${{ matrix.node }}.tfplan \
                  -target=module.worker_nodes[\"${{ matrix.node }}\"] 2>&1; then
                  echo "::warning::Re-plan failed, will retry with next attempt"
                fi
              fi
            elif ! echo "$APPLY_OUTPUT" | grep -q "Error releasing the state lock"; then
              # Real error, not lock related
              echo "$APPLY_OUTPUT"
              echo "::error::Terraform apply failed with non-recoverable error"
              exit 1
            fi

            ATTEMPT=$((ATTEMPT + 1))
          done

          if [ "$SUCCESS" = false ]; then
            echo "::error::Failed to destroy VM after $MAX_ATTEMPTS attempts"
            exit 1
          fi

          echo "::endgroup::"

      - name: Verify correct template exists
        working-directory: ./terraform
        run: |
          echo "::group::Verifying template for ${{ matrix.node }}"

          # Determine expected template
          TEMPLATE_TYPE="${{ matrix.template_type }}"
          PROXMOX_NODE="${{ matrix.proxmox_node }}"
          echo "Node type: $TEMPLATE_TYPE"
          echo "Proxmox host: $PROXMOX_NODE"

          # Verify template exists in Terraform state
          echo "Checking for template_${PROXMOX_NODE}_${TEMPLATE_TYPE} module..."

          if terraform state list | grep -q "module.template_${PROXMOX_NODE}_${TEMPLATE_TYPE}"; then
            echo "‚úÖ Template module template_${PROXMOX_NODE}_${TEMPLATE_TYPE} exists in state"
          else
            echo "::error::Required template module template_${PROXMOX_NODE}_${TEMPLATE_TYPE} not found in Terraform state"
            echo "::error::Available templates:"
            terraform state list | grep "module.template_" || echo "None found"
            exit 1
          fi

          # Log template selection for audit
          echo "GPU enabled: ${{ matrix.is_gpu }}"
          if [[ "${{ matrix.is_gpu }}" == "true" ]]; then
            echo "GPU model: ${{ matrix.gpu_model }}"
          fi

          echo "‚úÖ Template verified and ready for VM creation"
          echo "::endgroup::"

      # NOTE: Lock clearing action removed for parallel execution support
      # When running parallel jobs, clearing locks can delete locks held by other jobs
      # The terraform retry logic handles lock contention by waiting and retrying

      - name: Recreate VM from new template
        working-directory: ./terraform
        timeout-minutes: 20
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Recreating ${{ matrix.node }} from v${{ inputs.new_version }} template"

          # PARALLEL EXECUTION: Retry with linear backoff on lock contention
          # VM creation is the longest terraform operation - needs patience
          MAX_ATTEMPTS=12
          BASE_WAIT=15
          ATTEMPT=1
          SUCCESS=false

          while [[ $ATTEMPT -le $MAX_ATTEMPTS ]]; do
            echo "Create attempt $ATTEMPT of $MAX_ATTEMPTS..."

            # Capture terraform output and exit code
            set +e
            TERRAFORM_OUTPUT=$(terraform apply \
              -input=false \
              -auto-approve \
              -target=module.worker_nodes[\"${{ matrix.node }}\"] 2>&1)
            TERRAFORM_EXIT=$?
            set -e

            if [[ $TERRAFORM_EXIT -eq 0 ]]; then
              echo "$TERRAFORM_OUTPUT"
              echo "‚úÖ VM recreated successfully on attempt $ATTEMPT"
              SUCCESS=true
              break
            fi

            # PARALLEL EXECUTION: Handle lock release errors as non-fatal
            # If apply succeeded but lock release failed (e.g., lock file deleted by
            # another job), the operation was still successful
            if echo "$TERRAFORM_OUTPUT" | grep -q "Error releasing the state lock"; then
              echo "$TERRAFORM_OUTPUT"
              if echo "$TERRAFORM_OUTPUT" | grep -q "Apply complete"; then
                echo "::warning::Lock release failed but apply operation completed successfully"
                echo "::warning::This can happen when parallel jobs interact with state lock"
                echo "‚úÖ VM recreated successfully (lock release error ignored)"
                SUCCESS=true
                break
              else
                echo "::error::Lock release error but apply did not complete"
              fi
            fi

            # Check if failure was due to lock contention (acquiring)
            if echo "$TERRAFORM_OUTPUT" | grep -q "Error acquiring the state lock"; then
              echo "::notice::State lock held by another parallel job (attempt $ATTEMPT)"

              if [[ $ATTEMPT -lt $MAX_ATTEMPTS ]]; then
                # Linear backoff with jitter
                WAIT_TIME=$(( BASE_WAIT * ATTEMPT + RANDOM % 10 ))
                [[ $WAIT_TIME -gt 120 ]] && WAIT_TIME=120
                echo "Waiting ${WAIT_TIME}s before retry (parallel job has lock)..."
                sleep $WAIT_TIME
              fi
            elif echo "$TERRAFORM_OUTPUT" | grep -q "timeout"; then
              echo "::warning::Terraform timeout - Proxmox may be overloaded"

              if [[ $ATTEMPT -lt $MAX_ATTEMPTS ]]; then
                WAIT_TIME=30
                echo "Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              fi
            elif ! echo "$TERRAFORM_OUTPUT" | grep -q "Error releasing the state lock"; then
              # Real error, not lock related
              echo "$TERRAFORM_OUTPUT"
              echo "::error::Terraform apply failed with non-recoverable error"
              exit 1
            fi

            ATTEMPT=$((ATTEMPT + 1))
          done

          if [ "$SUCCESS" = false ]; then
            echo "::error::Failed to create VM after $MAX_ATTEMPTS attempts"
            exit 1
          fi

          echo "::endgroup::"

      # CRITICAL: Verify VM was actually created before attempting config apply
      # This prevents the "VM not network-ready" error when terraform silently failed
      - name: Verify VM exists in Terraform state
        id: verify_vm
        working-directory: ./terraform
        run: |
          echo "::group::Verifying ${{ matrix.node }} exists in Terraform state"

          # Check if VM resource exists in state
          if ! terraform state list 2>/dev/null | grep -q "module.worker_nodes\[\"${{ matrix.node }}\"\]"; then
            echo "::error::CRITICAL: VM ${{ matrix.node }} not found in Terraform state!"
            echo "::error::The terraform apply may have failed silently"
            echo "::error::Cannot proceed with config apply - VM does not exist"
            terraform state list | grep worker || echo "No worker nodes in state"
            exit 1
          fi

          echo "‚úÖ VM ${{ matrix.node }} exists in Terraform state"

          # Extract VM ID from state for additional verification
          VM_INFO=$(terraform state show "module.worker_nodes[\"${{ matrix.node }}\"].proxmox_virtual_environment_vm.talos_node" 2>/dev/null || true)

          if [[ -n "$VM_INFO" ]]; then
            VM_ID=$(echo "$VM_INFO" | grep "vm_id" | head -1 | awk '{print $3}' || echo "unknown")
            echo "VM ID: $VM_ID"
            echo "vm_id=$VM_ID" >> $GITHUB_OUTPUT
          fi

          echo "::endgroup::"

      # ==========================================================================
      # CRITICAL SECURITY OPERATION: Apply Machine Configuration
      # This step implements the 5-layer security architecture to inject
      # Talos machine configs from GitHub Secrets without leaking secrets
      # ==========================================================================
      - name: Apply machine configuration (CRITICAL SECURITY OPERATION)
        id: apply_config
        env:
          NODE_NAME: ${{ matrix.node }}
          # SECURITY: Secret passed directly to script, not stored in env var
          # Worker node configs: TALOS_MACHINE_CONFIG_WORK_1, WORK_2, WORK_3, etc.
          NODE_CONFIG_SECRET: ${{ secrets[format('TALOS_MACHINE_CONFIG_{0}', matrix.secret_suffix)] }}
        shell: bash
        timeout-minutes: 5
        run: |
          # CRITICAL: Absolutely no command echo
          set +x
          set -euo pipefail

          echo "Starting secure configuration deployment for $NODE_NAME..."

          # ============================================================
          # SECURITY LAYER 1: Disable Command Echo (set +x above)
          # ============================================================

          # ============================================================
          # SECURITY LAYER 2: Ephemeral Secure Storage
          # ============================================================

          # Create dedicated config directory with maximum restrictions
          CONFIG_DIR="$(mktemp -d -t talos-config-XXXXXX)"
          chmod 700 "$CONFIG_DIR"

          # Set aggressive cleanup trap
          cleanup_config() {
            if [[ -d "$CONFIG_DIR" ]]; then
              find "$CONFIG_DIR" -type f -exec shred -fz -n 3 {} \; 2>/dev/null || true
              rm -rf "$CONFIG_DIR"
            fi
          }
          trap cleanup_config EXIT

          # ============================================================
          # SECURITY LAYER 3: Decode and Protect
          # ============================================================

          # Decode configuration with maximum protection and error handling
          CONFIG_FILE="$CONFIG_DIR/machine.yaml"

          # Validate secret exists and is not empty
          if [[ -z "$NODE_CONFIG_SECRET" ]]; then
            echo "ERROR: Machine config secret for $NODE_NAME is not set"
            echo "ERROR: Expected secret: TALOS_MACHINE_CONFIG_<WORK_X>"
            exit 1
          fi

          if ! echo "$NODE_CONFIG_SECRET" | base64 -d > "$CONFIG_FILE" 2>/dev/null; then
            echo "ERROR: Failed to decode configuration"
            exit 1
          fi
          chmod 600 "$CONFIG_FILE"

          # Validate configuration has expected structure with error suppression
          if ! grep -q "machine:" "$CONFIG_FILE" 2>/dev/null; then
            echo "ERROR: Invalid machine configuration structure"
            exit 1
          fi

          echo "Configuration decoded and validated"

          # ============================================================
          # SECURITY LAYER 4: Apply with Maximum Output Suppression
          # ============================================================

          # Map node name to IP (hardcoded since node doesn't exist in kubectl yet)
          case "$NODE_NAME" in
            k8s-work-1)  NODE_IP="10.20.67.4" ;;
            k8s-work-2)  NODE_IP="10.20.67.5" ;;
            k8s-work-3)  NODE_IP="10.20.67.6" ;;
            k8s-work-4)  NODE_IP="10.20.67.7" ;;
            k8s-work-5)  NODE_IP="10.20.67.8" ;;
            k8s-work-6)  NODE_IP="10.20.67.9" ;;
            k8s-work-7)  NODE_IP="10.20.67.10" ;;
            k8s-work-8)  NODE_IP="10.20.67.11" ;;
            k8s-work-9)  NODE_IP="10.20.67.12" ;;
            k8s-work-10) NODE_IP="10.20.67.13" ;;
            k8s-work-11) NODE_IP="10.20.67.14" ;;
            k8s-work-12) NODE_IP="10.20.67.15" ;;
            *)
              echo "ERROR: Unknown worker node: $NODE_NAME"
              exit 1
              ;;
          esac

          echo "Applying configuration to node $NODE_IP..."

          # Wait for VM to be network-ready (VM just created)
          # GPU nodes need longer boot time due to:
          # - Larger Talos images (NVIDIA drivers + container toolkit)
          # - More kernel modules to initialize (NVIDIA driver stack)
          # - PCI passthrough initialization
          echo "Waiting for VM to be network-ready..."
          IS_GPU_NODE="${{ matrix.is_gpu }}"
          if [[ "$IS_GPU_NODE" == "true" ]]; then
            MAX_WAIT=180  # 3 minutes for GPU nodes
            MAX_ATTEMPTS=36  # 36 * 5s = 180s
            echo "  GPU node detected - using extended timeout (${MAX_WAIT}s)"
          else
            # Increased from 60s to 120s after workflow failures where VMs
            # didn't boot fast enough during disk exhaustion conditions
            MAX_WAIT=120  # 2 minutes for regular nodes
            MAX_ATTEMPTS=24  # 24 * 5s = 120s
          fi

          NETWORK_READY=false
          FULL_TALOS_DETECTED=false

          for i in $(seq 1 $MAX_ATTEMPTS); do
            # Capture output to diagnose failures
            # Use system timeout command - talosctl version has no --timeout flag
            VERSION_OUTPUT=$(timeout 5 talosctl -n "$NODE_IP" version --insecure 2>&1) || true

            if echo "$VERSION_OUTPUT" | grep -q "Server:"; then
              echo "‚úì VM is network-ready (maintenance mode) after $((i * 5)) seconds"
              NETWORK_READY=true
              break
            fi

            # CRITICAL: Detect if node is running FULL Talos (not maintenance mode)
            # This happens when terraform destroy/apply failed and the old VM is still running
            if echo "$VERSION_OUTPUT" | grep -qi "certificate required\|tls:\|x509:"; then
              echo ""
              echo "::error::CRITICAL: Node at $NODE_IP is running FULL Talos (not maintenance mode)"
              echo "::error::This means the VM was NOT destroyed and recreated by terraform"
              echo "::error::The --insecure flag only works in maintenance mode"
              echo "::error::"
              echo "::error::ROOT CAUSE: Terraform destroy or apply likely failed earlier"
              echo "::error::Check the 'Recreate VM from new template' step logs"
              echo "::error::"
              echo "::error::Manual recovery required:"
              echo "::error::  1. Check terraform state: terraform state list | grep ${{ matrix.node }}"
              echo "::error::  2. Manually destroy VM: terraform destroy -target='module.worker_nodes[\"${{ matrix.node }}\"]'"
              echo "::error::  3. Recreate VM: terraform apply -target='module.worker_nodes[\"${{ matrix.node }}\"]'"
              echo "::error::  4. Re-run workflow"
              FULL_TALOS_DETECTED=true
              break
            fi

            # Show diagnostic info every 30 seconds
            if (( i % 6 == 0 )); then
              echo "  Diagnostic: $VERSION_OUTPUT" | head -1 | sed 's/^/  /'
            fi

            echo "  Waiting... ($((i * 5))/${MAX_WAIT}s)"
            sleep 5
          done

          # Exit with clear error if full Talos detected (don't waste time waiting)
          if [[ "$FULL_TALOS_DETECTED" == "true" ]]; then
            echo ""
            echo "::error::ABORTING: Cannot apply config to node running full Talos"
            exit 1
          fi

          if [[ "$NETWORK_READY" != "true" ]]; then
            echo ""
            echo "::error::VM failed to become network-ready within ${MAX_WAIT}s"
            echo "::error::Possible causes:"
            echo "::error::  1. VM did not boot (check Proxmox console)"
            echo "::error::  2. Network configuration issue (check VM NICs)"
            echo "::error::  3. Talos image boot failure (check serial console)"
            echo "::error::  4. IP address conflict on network"
            echo ""
            echo "::error::Last talosctl output: $(echo "$VERSION_OUTPUT" | head -3)"
            exit 1
          fi

          # ============================================================
          # CRITICAL: Network Stabilization Delay
          # ============================================================
          # After VM responds to talosctl version --insecure, the network interface
          # goes through a brief reconfiguration phase where it becomes temporarily
          # unreachable ("no route to host"). This is a race condition between:
          #   1. VM boot completing (talosctl version succeeds)
          #   2. Network interface fully initializing for apply-config traffic
          #
          # Log analysis from workflow run 20842324836:
          #   - work-7 (SUCCESS): Network stabilized at ~37 seconds
          #   - work-1 (FAILED):  Network still unstable at ~40 seconds
          #
          # GPU nodes need longer stabilization due to NVIDIA driver initialization
          # affecting PCI passthrough network timing.
          # ============================================================
          if [[ "$IS_GPU_NODE" == "true" ]]; then
            STABILIZATION_DELAY=60  # 60s for GPU nodes (NVIDIA driver init)
          else
            STABILIZATION_DELAY=45  # 45s for regular nodes
          fi
          echo "Waiting ${STABILIZATION_DELAY}s for VM network to stabilize after boot..."
          echo "  (This prevents 'no route to host' errors during apply-config)"
          sleep $STABILIZATION_DELAY
          echo "‚úì Stabilization delay complete - proceeding with configuration"

          # Application with retry logic and complete output suppression
          # Increased retry count after workflow failures in run 20842324836
          # where 8/12 workers failed due to insufficient retry window
          if [[ "$IS_GPU_NODE" == "true" ]]; then
            MAX_ATTEMPTS=6  # 6 attempts for GPU nodes (was 5)
            echo "  GPU node - using extended retry count (${MAX_ATTEMPTS} attempts)"
          else
            MAX_ATTEMPTS=6  # 6 attempts for regular nodes (was 3)
          fi
          ATTEMPT=1
          SUCCESS=false

          while [[ $ATTEMPT -le $MAX_ATTEMPTS ]]; do
            echo "  Attempt $ATTEMPT of $MAX_ATTEMPTS..."

            # Create a log file for this attempt with restrictive permissions
            ATTEMPT_LOG="$CONFIG_DIR/attempt_${ATTEMPT}.log"
            touch "$ATTEMPT_LOG"
            chmod 600 "$ATTEMPT_LOG"

            # GPU nodes need longer apply-config timeout due to larger configs
            # and NVIDIA driver initialization
            if [[ "$IS_GPU_NODE" == "true" ]]; then
              APPLY_TIMEOUT="180s"  # 3 minutes for GPU nodes
            else
              APPLY_TIMEOUT="120s"  # 2 minutes for regular nodes
            fi

            # Apply configuration with all output captured
            if talosctl -n "$NODE_IP" apply-config \
                --insecure \
                --file "$CONFIG_FILE" \
                --timeout=$APPLY_TIMEOUT \
                >"$ATTEMPT_LOG" 2>&1; then

              # CRITICAL: Sanitize log IMMEDIATELY even on success
              sed -i 's/token:.*/token: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/secret:.*/secret: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/certificate:.*/certificate: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN .* PRIVATE KEY-----.*/[PRIVATE KEY REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN CERTIFICATE-----.*/[CERTIFICATE REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true

              echo "  ‚úì Configuration applied successfully"
              SUCCESS=true

              # Shred sanitized log (non-verbose to avoid path leaks)
              shred -fz -n 3 "$ATTEMPT_LOG" 2>/dev/null || true
              break

            else
              # CRITICAL: Sanitize log BEFORE checking error types
              sed -i 's/token:.*/token: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/secret:.*/secret: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/certificate:.*/certificate: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN .* PRIVATE KEY-----.*/[PRIVATE KEY REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN CERTIFICATE-----.*/[CERTIFICATE REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true

              echo "  ‚ö† Attempt $ATTEMPT failed"

              # Display sanitized error for debugging (critical for understanding failures)
              echo "  Sanitized error output:"
              head -20 "$ATTEMPT_LOG" 2>/dev/null | sed 's/^/    /' || true

              # GPU nodes need longer waits between retries due to NVIDIA driver initialization
              # Increased BASE_WAIT from 10s to 15s after workflow run 20842324836 analysis
              # showed VMs need longer network stabilization windows
              if [[ "$IS_GPU_NODE" == "true" ]]; then
                BASE_WAIT=30  # 30s base wait for GPU nodes
              else
                BASE_WAIT=15  # 15s base wait for regular nodes (was 10s)
              fi

              # Now safe to check error types from sanitized log
              if grep -q "connection refused" "$ATTEMPT_LOG" 2>/dev/null; then
                WAIT_TIME=$((ATTEMPT * BASE_WAIT))
                echo "    Error type: Connection refused - node not yet ready"
                echo "    Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              elif grep -qi "no route to host" "$ATTEMPT_LOG" 2>/dev/null; then
                # CRITICAL: "no route to host" indicates VM network interface is still
                # initializing after boot. This was the primary failure mode in run
                # 20842324836 where 8/12 workers failed. Use extended wait times.
                WAIT_TIME=$((ATTEMPT * BASE_WAIT * 2))
                [[ $WAIT_TIME -gt 90 ]] && WAIT_TIME=90  # Cap at 90 seconds
                echo "    Error type: No route to host - network interface still initializing"
                echo "    This is expected during VM boot transition - using extended wait"
                echo "    Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              elif grep -q "timeout" "$ATTEMPT_LOG" 2>/dev/null; then
                WAIT_TIME=$((BASE_WAIT / 2))
                echo "    Error type: Operation timed out"
                echo "    Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              elif grep -q "maintenance mode" "$ATTEMPT_LOG" 2>/dev/null; then
                WAIT_TIME=$((ATTEMPT * BASE_WAIT))
                echo "    Error type: Node in maintenance mode (expected)"
                echo "    Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              elif grep -q "i/o timeout" "$ATTEMPT_LOG" 2>/dev/null; then
                WAIT_TIME=$((ATTEMPT * BASE_WAIT * 2))
                [[ $WAIT_TIME -gt 120 ]] && WAIT_TIME=120  # Cap at 2 minutes
                echo "    Error type: I/O timeout - node may still be booting"
                echo "    Waiting ${WAIT_TIME}s before retry..."
                sleep $WAIT_TIME
              else
                # Show first line of error for unknown errors
                FIRST_ERROR=$(head -1 "$ATTEMPT_LOG" 2>/dev/null || echo "No error captured")
                echo "    Error type: Unknown - $FIRST_ERROR"
                sleep $((ATTEMPT * BASE_WAIT))
              fi

              # Shred sanitized log (non-verbose to avoid path leaks)
              shred -fz -n 3 "$ATTEMPT_LOG" 2>/dev/null || true
            fi

            ((ATTEMPT++))
          done

          if [[ "$SUCCESS" != "true" ]]; then
            echo "::error::Failed to apply configuration after $MAX_ATTEMPTS attempts"
            echo "::error::Check the error output above for details"
            exit 1
          fi

          # ============================================================
          # SECURITY LAYER 5: Aggressive Cleanup
          # ============================================================

          # Cleanup happens automatically via EXIT trap
          echo "Configuration deployment completed securely"

      - name: Wait for node to be Ready
        timeout-minutes: 10
        run: |
          echo "Waiting for ${{ matrix.node }} to become Ready..."

          for i in {1..60}; do
            if kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=10s 2>/dev/null; then
              echo "‚úÖ Node is Ready"
              break
            fi

            # Show node status every minute (every 6 attempts)
            if [[ $((i % 6)) -eq 0 ]]; then
              echo "Node status after $i attempts:"
              kubectl get node "${{ matrix.node }}" || true
              kubectl describe node "${{ matrix.node }}" | tail -20 || true
            fi

            echo "Attempt $i/60: Node not ready yet..."
            sleep 10
          done

          # Final check
          kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=30s


      - name: Verify Talos version
        timeout-minutes: 2
        run: |
          echo "Verifying Talos version on ${{ matrix.node }}..."

          # Get node IP from kubectl
          NODE_IP=$(kubectl get node "${{ matrix.node }}" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}')
          EXPECTED_VERSION="v${{ inputs.new_version }}"

          echo "Node IP: $NODE_IP"
          echo "Expected Talos version: $EXPECTED_VERSION"

          # Extract version from talosctl using JSON parsing for reliability
          VERSION=$(talosctl version --nodes "$NODE_IP" --json 2>/dev/null | jq -r '.version.tag // empty')

          echo "Node Talos version: $VERSION"

          if [[ "$VERSION" != "$EXPECTED_VERSION" ]]; then
            echo "::error::Version mismatch on ${{ matrix.node }}"
            echo "::error::Expected: $EXPECTED_VERSION"
            echo "::error::Got: $VERSION"
            exit 1
          fi

          echo "‚úÖ Node running correct Talos version: $VERSION"

      - name: Apply global patches
        timeout-minutes: 5
        run: |
          echo "::group::Applying global patches to ${{ matrix.node }}"
          echo "These patches ensure consistent sysctls, kubelet config, network, and time settings"

          # Use pre-installed talosctl from setup-cluster-tools
          echo "Using pre-installed talosctl..."
          which talosctl
          talosctl version --client

          # Determine node IP
          NODE_IP=$(kubectl get node "${{ matrix.node }}" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}')
          echo "Node IP: $NODE_IP"

          # Apply global patches in order
          echo "Applying global patches..."

          GLOBAL_PATCHES=("machine-kubelet" "machine-network" "machine-sysctls" "machine-time")

          for patch in "${GLOBAL_PATCHES[@]}"; do
            echo "  ‚Üí Applying talos/patches/global/${patch}.yaml..."
            talosctl patch machineconfig \
              --nodes $NODE_IP \
              --patch-file talos/patches/global/${patch}.yaml
          done

          echo "‚úÖ All global patches applied"

          # Wait for patches to be processed
          echo "Waiting for patches to be applied (15s)..."
          sleep 15

          # Wait for node to be ready again
          echo "Waiting for node to become Ready..."
          kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=180s

          echo "‚úÖ Global patches successfully applied to ${{ matrix.node }}"
          echo "::endgroup::"

      - name: Apply GPU patches
        if: matrix.is_gpu == true
        timeout-minutes: 5
        run: |
          echo "::group::Applying GPU patches for ${{ matrix.node }}"
          echo "Detected GPU node: ${{ matrix.node }}"

          # Use pre-installed talosctl from setup-cluster-tools
          echo "Using pre-installed talosctl..."
          which talosctl
          talosctl version --client

          # Determine node IP
          NODE_IP=$(kubectl get node "${{ matrix.node }}" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}')
          echo "Node IP: $NODE_IP"

          # Apply GPU-specific patches
          echo "Applying GPU patch from talos/patches/${{ matrix.node }}/nvidia-gpu.yaml..."
          talosctl patch machineconfig \
            --nodes $NODE_IP \
            --patch-file talos/patches/${{ matrix.node }}/nvidia-gpu.yaml

          # Wait for patch to be processed
          echo "Waiting for GPU patch to be applied (15s)..."
          sleep 15

          # Wait for node to be ready again
          kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=180s

          # Validate GPU detected
          echo "Validating GPU detection..."
          GPU_PRESENT=$(kubectl get node "${{ matrix.node }}" \
            -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' || echo "")

          if [[ "$GPU_PRESENT" != "true" ]]; then
            echo "::error::GPU not detected on ${{ matrix.node }}"
            echo "::error::Expected label nvidia.com/gpu.present=true, got: '$GPU_PRESENT'"
            exit 1
          fi

          echo "‚úÖ GPU patch successfully applied to ${{ matrix.node }}"
          echo "‚úÖ GPU detected: nvidia.com/gpu.present label found"
          echo "::endgroup::"

      - name: Uncordon node
        run: |
          echo "Uncordoning ${{ matrix.node }}..."
          kubectl uncordon "${{ matrix.node }}"

      # GPU nodes need time for NVIDIA components to fully initialize:
      # 1. NVIDIA kernel modules need to probe GPU hardware
      # 2. NVIDIA device plugin DaemonSet needs to schedule
      # 3. Device plugin needs to detect GPU and register with kubelet
      # 4. Kubelet needs to update node allocatable resources
      - name: GPU stabilization wait
        if: matrix.is_gpu == true
        run: |
          echo "::group::GPU stabilization wait for ${{ matrix.node }}"
          echo "Waiting for NVIDIA components to initialize (up to 90s including stabilization)..."
          echo "This ensures:"
          echo "  - NVIDIA kernel modules are fully loaded"
          echo "  - NVIDIA device plugin DaemonSet is scheduled"
          echo "  - Device plugin discovers GPU hardware"
          echo "  - Kubelet registers GPU resources"

          # Check if NVIDIA device plugin pod is running on this node
          DEVICE_PLUGIN_FOUND=false
          for i in {1..12}; do
            NVIDIA_POD=$(kubectl get pods -n nvidia-device-plugin -l app=nvidia-device-plugin \
              -o wide 2>/dev/null | grep "${{ matrix.node }}" | grep -c Running || echo "0")
            if [[ "$NVIDIA_POD" -ge 1 ]]; then
              echo "‚úì NVIDIA device plugin pod running on ${{ matrix.node }}"
              DEVICE_PLUGIN_FOUND=true
              break
            fi
            echo "Waiting for NVIDIA device plugin pod... (attempt $i/12)"
            sleep 5
          done

          # Warn if device plugin wasn't found (may still work if it starts during validation)
          if [[ "$DEVICE_PLUGIN_FOUND" != "true" ]]; then
            echo "::warning::NVIDIA device plugin pod not found on ${{ matrix.node }} after 60s"
            echo "::warning::GPU validation may still succeed if device plugin starts shortly"
          fi

          # Additional stabilization time for GPU resource registration
          echo "Waiting additional 30s for GPU resource registration..."
          sleep 30
          echo "::endgroup::"

      - name: Validate GPU functionality
        if: matrix.is_gpu == true
        timeout-minutes: 8
        run: |
          echo "::group::Validating GPU for ${{ matrix.node }}"

          # Wait for NVIDIA operator to detect GPU
          # Increased timeout for GPU nodes that may have longer initialization
          echo "Waiting for NVIDIA device plugin to register GPU..."
          RETRY_COUNT=0
          MAX_RETRIES=36  # 36 * 10s = 6 minutes

          while [[ $RETRY_COUNT -lt $MAX_RETRIES ]]; do
            if kubectl get nodes ${{ matrix.node }} -o jsonpath='{.status.allocatable}' | grep -q "nvidia.com/gpu"; then
              echo "‚úÖ GPU detected by Kubernetes after $((RETRY_COUNT * 10))s"
              break
            fi

            # Show diagnostic info every 30 seconds
            if [[ $((RETRY_COUNT % 3)) -eq 0 ]]; then
              echo "  Checking NVIDIA device plugin status..."
              kubectl get pods -n nvidia-device-plugin -l app=nvidia-device-plugin -o wide 2>/dev/null | grep "${{ matrix.node }}" || echo "  No NVIDIA device plugin pod found on node"
            fi

            echo "Waiting for GPU detection (attempt $((RETRY_COUNT+1))/$MAX_RETRIES)..."
            sleep 10
            RETRY_COUNT=$((RETRY_COUNT+1))
          done

          if [[ $RETRY_COUNT -eq $MAX_RETRIES ]]; then
            echo "::error::GPU not detected after $((MAX_RETRIES * 10))s"
            echo "Diagnostic information:"
            echo "  Node labels:"
            kubectl get node ${{ matrix.node }} -o jsonpath='{.metadata.labels}' | jq . || true
            echo "  Node allocatable:"
            kubectl get node ${{ matrix.node }} -o jsonpath='{.status.allocatable}' | jq . || true
            echo "  NVIDIA device plugin pods:"
            kubectl get pods -n nvidia-device-plugin -o wide || true
            exit 1
          fi

          # Verify GPU count
          GPU_COUNT=$(kubectl get nodes ${{ matrix.node }} -o jsonpath='{.status.allocatable.nvidia\.com/gpu}')
          echo "GPU count: $GPU_COUNT"

          if [[ "$GPU_COUNT" != "1" ]]; then
            echo "::error::Expected 1 GPU, found $GPU_COUNT"
            exit 1
          fi

          # Verify node labels
          echo "Checking GPU labels..."
          kubectl get nodes ${{ matrix.node }} --show-labels | grep -q "nvidia.com/gpu.present=true" || {
            echo "::error::GPU present label missing"
            exit 1
          }

          # Check for GPU model label (warning only if missing)
          kubectl get nodes ${{ matrix.node }} --show-labels | grep -q "gpu.nvidia.com/model=${{ matrix.gpu_model }}" || {
            echo "::warning::GPU model label missing or incorrect (expected: ${{ matrix.gpu_model }})"
          }

          # Test GPU with a pod
          echo "Testing GPU with nvidia-smi pod..."
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Pod
          metadata:
            name: gpu-test-${{ matrix.node }}
            namespace: default
          spec:
            runtimeClassName: nvidia
            nodeSelector:
              kubernetes.io/hostname: ${{ matrix.node }}
            containers:
            - name: nvidia-smi
              image: nvcr.io/nvidia/cuda:12.0.0-base-ubuntu20.04
              command: ["nvidia-smi"]
              resources:
                limits:
                  nvidia.com/gpu: 1
            restartPolicy: Never
          EOF

          # Wait for pod completion (use Succeeded phase, not Completed condition)
          if kubectl wait --for=jsonpath='{.status.phase}'=Succeeded pod/gpu-test-${{ matrix.node }} --timeout=60s; then
            # Show GPU info on success
            echo "GPU test output:"
            kubectl logs pod/gpu-test-${{ matrix.node }}

            # Clean up test pod
            kubectl delete pod gpu-test-${{ matrix.node }} --ignore-not-found

            echo "‚úÖ GPU validation successful for ${{ matrix.node }}"
          else
            # Handle failure
            POD_PHASE=$(kubectl get pod gpu-test-${{ matrix.node }} -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
            echo "::error::GPU test pod did not succeed (phase: $POD_PHASE)"
            kubectl logs pod/gpu-test-${{ matrix.node }} 2>/dev/null || echo "Could not retrieve logs"
            kubectl delete pod gpu-test-${{ matrix.node }} --ignore-not-found
            exit 1
          fi
          echo "::endgroup::"
      - name: Post-upgrade validation
        run: |
          echo "::group::Post-upgrade validation"

          # Wait for pods to reschedule
          echo "Waiting for workloads to redistribute (20s)..."
          sleep 20

          # Check for unhealthy deployments
          echo "Checking deployment health..."
          UNHEALTHY=$(kubectl get deployments --all-namespaces -o json 2>/dev/null | \
            jq -r '.items[] |
              select(.status.replicas != .status.readyReplicas) |
              "\(.metadata.namespace)/\(.metadata.name): \(.status.readyReplicas)/\(.status.replicas)"' || echo "")

          if [[ -n "$UNHEALTHY" ]]; then
            echo "::warning::Some deployments not fully ready:"
            echo "$UNHEALTHY"
          else
            echo "‚úÖ All deployments healthy"
          fi

          # Remove Ceph noout flag if it was set
          if [[ "${{ env.osd_count }}" != "0" ]]; then
            echo "Removing Ceph noout flag..."
            kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd unset noout || true
          fi

          echo "::endgroup::"

      - name: Upgrade summary
        if: always()
        run: |
          # Store GHA expressions in bash variables first
          NODE_NAME="${{ matrix.node }}"
          NEW_VERSION="${{ inputs.new_version }}"
          JOB_STATUS="${{ job.status }}"
          IS_GPU="${{ matrix.is_gpu }}"

          GPU_STATUS=""
          if [[ "$IS_GPU" == "true" ]]; then
            GPU_STATUS=" (GPU node)"
          fi

          # Determine status emoji and message based on job status
          if [[ "$JOB_STATUS" == "success" ]]; then
            STATUS_EMOJI="‚úÖ"
            STATUS_MSG="Upgrade successful"
          else
            STATUS_EMOJI="‚ùå"
            STATUS_MSG="Upgrade failed"
          fi

          cat >> $GITHUB_STEP_SUMMARY <<EOF
          ### Worker Node: $NODE_NAME$GPU_STATUS $STATUS_EMOJI

          **Status**: $STATUS_MSG
          **New Version**: v$NEW_VERSION
          **Job Status**: $JOB_STATUS
          EOF

      # CRITICAL: Cleanup any stale locks left by this job on failure
      - name: Cleanup Terraform locks on failure
        if: failure()
        uses: ./.github/actions/clear-terraform-lock
        timeout-minutes: 2
        with:
          terraform_state_bucket: ${{ env.TERRAFORM_STATE_BUCKET }}
          terraform_state_key: terraform.tfstate
          force_clear: "true"

  # ==========================================================================
  # PHASE 3.5: BETWEEN-PAIR VALIDATION (SAFE/STANDARD MODES)
  # ==========================================================================
  # This job runs after each pair completes to validate cluster health
  # before proceeding to the next pair. Only runs in safe/standard modes.
  # In aggressive mode, multiple pairs run simultaneously without waiting.
  #
  # LIMITATION: Due to GitHub Actions matrix dependencies, this job runs AFTER
  # all workers complete, not between pairs. This provides post-upgrade validation
  # per pair but not incremental safety gates. The pre-worker-health-check provides
  # adequate safety for the entire worker upgrade phase.
  validate-pair:
    name: Validate Pair ${{ matrix.pair_id }} Health
    needs: [upgrade-workers]
    # Only run in safe/standard modes AND only for pairs 1-6 (pair 7 is last, no validation needed after)
    if: |
      always() &&
      needs.upgrade-workers.result == 'success' &&
      inputs.worker_rollout_mode != 'aggressive' &&
      inputs.test_workers == false
    runs-on: cattle-runner
    timeout-minutes: 5
    strategy:
      max-parallel: 1
      matrix:
        pair_id: [1, 2, 3, 4, 5, 6]
    steps:
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      - name: Wait for pair to stabilize
        run: |
          echo "Waiting 60 seconds for pair ${{ matrix.pair_id }} nodes to stabilize..."
          sleep 60

      - name: Validate cluster health after pair ${{ matrix.pair_id }}
        run: |
          echo "::group::Post-Pair ${{ matrix.pair_id }} Health Check"

          # Check all nodes are Ready
          echo "Checking node health..."
          NOT_READY=$(kubectl get nodes --no-headers | grep -v " Ready " | wc -l)
          if [[ $NOT_READY -gt 0 ]]; then
            echo "::warning::Found $NOT_READY nodes not in Ready state after pair ${{ matrix.pair_id }}"
            kubectl get nodes
            # Don't fail, just warn - pair might still be stabilizing
          else
            echo "‚úÖ All nodes Ready"
          fi

          # Check for pods in bad state
          echo ""
          echo "Checking for unhealthy pods..."
          BAD_PODS=$(kubectl get pods -A --field-selector=status.phase!=Running,status.phase!=Succeeded \
            --no-headers 2>/dev/null | grep -v "Completed" | wc -l || echo "0")

          if [[ $BAD_PODS -gt 0 ]]; then
            echo "::warning::Found $BAD_PODS pods not in Running/Succeeded state"
            kubectl get pods -A --field-selector=status.phase!=Running,status.phase!=Succeeded
          else
            echo "‚úÖ All pods healthy"
          fi

          # Check Rook-Ceph health (if deployed)
          echo ""
          echo "Checking Rook-Ceph health..."
          if kubectl get namespace rook-ceph &>/dev/null; then
            CEPH_HEALTH=$(kubectl exec -n rook-ceph deploy/rook-ceph-tools -- \
              ceph status -f json 2>/dev/null | jq -r '.health.status' || echo "UNKNOWN")

            if [[ "$CEPH_HEALTH" == "HEALTH_OK" ]]; then
              echo "‚úÖ Ceph cluster healthy"
            elif [[ "$CEPH_HEALTH" == "HEALTH_WARN" ]]; then
              echo "::warning::Ceph cluster has warnings after pair ${{ matrix.pair_id }}"
              kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph status
            else
              echo "::warning::Ceph cluster status: $CEPH_HEALTH"
            fi
          else
            echo "‚ÑπÔ∏è  Rook-Ceph not deployed, skipping"
          fi

          echo ""
          echo "‚úÖ Pair ${{ matrix.pair_id }} validation complete"
          echo "::endgroup::"

  # ==========================================================================
  # PHASE 4: CLUSTER VALIDATION
  # ==========================================================================
  validate-cluster:
    name: Validate Cluster Health
    needs: [validate-inputs, rebuild-templates, upgrade-control-plane, upgrade-workers, validate-pair]
    # Run after all upgrades complete (validate-pair may be skipped in aggressive mode or test mode)
    if: |
      always() &&
      inputs.test_templates == false &&
      (needs.validate-pair.result == 'success' || needs.validate-pair.result == 'skipped')
    runs-on: cattle-runner
    timeout-minutes: 10
    steps:
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      - name: Check all nodes upgraded
        run: |
          echo "::group::Validating node versions"
          EXPECTED_VERSION="${{ inputs.new_version }}"
          FAILED_NODES=""

          echo "Expected version: v$EXPECTED_VERSION"
          echo ""
          echo "Node version check:"

          for node in $(kubectl get nodes -o name | cut -d/ -f2); do
            # Get node IP address for talosctl
            NODE_IP=$(kubectl get node "$node" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}')

            if [[ -z "$NODE_IP" ]]; then
              echo "::error::Failed to resolve IP for node $node"
              FAILED_NODES="$FAILED_NODES $node(no-ip)"
              continue
            fi

            # Get Talos version from node using JSON parsing
            VERSION=$(talosctl version --nodes "$NODE_IP" --json 2>/dev/null | jq -r '.version.tag // empty')
            echo "  $node: $VERSION"

            if [[ "$VERSION" != "v$EXPECTED_VERSION" ]]; then
              FAILED_NODES="$FAILED_NODES $node($VERSION)"
            fi
          done

          if [[ -n "$FAILED_NODES" ]]; then
            echo ""
            echo "::error::Nodes not upgraded:$FAILED_NODES"
            exit 1
          fi

          echo ""
          echo "‚úÖ All nodes running v$EXPECTED_VERSION"
          echo "::endgroup::"

      - name: Validate GPU nodes
        if: inputs.test_workers == true || (inputs.test_templates == false && inputs.test_controllers == false && inputs.test_workers == false)
        run: |
          echo "::group::Validating GPU functionality"

          # Check k8s-work-4 (RTX A2000)
          echo "Checking k8s-work-4 (RTX A2000)..."
          GPU_4=$(kubectl get node k8s-work-4 \
            -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' || echo "")

          if [[ "$GPU_4" != "true" ]]; then
            echo "::error::GPU not detected on k8s-work-4"
            echo "::error::Expected label nvidia.com/gpu.present=true, got: '$GPU_4'"
            exit 1
          fi
          echo "‚úÖ k8s-work-4: GPU detected"

          # Check k8s-work-10 (RTX A5000)
          echo "Checking k8s-work-10 (RTX A5000)..."
          GPU_10=$(kubectl get node k8s-work-10 \
            -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' || echo "")

          if [[ "$GPU_10" != "true" ]]; then
            echo "::error::GPU not detected on k8s-work-10"
            echo "::error::Expected label nvidia.com/gpu.present=true, got: '$GPU_10'"
            exit 1
          fi
          echo "‚úÖ k8s-work-10: GPU detected"

          echo ""
          echo "‚úÖ All GPU nodes validated"
          echo "::endgroup::"

      - name: Validate workloads
        run: |
          echo "::group::Validating workload health"

          # Check HelmReleases
          echo "Checking HelmRelease status..."
          FAILED_HR=$(kubectl get helmreleases --all-namespaces -o json 2>/dev/null | \
            jq -r '.items[] |
              select(
                (.status.conditions // []) |
                map(select(.type == "Ready" and .status != "True")) |
                length > 0
              ) |
              "\(.metadata.namespace)/\(.metadata.name): \(.status.conditions[] | select(.type == "Ready") | .message // "Unknown")"' || echo "")

          if [[ -n "$FAILED_HR" ]]; then
            echo "::warning::Some HelmReleases unhealthy:"
            echo "$FAILED_HR"
          else
            echo "‚úÖ All HelmReleases healthy"
          fi

          # Check for non-running pods (excluding completed jobs)
          echo ""
          echo "Checking pod status..."
          UNHEALTHY_PODS=$(kubectl get pods --all-namespaces 2>/dev/null | \
            grep -v Running | grep -v Completed || echo "")

          if [[ -n "$UNHEALTHY_PODS" ]]; then
            echo "::warning::Some pods not running:"
            echo "$UNHEALTHY_PODS"
          else
            echo "‚úÖ All pods running or completed"
          fi

          echo "::endgroup::"

      - name: Validate Flux GitOps
        run: |
          echo "::group::Validating Flux GitOps"

          # Check GitRepository connected to GitHub
          echo "Checking Flux GitRepository..."
          GITREPO_STATUS=$(kubectl get gitrepositories.source.toolkit.fluxcd.io flux-system -n flux-system -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "")

          if [[ "$GITREPO_STATUS" != "True" ]]; then
            echo "::error::Flux GitRepository not connected to GitHub"
            echo "::error::Expected Ready=True, got: '$GITREPO_STATUS'"
            kubectl get gitrepositories.source.toolkit.fluxcd.io flux-system -n flux-system -o yaml || true
            exit 1
          fi

          GITREPO_URL=$(kubectl get gitrepositories.source.toolkit.fluxcd.io flux-system -n flux-system -o jsonpath='{.spec.url}' 2>/dev/null || echo "")
          echo "‚úÖ Flux connected to: $GITREPO_URL"

          # Check core Kustomizations reconciling
          echo ""
          echo "Checking core Flux Kustomizations..."
          FAILED_KUST=$(kubectl get kustomizations.kustomize.toolkit.fluxcd.io -n flux-system -o json 2>/dev/null | jq -r '.items[] | select(.metadata.name as $name | ["cluster-apps", "external-secrets-operator", "1password-connect"] | index($name)) | select((.status.conditions // []) | map(select(.type == "Ready" and .status != "True")) | length > 0) | "\(.metadata.name): \(.status.conditions[] | select(.type == "Ready") | .message // "Unknown")"' || echo "")

          if [[ -n "$FAILED_KUST" ]]; then
            echo "::error::Core Flux Kustomizations not reconciling:"
            echo "$FAILED_KUST"
            exit 1
          fi

          echo "‚úÖ Core Flux Kustomizations reconciling:"
          kubectl get kustomizations.kustomize.toolkit.fluxcd.io -n flux-system -o custom-columns='NAME:.metadata.name,READY:.status.conditions[?(@.type=="Ready")].status,MESSAGE:.status.conditions[?(@.type=="Ready")].message' | grep -E 'cluster-apps|external-secrets-operator|1password-connect' || true

          echo ""
          echo "‚úÖ Flux GitOps functional"
          echo "::endgroup::"

      - name: Final summary
        if: always()
        run: |
          # Determine emojis based on actual phase results
          case "${{ needs.rebuild-templates.result }}" in
            success) TEMPLATE_EMOJI="‚úÖ" ;;
            skipped) TEMPLATE_EMOJI="‚è≠Ô∏è" ;;
            failure) TEMPLATE_EMOJI="‚ùå" ;;
            *) TEMPLATE_EMOJI="‚ùì" ;;
          esac

          case "${{ needs.upgrade-control-plane.result }}" in
            success) CTRL_EMOJI="‚úÖ" ;;
            skipped) CTRL_EMOJI="‚è≠Ô∏è" ;;
            failure) CTRL_EMOJI="‚ùå" ;;
            *) CTRL_EMOJI="‚ùì" ;;
          esac

          case "${{ needs.upgrade-workers.result }}" in
            success) WORKER_EMOJI="‚úÖ" ;;
            skipped) WORKER_EMOJI="‚è≠Ô∏è" ;;
            failure) WORKER_EMOJI="‚ùå" ;;
            *) WORKER_EMOJI="‚ùì" ;;
          esac

          # Determine overall workflow status
          if [[ "${{ job.status }}" == "success" ]]; then
            OVERALL_EMOJI="üéâ"
            OVERALL_TITLE="Cattle Upgrade Complete"
          else
            OVERALL_EMOJI="‚ö†Ô∏è"
            OVERALL_TITLE="Cattle Upgrade Failed"
          fi

          # Capture validation results for health summary
          GPU_STATUS="‚è≠Ô∏è Skipped"
          if [[ "${{ inputs.test_workers }}" == "true" ]] || [[ "${{ inputs.test_templates }}" == "false" && "${{ inputs.test_controllers }}" == "false" && "${{ inputs.test_workers }}" == "false" ]]; then
            GPU_4=$(kubectl get node k8s-work-4 -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' 2>/dev/null || echo "")
            GPU_10=$(kubectl get node k8s-work-10 -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' 2>/dev/null || echo "")
            if [[ "$GPU_4" == "true" && "$GPU_10" == "true" ]]; then
              GPU_STATUS="‚úÖ Both GPUs detected"
            else
              GPU_STATUS="‚ùå GPU detection failed"
            fi
          fi

          FLUX_STATUS=$(kubectl get gitrepositories.source.toolkit.fluxcd.io flux-system -n flux-system -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "Unknown")
          if [[ "$FLUX_STATUS" == "True" ]]; then
            FLUX_STATUS="‚úÖ GitOps connected"
          else
            FLUX_STATUS="‚ùå GitOps issue"
          fi

          FAILED_HR_COUNT=$(kubectl get helmreleases --all-namespaces -o json 2>/dev/null | jq '[.items[] | select((.status.conditions // []) | map(select(.type == "Ready" and .status != "True")) | length > 0)] | length' || echo "0")
          if [[ "$FAILED_HR_COUNT" == "0" ]]; then
            HR_STATUS="‚úÖ All healthy"
          else
            HR_STATUS="‚ö†Ô∏è $FAILED_HR_COUNT unhealthy"
          fi

          UNHEALTHY_POD_COUNT=$(kubectl get pods --all-namespaces 2>/dev/null | grep -v Running | grep -v Completed | grep -v NAMESPACE | wc -l || echo "0")
          if [[ "$UNHEALTHY_POD_COUNT" == "0" ]]; then
            POD_STATUS="‚úÖ All running"
          else
            POD_STATUS="‚ö†Ô∏è $UNHEALTHY_POD_COUNT not running"
          fi

          cat >> $GITHUB_STEP_SUMMARY <<EOF
          ## $OVERALL_TITLE $OVERALL_EMOJI

          **Old Version**: v${{ inputs.old_version }}
          **New Version**: v${{ inputs.new_version }}
          **Test Flags**: Templates=${{ inputs.test_templates }}, Controllers=${{ inputs.test_controllers }}, Workers=${{ inputs.test_workers }}

          ### Phase Results
          - $TEMPLATE_EMOJI Template Rebuild: ${{ needs.rebuild-templates.result }}
          - $CTRL_EMOJI Control Plane Upgrade: ${{ needs.upgrade-control-plane.result }}
          - $WORKER_EMOJI Worker Upgrade: ${{ needs.upgrade-workers.result }}
          - ${{ job.status == 'success' && '‚úÖ' || '‚ùå' }} Validation: ${{ job.status }}

          ### Health Summary
          - **GPU Nodes**: $GPU_STATUS
          - **Flux GitOps**: $FLUX_STATUS
          - **HelmReleases**: $HR_STATUS
          - **Pods**: $POD_STATUS

          ### Cluster State
          \`\`\`
          $(kubectl get nodes -o wide | head -1)
          $(kubectl get nodes -o wide | grep -v NAME | sort -V)
          \`\`\`

          $(
          TEST_RUN=false
          if [[ "${{ inputs.test_templates }}" == "true" ]]; then
            echo "### Next Steps"
            echo ""
            echo "This was a **template test** run only."
            echo "To upgrade controllers: \`gh workflow run upgrade-cattle.yaml -f old_version=${{ inputs.old_version }} -f new_version=${{ inputs.new_version }} -f test_controllers=true\`"
            TEST_RUN=true
          elif [[ "${{ inputs.test_controllers }}" == "true" ]]; then
            echo "### Next Steps"
            echo ""
            echo "This was a **controller test** run (1 controller only)."
            echo "To upgrade all controllers: \`gh workflow run upgrade-cattle.yaml -f old_version=${{ inputs.old_version }} -f new_version=${{ inputs.new_version }}\`"
            TEST_RUN=true
          elif [[ "${{ inputs.test_workers }}" == "true" ]]; then
            echo "### Next Steps"
            echo ""
            echo "This was a **worker test** run (2 workers only)."
            echo "To upgrade all workers: \`gh workflow run upgrade-cattle.yaml -f old_version=${{ inputs.old_version }} -f new_version=${{ inputs.new_version }}\`"
            TEST_RUN=true
          fi
          )
          EOF

          # Exit with error if any critical phase failed
          if [[ "${{ needs.rebuild-templates.result }}" != "success" ]] && [[ "${{ needs.rebuild-templates.result }}" != "skipped" ]]; then
            echo "::error::Template rebuild failed - upgrade incomplete"
            exit 1
          fi

          # Only fail on controller errors if controllers were supposed to run
          if [[ "${{ inputs.test_templates }}" == "false" ]] && [[ "${{ inputs.test_workers }}" == "false" ]] && [[ "${{ needs.upgrade-control-plane.result }}" != "success" ]] && [[ "${{ needs.upgrade-control-plane.result }}" != "skipped" ]]; then
            echo "::error::Control plane upgrade failed"
            exit 1
          fi

          echo "‚úÖ Cattle upgrade workflow completed successfully"

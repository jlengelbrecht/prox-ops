---
name: Cattle Upgrade - Complete Workflow

on:
  workflow_call:
    inputs:
      old_version:
        description: Previous Talos version
        type: string
        required: true
      new_version:
        description: New Talos version to upgrade to
        type: string
        required: true
      test_mode:
        description: Test mode (only 2 workers, skip control plane)
        type: boolean
        default: true
  workflow_dispatch:
    inputs:
      old_version:
        description: Previous Talos version
        type: string
        required: true
      new_version:
        description: New Talos version to upgrade to
        type: string
        required: true
      test_mode:
        description: Test mode (only 2 workers, skip control plane)
        type: boolean
        default: true

env:
  AWS_REGION: us-east-1
  TERRAFORM_STATE_BUCKET: prox-ops-terraform-state
  TERRAFORM_VERSION: "1.10.3"

jobs:
  # ==========================================================================
  # PHASE 1: TEMPLATE REBUILD (CRITICAL - MISSING IN PREVIOUS IMPLEMENTATION)
  # ==========================================================================
  rebuild-templates:
    name: Rebuild Templates v${{ inputs.new_version }}
    runs-on: gha-runner-scale-set
    timeout-minutes: 60
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Validate version inputs
        run: |
          if ! [[ "${{ inputs.old_version }}" =~ ^[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
            echo "::error::Invalid old_version format: '${{ inputs.old_version }}'"
            exit 1
          fi
          if ! [[ "${{ inputs.new_version }}" =~ ^[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
            echo "::error::Invalid new_version format: '${{ inputs.new_version }}'"
            exit 1
          fi
          echo "âœ… Version inputs validated"

      - name: Setup Node.js (required for Terraform action)
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Proxmox SSH key
        env:
          PROXMOX_SSH_KEY: ${{ secrets.PROXMOX_SSH_KEY }}
        run: |
          mkdir -p ~/.ssh
          printf '%s\n' "$PROXMOX_SSH_KEY" > ~/.ssh/proxmox_terraform
          chmod 600 ~/.ssh/proxmox_terraform

      - name: Initialize Terraform
        working-directory: ./terraform
        run: terraform init

      - name: Destroy all old templates
        working-directory: ./terraform
        timeout-minutes: 15
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Destroying all v${{ inputs.old_version }} templates"
          echo "Destroying 8 templates (4 hosts Ã— 2 roles)..."

          terraform destroy \
            -input=false \
            -auto-approve \
            -target=module.template_baldar_controller \
            -target=module.template_baldar_worker \
            -target=module.template_heimdall_controller \
            -target=module.template_heimdall_worker \
            -target=module.template_odin_controller \
            -target=module.template_odin_worker \
            -target=module.template_thor_controller \
            -target=module.template_thor_worker

          echo "All old templates destroyed successfully"
          echo "::endgroup::"

      - name: Create all new templates
        working-directory: ./terraform
        timeout-minutes: 50
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Creating all v${{ inputs.new_version }} templates"
          echo "Creating 8 templates (sequential due to depends_on constraints)..."
          echo "Expected duration: ~45 minutes (8 templates Ã— ~5-6 min each)"

          terraform apply \
            -input=false \
            -auto-approve \
            -target=module.template_baldar_controller \
            -target=module.template_baldar_worker \
            -target=module.template_heimdall_controller \
            -target=module.template_heimdall_worker \
            -target=module.template_odin_controller \
            -target=module.template_odin_worker \
            -target=module.template_thor_controller \
            -target=module.template_thor_worker

          echo "All new templates created successfully"
          echo "::endgroup::"

      - name: Validate templates exist
        working-directory: ./terraform
        run: |
          echo "::group::Validating template creation"
          echo "Verifying all 8 templates exist in Terraform state..."

          # Query Terraform state for template modules
          TEMPLATES=(
            "module.template_baldar_controller"
            "module.template_baldar_worker"
            "module.template_heimdall_controller"
            "module.template_heimdall_worker"
            "module.template_odin_controller"
            "module.template_odin_worker"
            "module.template_thor_controller"
            "module.template_thor_worker"
          )

          ALL_VALID=true
          for template in "${TEMPLATES[@]}"; do
            if terraform state show "$template" &>/dev/null; then
              echo "âœ… $template exists"
            else
              echo "âŒ $template NOT FOUND"
              ALL_VALID=false
            fi
          done

          if [[ "$ALL_VALID" != "true" ]]; then
            echo "::error::Template validation failed - not all templates exist"
            exit 1
          fi

          echo ""
          echo "âœ… All 8 templates validated successfully"
          echo "Ready to proceed with VM upgrades"
          echo "::endgroup::"

      - name: Template rebuild summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY <<EOF
          ## Phase 1: Template Rebuild âœ…

          **Status**: All templates successfully rebuilt

          ### Templates Created
          - Baldar: controller + worker
          - Heimdall: controller + worker
          - Odin: controller + worker
          - Thor: controller + worker

          **Total**: 8 templates
          **Version**: v${{ inputs.new_version }}

          VMs created from these templates will run Talos **v${{ inputs.new_version }}**.
          EOF

  # ==========================================================================
  # PHASE 2: CONTROL PLANE UPGRADE (SEQUENTIAL)
  # ==========================================================================
  upgrade-control-plane:
    name: Upgrade Control ${{ matrix.node.name }}
    needs: rebuild-templates
    runs-on: gha-runner-scale-set
    timeout-minutes: 30
    if: inputs.test_mode == false
    strategy:
      max-parallel: 1
      fail-fast: true
      matrix:
        node:
          - name: "k8s-ctrl-1"
            ip: "10.20.67.1"
            secret_suffix: "CTRL_1"
          - name: "k8s-ctrl-2"
            ip: "10.20.67.2"
            secret_suffix: "CTRL_2"
          - name: "k8s-ctrl-3"
            ip: "10.20.67.3"
            secret_suffix: "CTRL_3"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Proxmox SSH key
        env:
          PROXMOX_SSH_KEY: ${{ secrets.PROXMOX_SSH_KEY }}
        run: |
          mkdir -p ~/.ssh
          printf '%s\n' "$PROXMOX_SSH_KEY" > ~/.ssh/proxmox_terraform
          chmod 600 ~/.ssh/proxmox_terraform

      - name: Initialize Terraform
        working-directory: ./terraform
        run: terraform init

      - name: Validate etcd quorum health
        run: |
          echo "::group::Checking etcd quorum health"
          echo "Ensuring cluster has healthy quorum before upgrading ${{ matrix.node.name }}..."

          # Wait up to 2 minutes for etcd to be healthy
          for i in {1..12}; do
            # Find any healthy etcd pod
            ETCD_POD=$(kubectl get pod -n kube-system -l component=etcd \
              --field-selector=status.phase=Running -o name 2>/dev/null | head -n1)

            if [[ -z "$ETCD_POD" ]]; then
              echo "::error::No healthy etcd pods found"
              exit 1
            fi

            HEALTHY_MEMBERS=$(kubectl exec -n kube-system "$ETCD_POD" -- \
              etcdctl member list 2>/dev/null | grep started | wc -l || echo "0")

            echo "Attempt $i/12: $HEALTHY_MEMBERS healthy etcd members"

            # SECURITY FIX: Require at least 2 healthy members BEFORE destroying ANY node
            if [[ "$HEALTHY_MEMBERS" -ge 2 ]]; then
              echo "âœ… etcd quorum healthy ($HEALTHY_MEMBERS members)"
              echo "Safe to upgrade ${{ matrix.node.name }} - quorum will survive with 2 members"
              echo "::endgroup::"
              exit 0
            fi

            sleep 10
          done

          echo "::error::Cannot proceed: etcd quorum at risk"
          echo "::error::Only $HEALTHY_MEMBERS healthy members (need at least 2 before destroying any node)"
          exit 1

      - name: Cordon node
        run: |
          echo "Cordoning ${{ matrix.node.name }}..."
          kubectl cordon "${{ matrix.node.name }}"

      - name: Drain node
        timeout-minutes: 10
        run: |
          echo "::group::Draining ${{ matrix.node.name }}"
          echo "Evicting all pods from ${{ matrix.node.name }}..."

          kubectl drain "${{ matrix.node.name }}" \
            --ignore-daemonsets \
            --delete-emptydir-data \
            --force \
            --grace-period=60 \
            --timeout=300s

          echo "âœ… Node drained successfully"
          echo "::endgroup::"

      - name: Backup Terraform state
        working-directory: ./terraform
        run: |
          echo "Creating state backup before destroy..."

          # SECURITY: Check for state lock before proceeding
          if aws s3 ls s3://${{ env.TERRAFORM_STATE_BUCKET }}/.terraform.tfstate.lock.info 2>/dev/null; then
            echo "::error::Terraform state is locked - another operation in progress"
            echo "::error::Wait for other operation to complete or manually remove lock if stale"
            exit 1
          fi

          aws s3 cp s3://${{ env.TERRAFORM_STATE_BUCKET }}/terraform.tfstate \
            s3://${{ env.TERRAFORM_STATE_BUCKET }}/backups/pre-upgrade-${{ matrix.node.name }}-$(date +%s).tfstate

      - name: Destroy VM
        working-directory: ./terraform
        timeout-minutes: 5
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Destroying ${{ matrix.node.name }} VM"
          terraform destroy \
            -input=false \
            -auto-approve \
            -target=module.control_plane_nodes[\"${{ matrix.node.name }}\"]

          echo "âœ… VM destroyed"
          echo "::endgroup::"

      - name: Recreate VM from new template
        working-directory: ./terraform
        timeout-minutes: 10
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Recreating ${{ matrix.node.name }} from v${{ inputs.new_version }} template"
          terraform apply \
            -input=false \
            -auto-approve \
            -target=module.control_plane_nodes[\"${{ matrix.node.name }}\"]

          echo "âœ… VM recreated from new template"
          echo "::endgroup::"

      # ==========================================================================
      # CRITICAL SECURITY OPERATION: Apply Machine Configuration
      # This step implements the 5-layer security architecture to inject
      # Talos machine configs from GitHub Secrets without leaking secrets
      # ==========================================================================
      - name: Apply machine configuration (CRITICAL SECURITY OPERATION)
        id: apply_config
        env:
          NODE_IP: ${{ matrix.node.ip }}
          NODE_NAME: ${{ matrix.node.name }}
          # SECURITY: Secret passed directly to script, not stored in env var
          NODE_CONFIG_SECRET: ${{ secrets[format('TALOS_MACHINE_CONFIG_{0}', matrix.node.secret_suffix)] }}
        shell: bash
        timeout-minutes: 5
        run: |
          # CRITICAL: Absolutely no command echo
          set +x
          set -euo pipefail

          echo "Starting secure configuration deployment for $NODE_NAME..."

          # ============================================================
          # SECURITY LAYER 1: Disable Command Echo (set +x above)
          # ============================================================

          # ============================================================
          # SECURITY LAYER 2: Ephemeral Secure Storage
          # ============================================================

          # Create dedicated config directory with maximum restrictions
          CONFIG_DIR="$(mktemp -d -t talos-config-XXXXXX)"
          chmod 700 "$CONFIG_DIR"

          # Set aggressive cleanup trap
          cleanup_config() {
            if [[ -d "$CONFIG_DIR" ]]; then
              find "$CONFIG_DIR" -type f -exec shred -fz -n 3 {} \; 2>/dev/null || true
              rm -rf "$CONFIG_DIR"
            fi
          }
          trap cleanup_config EXIT

          # ============================================================
          # SECURITY LAYER 3: Decode and Protect
          # ============================================================

          # Decode configuration with maximum protection and error handling
          CONFIG_FILE="$CONFIG_DIR/machine.yaml"

          # Validate secret exists and is not empty
          if [[ -z "$NODE_CONFIG_SECRET" ]]; then
            echo "ERROR: Machine config secret TALOS_MACHINE_CONFIG_${{ matrix.node.secret_suffix }} is not set"
            exit 1
          fi

          if ! echo "$NODE_CONFIG_SECRET" | base64 -d > "$CONFIG_FILE" 2>/dev/null; then
            echo "ERROR: Failed to decode configuration"
            exit 1
          fi
          chmod 600 "$CONFIG_FILE"

          # Validate configuration has expected structure with error suppression
          if ! grep -q "machine:" "$CONFIG_FILE" 2>/dev/null; then
            echo "ERROR: Invalid machine configuration structure"
            exit 1
          fi

          echo "Configuration decoded and validated"

          # ============================================================
          # SECURITY LAYER 4: Apply with Maximum Output Suppression
          # ============================================================

          echo "Applying configuration to node $NODE_IP..."

          # Wait for VM to be network-ready (VM just created)
          echo "Waiting for VM to be network-ready..."
          MAX_WAIT=60
          for i in {1..12}; do
            if talosctl -n "$NODE_IP" version --insecure --timeout=5s >/dev/null 2>&1; then
              echo "âœ“ VM is network-ready after $((i * 5)) seconds"
              break
            fi
            echo "  Waiting... ($((i * 5))/${MAX_WAIT}s)"
            sleep 5
          done

          # Application with retry logic and complete output suppression
          MAX_ATTEMPTS=3
          ATTEMPT=1
          SUCCESS=false

          while [[ $ATTEMPT -le $MAX_ATTEMPTS ]]; do
            echo "  Attempt $ATTEMPT of $MAX_ATTEMPTS..."

            # Create a log file for this attempt with restrictive permissions
            ATTEMPT_LOG="$CONFIG_DIR/attempt_${ATTEMPT}.log"
            touch "$ATTEMPT_LOG"
            chmod 600 "$ATTEMPT_LOG"

            # Apply configuration with all output captured
            if talosctl -n "$NODE_IP" apply-config \
                --insecure \
                --file "$CONFIG_FILE" \
                --timeout=120s \
                >"$ATTEMPT_LOG" 2>&1; then

              # CRITICAL: Sanitize log IMMEDIATELY even on success
              sed -i 's/token:.*/token: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/secret:.*/secret: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/certificate:.*/certificate: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN .* PRIVATE KEY-----.*/[PRIVATE KEY REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN CERTIFICATE-----.*/[CERTIFICATE REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true

              echo "  âœ“ Configuration applied successfully"
              SUCCESS=true

              # Shred sanitized log (non-verbose to avoid path leaks)
              shred -fz -n 3 "$ATTEMPT_LOG" 2>/dev/null || true
              break

            else
              # CRITICAL: Sanitize log BEFORE checking error types
              sed -i 's/token:.*/token: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/secret:.*/secret: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/certificate:.*/certificate: [REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN .* PRIVATE KEY-----.*/[PRIVATE KEY REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true
              sed -i 's/-----BEGIN CERTIFICATE-----.*/[CERTIFICATE REDACTED]/g' "$ATTEMPT_LOG" 2>/dev/null || true

              echo "  âš  Attempt $ATTEMPT failed"

              # Now safe to check error types from sanitized log
              if grep -q "connection refused" "$ATTEMPT_LOG" 2>/dev/null; then
                echo "    Node not yet ready, waiting..."
                sleep $((ATTEMPT * 10))
              elif grep -q "timeout" "$ATTEMPT_LOG" 2>/dev/null; then
                echo "    Operation timed out, retrying..."
                sleep 5
              else
                echo "    Unknown error occurred"
              fi

              # Shred sanitized log (non-verbose to avoid path leaks)
              shred -fz -n 3 "$ATTEMPT_LOG" 2>/dev/null || true
            fi

            ((ATTEMPT++))
          done

          if [[ "$SUCCESS" != "true" ]]; then
            echo "ERROR: Failed to apply configuration after $MAX_ATTEMPTS attempts"
            exit 1
          fi

          # ============================================================
          # SECURITY LAYER 5: Aggressive Cleanup
          # ============================================================

          # Cleanup happens automatically via EXIT trap
          echo "Configuration deployment completed securely"

      - name: Wait for node to be Ready
        timeout-minutes: 10
        run: |
          echo "Waiting for ${{ matrix.node.name }} to become Ready..."

          for i in {1..60}; do
            if kubectl wait --for=condition=Ready node/"${{ matrix.node.name }}" --timeout=10s 2>/dev/null; then
              echo "âœ… Node is Ready"
              break
            fi

            # Show node status every minute (every 6 attempts)
            if [[ $((i % 6)) -eq 0 ]]; then
              echo "Node status after $i attempts:"
              kubectl get node "${{ matrix.node.name }}" || true
              kubectl describe node "${{ matrix.node.name }}" | tail -20 || true
            fi

            echo "Attempt $i/60: Node not ready yet..."
            sleep 10
          done

          # Final check
          kubectl wait --for=condition=Ready node/"${{ matrix.node.name }}" --timeout=30s

      - name: Verify version
        run: |
          NODE_VERSION=$(kubectl get node "${{ matrix.node.name }}" -o jsonpath='{.status.nodeInfo.kubeletVersion}')
          echo "Node version: $NODE_VERSION"

          if [[ "$NODE_VERSION" != *"${{ inputs.new_version }}"* ]]; then
            echo "::error::Version mismatch: expected v${{ inputs.new_version }}, got $NODE_VERSION"
            exit 1
          fi

          echo "âœ… Node running correct version: $NODE_VERSION"

      - name: Uncordon node
        run: |
          echo "Uncordoning ${{ matrix.node.name }}..."
          kubectl uncordon "${{ matrix.node.name }}"

      - name: Upgrade summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY <<EOF
          ### Control Plane Node: ${{ matrix.node.name }} âœ…

          **Status**: Upgrade successful
          **New Version**: v${{ inputs.new_version }}
          **Duration**: ${{ job.duration || 'N/A' }}
          EOF

  # ==========================================================================
  # PHASE 3: WORKER UPGRADE (SEQUENTIAL WITH GPU PATCH SUPPORT)
  # ==========================================================================
  upgrade-workers:
    name: Upgrade Worker ${{ matrix.node }}
    needs: [rebuild-templates, upgrade-control-plane]
    if: |
      always() &&
      needs.rebuild-templates.result == 'success' &&
      (inputs.test_mode || needs.upgrade-control-plane.result == 'success')
    runs-on: gha-runner-scale-set
    timeout-minutes: 30
    strategy:
      max-parallel: 1
      fail-fast: false
      matrix:
        node: ${{ inputs.test_mode && fromJSON('["k8s-work-1","k8s-work-2"]') || fromJSON('["k8s-work-1","k8s-work-2","k8s-work-3","k8s-work-4","k8s-work-5","k8s-work-6","k8s-work-11","k8s-work-12","k8s-work-13","k8s-work-14","k8s-work-15","k8s-work-16"]') }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Proxmox SSH key
        env:
          PROXMOX_SSH_KEY: ${{ secrets.PROXMOX_SSH_KEY }}
        run: |
          mkdir -p ~/.ssh
          printf '%s\n' "$PROXMOX_SSH_KEY" > ~/.ssh/proxmox_terraform
          chmod 600 ~/.ssh/proxmox_terraform

      - name: Initialize Terraform
        working-directory: ./terraform
        run: terraform init

      - name: Pre-drain health check
        run: |
          echo "::group::Pre-drain health check"
          echo "Checking critical workload distribution before draining ${{ matrix.node }}..."

          # Ensure CoreDNS has replicas on other nodes
          OTHER_DNS=$(kubectl get pods -n kube-system -l k8s-app=kube-dns \
            -o wide 2>/dev/null | grep -vF '${{ matrix.node }}' | grep Running | wc -l || echo "0")

          echo "CoreDNS replicas on other nodes: $OTHER_DNS"

          if [[ $OTHER_DNS -lt 1 ]]; then
            echo "::warning::No CoreDNS replicas on other nodes, scaling up..."
            kubectl scale deployment coredns -n kube-system --replicas=3
            sleep 30
          fi

          # Check for Rook-Ceph OSDs on this node
          OSD_COUNT=$(kubectl get pods -n rook-ceph -l app=rook-ceph-osd \
            -o wide 2>/dev/null | grep -F '${{ matrix.node }}' | grep Running | wc -l || echo "0")

          if [[ $OSD_COUNT -gt 0 ]]; then
            echo "::notice::Node has $OSD_COUNT Ceph OSD(s)"
            echo "Setting Ceph noout flag to prevent rebalancing during upgrade..."
            kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd set noout || true
            echo "osd_count=$OSD_COUNT" >> $GITHUB_ENV
          else
            echo "osd_count=0" >> $GITHUB_ENV
          fi

          echo "::endgroup::"

      - name: Cordon node
        run: |
          echo "Cordoning ${{ matrix.node }}..."
          kubectl cordon "${{ matrix.node }}"

      - name: Drain node
        timeout-minutes: 10
        run: |
          echo "::group::Draining ${{ matrix.node }}"
          echo "Evicting all pods from ${{ matrix.node }}..."

          kubectl drain "${{ matrix.node }}" \
            --ignore-daemonsets \
            --delete-emptydir-data \
            --force \
            --grace-period=60 \
            --timeout=300s

          # Show pods remaining (should be only daemonsets)
          echo ""
          echo "Pods remaining on node (daemonsets only):"
          kubectl get pods --all-namespaces -o wide 2>/dev/null | grep -F '${{ matrix.node }}' || echo "None"

          echo "âœ… Node drained successfully"
          echo "::endgroup::"

      - name: Backup Terraform state
        working-directory: ./terraform
        run: |
          echo "Creating state backup before destroy..."

          # SECURITY: Check for state lock before proceeding
          if aws s3 ls s3://${{ env.TERRAFORM_STATE_BUCKET }}/.terraform.tfstate.lock.info 2>/dev/null; then
            echo "::error::Terraform state is locked - another operation in progress"
            echo "::error::Wait for other operation to complete or manually remove lock if stale"
            exit 1
          fi

          aws s3 cp s3://${{ env.TERRAFORM_STATE_BUCKET }}/terraform.tfstate \
            s3://${{ env.TERRAFORM_STATE_BUCKET }}/backups/pre-upgrade-${{ matrix.node }}-$(date +%s).tfstate

      - name: Destroy VM
        working-directory: ./terraform
        timeout-minutes: 5
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Destroying ${{ matrix.node }} VM"
          terraform destroy \
            -input=false \
            -auto-approve \
            -target=module.worker_nodes[\"${{ matrix.node }}\"]

          echo "âœ… VM destroyed"
          echo "::endgroup::"

      - name: Recreate VM from new template
        working-directory: ./terraform
        timeout-minutes: 10
        env:
          TF_VAR_proxmox_username: ${{ secrets.PROXMOX_USERNAME }}
          TF_VAR_proxmox_password: ${{ secrets.PROXMOX_PASSWORD }}
        run: |
          echo "::group::Recreating ${{ matrix.node }} from v${{ inputs.new_version }} template"
          terraform apply \
            -input=false \
            -auto-approve \
            -target=module.worker_nodes[\"${{ matrix.node }}\"]

          echo "âœ… VM recreated from new template"
          echo "::endgroup::"

      - name: Wait for node to be Ready
        timeout-minutes: 10
        run: |
          echo "Waiting for ${{ matrix.node }} to become Ready..."

          for i in {1..60}; do
            if kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=10s 2>/dev/null; then
              echo "âœ… Node is Ready"
              break
            fi

            # Show node status every minute (every 6 attempts)
            if [[ $((i % 6)) -eq 0 ]]; then
              echo "Node status after $i attempts:"
              kubectl get node "${{ matrix.node }}" || true
              kubectl describe node "${{ matrix.node }}" | tail -20 || true
            fi

            echo "Attempt $i/60: Node not ready yet..."
            sleep 10
          done

          # Final check
          kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=30s

      - name: Verify version
        run: |
          NODE_VERSION=$(kubectl get node "${{ matrix.node }}" -o jsonpath='{.status.nodeInfo.kubeletVersion}')
          echo "Node version: $NODE_VERSION"

          if [[ "$NODE_VERSION" != *"${{ inputs.new_version }}"* ]]; then
            echo "::error::Version mismatch: expected v${{ inputs.new_version }}, got $NODE_VERSION"
            exit 1
          fi

          echo "âœ… Node running correct version: $NODE_VERSION"

      - name: Apply global patches
        timeout-minutes: 5
        run: |
          echo "::group::Applying global patches to ${{ matrix.node }}"
          echo "These patches ensure consistent sysctls, kubelet config, network, and time settings"

          # Download talosctl for the new version
          echo "Installing talosctl v${{ inputs.new_version }}..."

          # Download binary
          curl -sL "https://github.com/siderolabs/talos/releases/download/v${{ inputs.new_version }}/talosctl-linux-amd64" \
            -o talosctl

          # Download checksums
          curl -sL "https://github.com/siderolabs/talos/releases/download/v${{ inputs.new_version }}/sha512sum.txt" \
            -o sha512sum.txt

          # SECURITY: Verify checksum
          if ! sha512sum -c --ignore-missing sha512sum.txt 2>&1 | grep -q "talosctl-linux-amd64: OK"; then
            echo "::error::Checksum verification failed for talosctl v${{ inputs.new_version }}"
            exit 1
          fi

          chmod +x talosctl
          ./talosctl version --client

          # Determine node IP
          NODE_IP=$(kubectl get node "${{ matrix.node }}" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}')
          echo "Node IP: $NODE_IP"

          # Apply global patches in order
          echo "Applying global patches..."

          GLOBAL_PATCHES=("machine-kubelet" "machine-network" "machine-sysctls" "machine-time")

          for patch in "${GLOBAL_PATCHES[@]}"; do
            echo "  â†’ Applying talos/patches/global/${patch}.yaml..."
            ./talosctl patch machineconfig \
              --nodes $NODE_IP \
              --patch-file talos/patches/global/${patch}.yaml
          done

          echo "âœ… All global patches applied"

          # Wait for patches to be processed
          echo "Waiting for patches to be applied (30s)..."
          sleep 30

          # Wait for node to be ready again
          echo "Waiting for node to become Ready..."
          kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=300s

          echo "âœ… Global patches successfully applied to ${{ matrix.node }}"
          echo "::endgroup::"

      - name: Apply GPU patches
        if: matrix.node == 'k8s-work-4' || matrix.node == 'k8s-work-14'
        timeout-minutes: 5
        run: |
          echo "::group::Applying GPU patches for ${{ matrix.node }}"
          echo "Detected GPU node: ${{ matrix.node }}"

          # Download talosctl for the new version
          echo "Installing talosctl v${{ inputs.new_version }}..."

          # Download binary
          curl -sL "https://github.com/siderolabs/talos/releases/download/v${{ inputs.new_version }}/talosctl-linux-amd64" \
            -o talosctl

          # Download checksums
          curl -sL "https://github.com/siderolabs/talos/releases/download/v${{ inputs.new_version }}/sha512sum.txt" \
            -o sha512sum.txt

          # SECURITY: Verify checksum
          if ! sha512sum -c --ignore-missing sha512sum.txt 2>&1 | grep -q "talosctl-linux-amd64: OK"; then
            echo "::error::Checksum verification failed for talosctl v${{ inputs.new_version }}"
            exit 1
          fi

          chmod +x talosctl
          ./talosctl version --client

          # Determine node IP
          NODE_IP=$(kubectl get node "${{ matrix.node }}" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}')
          echo "Node IP: $NODE_IP"

          # Apply GPU-specific patches
          echo "Applying GPU patch from talos/patches/${{ matrix.node }}/nvidia-gpu.yaml..."
          ./talosctl patch machineconfig \
            --nodes $NODE_IP \
            --patch-file talos/patches/${{ matrix.node }}/nvidia-gpu.yaml

          # Wait for patch to be processed
          echo "Waiting for GPU patch to be applied (30s)..."
          sleep 30

          # Wait for node to be ready again
          kubectl wait --for=condition=Ready node/"${{ matrix.node }}" --timeout=300s

          # Validate GPU detected
          echo "Validating GPU detection..."
          GPU_PRESENT=$(kubectl get node "${{ matrix.node }}" \
            -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' || echo "")

          if [[ "$GPU_PRESENT" != "true" ]]; then
            echo "::error::GPU not detected on ${{ matrix.node }}"
            echo "::error::Expected label nvidia.com/gpu.present=true, got: '$GPU_PRESENT'"
            exit 1
          fi

          echo "âœ… GPU patch successfully applied to ${{ matrix.node }}"
          echo "âœ… GPU detected: nvidia.com/gpu.present label found"
          echo "::endgroup::"

      - name: Uncordon node
        run: |
          echo "Uncordoning ${{ matrix.node }}..."
          kubectl uncordon "${{ matrix.node }}"

      - name: Post-upgrade validation
        run: |
          echo "::group::Post-upgrade validation"

          # Wait for pods to reschedule
          echo "Waiting for workloads to redistribute (30s)..."
          sleep 30

          # Check for unhealthy deployments
          echo "Checking deployment health..."
          UNHEALTHY=$(kubectl get deployments --all-namespaces -o json 2>/dev/null | \
            jq -r '.items[] |
              select(.status.replicas != .status.readyReplicas) |
              "\(.metadata.namespace)/\(.metadata.name): \(.status.readyReplicas)/\(.status.replicas)"' || echo "")

          if [[ -n "$UNHEALTHY" ]]; then
            echo "::warning::Some deployments not fully ready:"
            echo "$UNHEALTHY"
          else
            echo "âœ… All deployments healthy"
          fi

          # Remove Ceph noout flag if it was set
          if [[ "${{ env.osd_count }}" != "0" ]]; then
            echo "Removing Ceph noout flag..."
            kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd unset noout || true
          fi

          echo "::endgroup::"

      - name: Upgrade summary
        if: always()
        run: |
          GPU_STATUS=""
          if [[ "${{ matrix.node }}" == "k8s-work-4" ]] || [[ "${{ matrix.node }}" == "k8s-work-14" ]]; then
            GPU_STATUS=" (GPU node)"
          fi

          cat >> $GITHUB_STEP_SUMMARY <<EOF
          ### Worker Node: ${{ matrix.node }}$GPU_STATUS âœ…

          **Status**: Upgrade successful
          **New Version**: v${{ inputs.new_version }}
          EOF

  # ==========================================================================
  # PHASE 4: CLUSTER VALIDATION
  # ==========================================================================
  validate-cluster:
    name: Validate Cluster Health
    needs: [rebuild-templates, upgrade-control-plane, upgrade-workers]
    if: always()
    runs-on: gha-runner-scale-set
    timeout-minutes: 10
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup cluster tools
        uses: ./.github/actions/setup-cluster-tools
        with:
          talosconfig: ${{ secrets.TALOSCONFIG }}

      - name: Check all nodes upgraded
        run: |
          echo "::group::Validating node versions"
          EXPECTED_VERSION="${{ inputs.new_version }}"
          FAILED_NODES=""

          echo "Expected version: v$EXPECTED_VERSION"
          echo ""
          echo "Node version check:"

          for node in $(kubectl get nodes -o name | cut -d/ -f2); do
            VERSION=$(kubectl get node "$node" -o jsonpath='{.status.nodeInfo.kubeletVersion}')
            echo "  $node: $VERSION"

            if [[ "$VERSION" != *"$EXPECTED_VERSION"* ]]; then
              FAILED_NODES="$FAILED_NODES $node($VERSION)"
            fi
          done

          if [[ -n "$FAILED_NODES" ]]; then
            echo ""
            echo "::error::Nodes not upgraded:$FAILED_NODES"
            exit 1
          fi

          echo ""
          echo "âœ… All nodes running v$EXPECTED_VERSION"
          echo "::endgroup::"

      - name: Validate GPU nodes
        if: inputs.test_mode == false
        run: |
          echo "::group::Validating GPU functionality"

          # Check k8s-work-4 (RTX A2000)
          echo "Checking k8s-work-4 (RTX A2000)..."
          GPU_4=$(kubectl get node k8s-work-4 \
            -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' || echo "")

          if [[ "$GPU_4" != "true" ]]; then
            echo "::error::GPU not detected on k8s-work-4"
            echo "::error::Expected label nvidia.com/gpu.present=true, got: '$GPU_4'"
            exit 1
          fi
          echo "âœ… k8s-work-4: GPU detected"

          # Check k8s-work-14 (RTX A5000)
          echo "Checking k8s-work-14 (RTX A5000)..."
          GPU_14=$(kubectl get node k8s-work-14 \
            -o jsonpath='{.metadata.labels.nvidia\.com/gpu\.present}' || echo "")

          if [[ "$GPU_14" != "true" ]]; then
            echo "::error::GPU not detected on k8s-work-14"
            echo "::error::Expected label nvidia.com/gpu.present=true, got: '$GPU_14'"
            exit 1
          fi
          echo "âœ… k8s-work-14: GPU detected"

          echo ""
          echo "âœ… All GPU nodes validated"
          echo "::endgroup::"

      - name: Validate workloads
        run: |
          echo "::group::Validating workload health"

          # Check HelmReleases
          echo "Checking HelmRelease status..."
          FAILED_HR=$(kubectl get helmreleases --all-namespaces -o json 2>/dev/null | \
            jq -r '.items[] |
              select(
                (.status.conditions // []) |
                map(select(.type == "Ready" and .status != "True")) |
                length > 0
              ) |
              "\(.metadata.namespace)/\(.metadata.name): \(.status.conditions[] | select(.type == "Ready") | .message // "Unknown")"' || echo "")

          if [[ -n "$FAILED_HR" ]]; then
            echo "::warning::Some HelmReleases unhealthy:"
            echo "$FAILED_HR"
          else
            echo "âœ… All HelmReleases healthy"
          fi

          # Check for non-running pods (excluding completed jobs)
          echo ""
          echo "Checking pod status..."
          UNHEALTHY_PODS=$(kubectl get pods --all-namespaces 2>/dev/null | \
            grep -v Running | grep -v Completed || echo "")

          if [[ -n "$UNHEALTHY_PODS" ]]; then
            echo "::warning::Some pods not running:"
            echo "$UNHEALTHY_PODS"
          else
            echo "âœ… All pods running or completed"
          fi

          echo "::endgroup::"

      - name: Final summary
        if: always()
        run: |
          cat >> $GITHUB_STEP_SUMMARY <<EOF
          ## Cattle Upgrade Complete ðŸŽ‰

          **Old Version**: v${{ inputs.old_version }}
          **New Version**: v${{ inputs.new_version }}
          **Test Mode**: ${{ inputs.test_mode }}

          ### Phase Results
          - âœ… Template Rebuild: ${{ needs.rebuild-templates.result }}
          - ${{ inputs.test_mode && 'â­ï¸ ' || 'âœ… ' }}Control Plane Upgrade: ${{ inputs.test_mode && 'skipped (test mode)' || needs.upgrade-control-plane.result }}
          - âœ… Worker Upgrade: ${{ needs.upgrade-workers.result }}
          - âœ… Validation: success

          ### Cluster State
          \`\`\`
          $(kubectl get nodes -o wide)
          \`\`\`

          $(if [[ "${{ inputs.test_mode }}" == "true" ]]; then
            echo "### Next Steps"
            echo ""
            echo "This was a **test mode** run (2 workers only)."
            echo ""
            echo "To run full production upgrade:"
            echo "\`\`\`"
            echo "gh workflow run upgrade-cattle.yaml \\"
            echo "  --field old_version=${{ inputs.old_version }} \\"
            echo "  --field new_version=${{ inputs.new_version }} \\"
            echo "  --field test_mode=false"
            echo "\`\`\`"
          fi)
          EOF

          # Exit with error if any critical phase failed
          if [[ "${{ needs.rebuild-templates.result }}" != "success" ]]; then
            echo "::error::Template rebuild failed - upgrade incomplete"
            exit 1
          fi

          if [[ "${{ inputs.test_mode }}" == "false" ]] && [[ "${{ needs.upgrade-control-plane.result }}" != "success" ]]; then
            echo "::error::Control plane upgrade failed"
            exit 1
          fi

          echo "âœ… Cattle upgrade workflow completed successfully"
